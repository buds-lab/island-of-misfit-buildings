{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T08:51:30.786132Z",
     "start_time": "2018-12-27T08:51:28.670475Z"
    }
   },
   "outputs": [],
   "source": [
    "# Built-in libraries\n",
    "from datetime import datetime, timedelta\n",
    "import math\n",
    "\n",
    "# NumPy, SciPy and Pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Scikit-Learn\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import linear_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T08:51:30.836479Z",
     "start_time": "2018-12-27T08:51:30.788705Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function that calculates a load profile curve, for each building on a dataframe, based on the specified function.\n",
    "Currently the following functions are supported:\n",
    "- Average\n",
    "- Median\n",
    "\n",
    "And the currently resolution:\n",
    "- Daily\n",
    "\n",
    "The name parameter allow us to save the resulting csv with a more comprehensive title\n",
    "\"\"\"\n",
    "def doAggregation(datasetName, context, function, resolution='day'):\n",
    "    dataframe = pd.read_csv('../data/processed/{}_{}_dataset.csv'.format(datasetName, context), parse_dates=True, \n",
    "                            infer_datetime_format=True, index_col=0)\n",
    "    df_load_curves = pd.DataFrame() # dataframe that will hold all load curves\n",
    "\n",
    "    # resample based on parameter\n",
    "    if (resolution == 'day'):\n",
    "        availableSamples = (dataframe.resample('1D').asfreq()).index # get list of timestamps group by day\n",
    "        delta = 23 # timedelta based on resample\n",
    "    else:\n",
    "        print(\"Please choose a valid resolution\")\n",
    "        exit()\n",
    "\n",
    "    # iterate through all buildings (column)\n",
    "    for column in range(len(dataframe.columns)):\n",
    "        df_sampledReadings = pd.DataFrame() # dataframe to hold new samples for a column\n",
    "        currentColumn = pd.DataFrame(dataframe.iloc[:, column])\n",
    "        \n",
    "        # iterate through each day\n",
    "        for timestamp in availableSamples:\n",
    "            # update time limits to the current date\n",
    "            start = timestamp\n",
    "            end = timestamp + timedelta(hours=delta)\n",
    "            # get meter data from only this resolution\n",
    "            df_reading = currentColumn[(currentColumn.index >= start) & (currentColumn.index <= end)]\n",
    "            # ignore index since they are unique timestamps\n",
    "            df_reading.reset_index(drop=True, inplace=True)         \n",
    "            # append new sample as columns\n",
    "            df_sampledReadings = pd.concat([df_sampledReadings, df_reading], axis=1)\n",
    "            \n",
    "        # make sure sure there are no columns with NaN values\n",
    "        df_sampledReadings.dropna(axis=1, how='all', inplace=True)\n",
    "        df_sampledReadings = df_sampledReadings.T # transpose it so it's easier to see and operate\n",
    "        # up to this point, the matrix above has the shape nxm where is the number of instances and m is the number of readings\n",
    "    \n",
    "        # if any NaN prevailed\n",
    "        df_sampledReadings.fillna(value=0, inplace=True) \n",
    "\n",
    "        # calculate load curve based on function\n",
    "        if function == 'average':\n",
    "            load_curve = np.mean(df_sampledReadings, axis = 0)\n",
    "\n",
    "        elif function =='median':\n",
    "            load_curve = np.median(df_sampledReadings, axis = 0)\n",
    "            \n",
    "#         elif function == 'regression':\n",
    "#             # 1. Generate one single time series for the entire building\n",
    "#             df_one_ts = pd.DataFrame() # empty data frame to hold complete time series\n",
    "#             df_trans = df_sampledReadings.T\n",
    "#             # iterate through each day worth of readings\n",
    "#             for column in range(len(df_trans.columns)):\n",
    "#                 currentColumn = pd.DataFrame(df_trans.iloc[:, column])\n",
    "#                 df_one_ts = df_one_ts.append(currentColumn, ignore_index=True)\n",
    "#             # rename variables            \n",
    "#             x_values = df_one_ts.index.values.reshape(-1, 1)\n",
    "#             y_values = df_one_ts.values\n",
    "            \n",
    "#             # 2. Perform polynomial regressions on the single time series\n",
    "#             degrees = range(1, 21)\n",
    "#             base_model = linear_model.LinearRegression().fit(x_values, y_values)\n",
    "#             base_curve = base_model.predict(x_values)\n",
    "#             rmse = np.sqrt(mean_squared_error(y_values, base_curve))\n",
    "#             load_curve = base_curve\n",
    "\n",
    "#             ####################################################################\n",
    "#             # TODO: multiple reg plotting\n",
    "#             # plt.figure(figsize=(18,10))\n",
    "#             # plt.scatter(x_values, y_values)\n",
    "#             ####################################################################\n",
    "\n",
    "#             for d in degrees: # fit a curve for each degree\n",
    "#                 polynomial_features= PolynomialFeatures(degree=d)\n",
    "#                 x_poly = polynomial_features.fit_transform(x_values)    \n",
    "#                 poly_model = linear_model.LinearRegression()\n",
    "#                 poly_model.fit(x_poly, y_values)\n",
    "#                 poly_curve = poly_model.predict(x_poly)\n",
    "#                 rmse_d = np.sqrt(mean_squared_error(y_values,poly_curve))\n",
    "\n",
    "#                 # print(rmse_d)\n",
    "                \n",
    "#                 # keep the polynomial with lowest RSME\n",
    "#                 if rmse_d < rmse :\n",
    "#                     rmse = rmse_d\n",
    "#                     load_curve = poly_curve\n",
    "                    \n",
    "\n",
    "                ####################################################################\n",
    "                # TODO: multiple reg plotting\n",
    "            #     plt.plot(x_values, poly_curve, \"k-\")\n",
    "            # plt.plot(x_values, load_curve, \"r-\")\n",
    "            # plt.title(\"Load Profiles and red representative curve based on {}\".format(function))        \n",
    "            # plt.show()\n",
    "            # print(rmse)\n",
    "            # exit()\n",
    "            ###################################################################\n",
    "\n",
    "        else:\n",
    "            print(\"Please choose a valid context\")\n",
    "            exit()\n",
    "\n",
    "        ####################################################################\n",
    "        # TODO: coding is for plotting purposes\n",
    "        # plt.figure(figsize=(18,10))\n",
    "        # x_axis = range(0, len(df_sampledReadings.columns))\n",
    "        # for _, curve in df_sampledReadings.iterrows():\n",
    "        #     plt.plot(curve, \"k-\", alpha=.2)\n",
    "        # plt.plot(load_curve, \"r-\")\n",
    "        # plt.title(\"Load Profiles and red representative curve based on {}\".format(function))        \n",
    "        # plt.show()\n",
    "        # print(X)\n",
    "        # exit()\n",
    "        ####################################################################\n",
    "\n",
    "        # turn into one column dataframe for easier manipulation\n",
    "        load_curve = pd.DataFrame(load_curve)\n",
    "        # keep the instance name as column name\n",
    "        instance_name = []\n",
    "        instance_name.append(df_sampledReadings.index[0])\n",
    "        load_curve.columns = instance_name\n",
    "        # append current load curve to dataframe\n",
    "        df_load_curves = pd.concat([df_load_curves, load_curve], axis=1)\n",
    "        \n",
    "        # end of for loop for one column\n",
    "\n",
    "    # replace NaN's with 0    \n",
    "    df_load_curves = df_load_curves.replace(0.0, np.nan)\n",
    "    # drop rows with all nan values\n",
    "    df_load_curves = df_load_curves.dropna(axis=1, how='all') \n",
    "    \n",
    "    # particular to the DGS dataset\n",
    "    if datasetName =='DGS':    \n",
    "        # drop columns with more than 4 nan values (seems to be a sweet spot)\n",
    "        df_load_curves = df_load_curves.dropna(thresh=len(df_load_curves) - 4, axis=1)\n",
    "\n",
    "    df_load_curves.fillna(value=0, inplace=True) \n",
    "    df_load_curves = df_load_curves.T # rotate the final dataframe\n",
    "    \n",
    "    # save the file and return the dataframe\n",
    "    df_load_curves.to_csv(\"../data/processed/{}_{}_{}_dataset.csv\".format(datasetName, context, function))\n",
    "    return df_load_curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T08:57:16.173501Z",
     "start_time": "2018-12-27T08:51:32.792012Z"
    }
   },
   "outputs": [],
   "source": [
    "df_BDG_weekday_average = doAggregation('BDG', 'weekday', 'average')\n",
    "df_BDG_weekday_median = doAggregation('BDG', 'weekday', 'median')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T09:03:04.607698Z",
     "start_time": "2018-12-27T08:57:16.175097Z"
    }
   },
   "outputs": [],
   "source": [
    "df_BDG_weekend_average = doAggregation('BDG', 'weekend', 'average')\n",
    "df_BDG_weekend_median = doAggregation('BDG', 'weekend', 'median')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T09:08:18.699388Z",
     "start_time": "2018-12-27T09:03:04.609535Z"
    }
   },
   "outputs": [],
   "source": [
    "df_BDG_fullweek_average = doAggregation('BDG', 'fullweek', 'average')\n",
    "df_BDG_fullweek_median = doAggregation('BDG', 'fullweek', 'median')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T09:23:46.242235Z",
     "start_time": "2018-12-27T09:08:18.701307Z"
    }
   },
   "outputs": [],
   "source": [
    "df_DGS_weekday_average = doAggregation('DGS', 'weekday', 'average')\n",
    "df_DGS_weekday_median = doAggregation('DGS', 'weekday', 'median')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T09:39:59.628774Z",
     "start_time": "2018-12-27T09:23:46.244756Z"
    }
   },
   "outputs": [],
   "source": [
    "df_DGS_weekend_average = doAggregation('DGS', 'weekend', 'average')\n",
    "df_DGS_weekend_median = doAggregation('DGS', 'weekend', 'median')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T09:56:08.714290Z",
     "start_time": "2018-12-27T09:39:59.631142Z"
    }
   },
   "outputs": [],
   "source": [
    "df_DGS_fullweek_average = doAggregation('DGS', 'fullweek', 'average')\n",
    "df_DGS_fullweek_median = doAggregation('DGS', 'fullweek', 'median')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

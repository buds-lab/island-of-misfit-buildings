{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Energy Clustering and Outlier Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-06T05:57:17.347Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Gnome Dataset: hourly meter data from 507 buildings\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load building gnome dataset (BGD)\n",
    "df_bgd = pd.read_csv('data/temp_open_utc_complete.csv')\n",
    "print(\"Building Gnome Dataset: hourly meter data from {} buildings\".format(len(df_bgd.columns) - 1))\n",
    "\n",
    "# load dc building dataset (DC)\n",
    "# df_dc = pd.read_csv('../data/temp_open_utc_complete.csv')\n",
    "# print(\"DC Dataset: hourly meter data from {} buildings\".format(len(df_dc.columns)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to have sample the original raw data in such a way that we end up with a n x m matrix, where n is the number of buildings and m is the meter data time series values. An additional feature can be appended at the beginning to indicate the building ID, although the index of the table can be used for this purpose.\n",
    "\n",
    "\\begin{bmatrix}%\n",
    "x_1^1 & x_2^1 & \\dots & x_m^1 \\\\\n",
    "x_1^2 & x_2^2 & \\dots & x_m^2 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_1^n & x_2^n & \\dots & x_m^n \\\\\n",
    "\\end{bmatrix}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to determine the matrix stated above, a common time window is found (**from 01/01/15 to 30/11/15**). In this time period there are **368** buildings (**72.6%** of the dataset). Additionally, the following contexts are defined:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Week day: Daily cumulative meter readings for each week day of the selected time window **(data/weekdayContext.csv)**.\n",
    "- Weekend: Daily cumulative meter readongs for weekend days of the selected time window **(data/weekendContext.csv)**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw values of the time series, within from 01/01/15 and 30/11/15:\n",
    "- Week day context: 368 buildings x 238 days\n",
    "- Weekend context: 368 buildings x 97 days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features learned using TSFRESH and DTW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Time Series Feature extraction based on scalable hypothesis tests (TSFRESH) library (https://github.com/blue-yonder/tsfresh). Additonally, another feature that will be appended will be the Dynamic Time Wrapping (DTW).\n",
    "\n",
    "It is important to highlight that TSFRESH will require the raw time series values from above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Features from existing work on BGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximately 215 features have already been extracted in previous work (https://github.com/buds-lab/temporal-features-for-nonres-buildings-library)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, generate the csv files for each context. Currently, the **week day** and **weekend** context csv files can be found in **data/**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1: k-Shape on Raw Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Select context csv to work with (see above)\n",
    "2. Download k-Shape library (https://github.com/Mic92/kshape)\n",
    "3. Run k-Shape algorithm\n",
    "4. Evaluation:\n",
    "    1. Evaluate resulting clusters with sillouhette coefficient plot\n",
    "    2. Evaluate resulting clusters with elbow method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: Feature Extraction and Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Select context csv to work with (see above)\n",
    "2. Download TSFERSH library (https://github.com/blue-yonder/tsfresh)\n",
    "3. Run TSFRESH on dataset\n",
    "4. Calculate Dynamic time Warping (DTW) (https://pypi.org/project/fastdtw/) as an extra feature\n",
    "5. Run clustering algorithms\n",
    "    1. Run K-means on resulting features (TSFRESH + DTW)\n",
    "        1. Run with K = 5\n",
    "        2. Run with K $\\epsilon$ [2,10]\n",
    "    2. Run Hierarchical clustering on resulting features (TSFRESH + DTW)\n",
    "        1. Run with K = 5\n",
    "        2. Run with K $\\epsilon$ [2,10]\n",
    "6. Evaluation:\n",
    "    1. Evaluate resulting clusters with sillouhette coefficient plot\n",
    "    2. Evaluate resulting clusters with elbow method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3: Feature Extraction and  Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Select context csv to work with (see above)\n",
    "2. Download TSFERSH library (https://github.com/blue-yonder/tsfresh)\n",
    "3. Run TSFRESH on dataset\n",
    "4. Calculate Dynamic time Warping (DTW) (https://pypi.org/project/fastdtw/) as an extra feature\n",
    "5. Run classification algorithms:\n",
    "    1. Append primary use type as ground truth labels from meta data **(data/meta_open.csv)**\n",
    "    2. Run Random-Forest on resulting features (TSFRESH + DTW)\n",
    "    3. Run SVM on resulting features (TSFRESH + DTW)\n",
    "6. Evaluation:\n",
    "    1. F-1 micro score using ground truth labels from metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Energy Clustering and Outlier Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file name `functions.py` contails all the functions that will be explaind and use across this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently the following datasets are in the repo:\n",
    "- Building Genome Dataset\n",
    "- Washington D.C. dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T03:51:25.858475Z",
     "start_time": "2018-12-19T03:51:25.853280Z"
    }
   },
   "outputs": [],
   "source": [
    "# functions already ran, csv files can be found in data/\n",
    "\n",
    "import functions as func\n",
    "\n",
    "# load building gnome dataset (BDG)\n",
    "# df_bdg = func.loadDataset('BDG')\n",
    "# print(\"Building Gnome Dataset: hourly meter data from {} buildings\".format(len(df_bdg.columns)))\n",
    "\n",
    "# load dc building dataset (DC)\n",
    "# df_dc = func.loadDataset('DC')\n",
    "# print(\"DC Dataset: 15min interval meter data (resampled to hourly) from {} buildings\".format(len(df_dc.columns)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this step is to make the data homogeneous by grouping the hourley read readings. Currently, the following context are being considered\n",
    "- Weekday `weekday`\n",
    "- Weekend `weekend`\n",
    "- Entire Week `entireweek`\n",
    "\n",
    "The function `extractContext(context, dataframe, datasetName)` from `functions.py` takes a time series dataframe and returns the context-related dataframe of the specified dataset. Meaning it will only keep the instances where its timestamps matches the context. The dataset name is needed because depending on it, only some specific time periods are being evaluated. For more details view the file `RawFeatures_BDG.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T03:51:42.818027Z",
     "start_time": "2018-12-19T03:51:42.812183Z"
    }
   },
   "outputs": [],
   "source": [
    "# functions already ran, csv files can be found in data/\n",
    "\n",
    "# import functions as func\n",
    "\n",
    "# df_weekday_BDG = func.getContext('weekday', df_bdg, 'BDG')\n",
    "# df_weekend_BDG = func.getContext('weekend', df_bdg, 'BDG')\n",
    "# df_weekday_DC = func.getContext('weekday', df_dc, 'DC')\n",
    "# df_weekend_DC = func.getContext('weekend', df_dc, 'DC')\n",
    "\n",
    "# df_weekday_BDG.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T03:51:45.512222Z",
     "start_time": "2018-12-19T03:51:45.500394Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_weekend_BDG.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T03:51:47.680175Z",
     "start_time": "2018-12-19T03:51:47.676828Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_weekday_DC.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T03:51:49.571810Z",
     "start_time": "2018-12-19T03:51:49.565800Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_weekend_DC.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Curves Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance aggregation of the energy consumption based on a specific context and aggregation function. Currently the following functions are implemented:\n",
    "- Average\n",
    "- Median\n",
    "- Linear Regression\n",
    "\n",
    "Up to this point, the dataframes will have the following shape:\n",
    "\n",
    "\\begin{bmatrix}%\n",
    "a_1^1 & a_2^1 & \\dots & a_m^1 \\\\\n",
    "a_1^2 & a_2^2 & \\dots & a_m^2 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a_1^n & a_2^n & \\dots & a_m^n \\\\\n",
    "\\end{bmatrix}\n",
    "\n",
    "Where `n` are the different timestamps (1hour timestamps) and `m` is the different buildings. The values themselves, `a`, are the meter readings. The goal is to aggregate all existing pair of timestamps-values of each building, based on the defined granularity, and perform an aggregation on them.\n",
    "\n",
    "For example, by calling the function `doAggregation(df_weekday_BDG, 'average', 'day', 'BDG')`, we will calculate one load curve for a dataframe from the Building Data Genome Dataset (`BDG`), where the hourly readings for each building have been added by `day`. Finally, each building will have a matrix like the following:\n",
    "\n",
    "\\begin{bmatrix}%\n",
    "b_1^1 & b_2^1 & \\dots & b_j^1 \\\\\n",
    "b_1^2 & b_2^2 & \\dots & b_j^2 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "b_1^i & b_2^i & \\dots & b_j^i \\\\\n",
    "\\end{bmatrix}\n",
    "\n",
    "Where `i` are the different days obtained for the specific building and `j` are the daily hourly timestamp, from 0 to 23 in this case, and the values `b` are the hourly readings. Finally, the aggregation function `average` will calculate the average for each hour for all calculated days. This means that a column-wise average will be calculated, resulting in one vector of shape `(1, 24)`, one building with one day worth of readings.\n",
    "\n",
    "After repeating the process for all buildings, csv will be generated (main reason why the `name` parameter is used, to distinguishing saved csv) and the returned dataframe will look like the following:\n",
    "\n",
    "\\begin{bmatrix}%\n",
    "c_1^1 & c_2^1 & \\dots & c_l^1 \\\\\n",
    "c_1^2 & c_2^2 & \\dots & c_l^2 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "c_1^k & c_2^k & \\dots & c_l^k \\\\\n",
    "\\end{bmatrix}\n",
    "\n",
    "Where `k` is the different buildings, `l` are the different hours in a `day`, and the `c` values are the representative curve calculated based on the aggregation function that was chosen, in the example `average`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T03:55:35.258218Z",
     "start_time": "2018-12-19T03:55:35.252243Z"
    }
   },
   "outputs": [],
   "source": [
    "# the following lines have already been ran and the resulting csv can be found in data/\n",
    "\n",
    "# df_average_weekday_BDG = func.\n",
    "\n",
    "# df_median_weekday_BDG = func.doAggregation(df_weekday_BDG, 'median', 'day', 'BDG')\n",
    "# df_median_weekday_BDG = func.doAggregation(df_weekday_BDG, 'linear', 'day', 'BDG')\n",
    "\n",
    "# df_average_weekday_DC = func.doAggregation(df_weekday_DC, 'average', 'day', 'DC')\n",
    "# df_median_weekday_DC = func.doAggregation(df_weekend_DC, 'median', 'day', 'DC')\n",
    "# df_median_weekday_DC = func.doAggregation(df_weekend_DC, 'linear', 'day', 'Dc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features learned using TSFRESH and DTW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Time Series Feature extraction based on scalable hypothesis tests (TSFRESH) library (https://github.com/blue-yonder/tsfresh). Additonally, another feature that will be appended will be the Dynamic Time Wrapping (DTW).\n",
    "\n",
    "It is important to highlight that TSFRESH will require the raw time series values from above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Features from existing work on BGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximately 215 features have already been extracted in previous work (https://github.com/buds-lab/temporal-features-for-nonres-buildings-library)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1: k-Shape on Raw Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Select context csv to work with (see above)\n",
    "2. Download k-Shape library (https://github.com/Mic92/kshape and https://tslearn.readthedocs.io/en/latest/gen_modules/clustering/tslearn.clustering.KShape.html#tslearn.clustering.KShape)\n",
    "3. Run k-Shape algorithm\n",
    "4. Evaluation:\n",
    "    1. Evaluate resulting clusters with sillouhette coefficient plot\n",
    "    2. Evaluate resulting clusters with elbow method\n",
    "    \n",
    "See the notebook `Experiment1_kshape.ipynb` for the actual code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: Feature Extraction and Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Select context csv to work with (see above)\n",
    "2. Download TSFERSH library (https://github.com/blue-yonder/tsfresh)\n",
    "3. Run TSFRESH on dataset\n",
    "4. Calculate Dynamic time Warping (DTW) (https://pypi.org/project/fastdtw/) as an extra feature\n",
    "5. Run clustering algorithms\n",
    "    1. Run K-means on resulting features (TSFRESH + DTW)\n",
    "        1. Run with K = 5\n",
    "        2. Run with K $\\epsilon$ [2,10]\n",
    "    2. Run Hierarchical clustering on resulting features (TSFRESH + DTW)\n",
    "        1. Run with K = 5\n",
    "        2. Run with K $\\epsilon$ [2,10]\n",
    "6. Evaluation:\n",
    "    1. Evaluate resulting clusters with sillouhette coefficient plot\n",
    "    2. Evaluate resulting clusters with elbow method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3: Feature Extraction and  Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Select context csv to work with (see above)\n",
    "2. Download TSFERSH library (https://github.com/blue-yonder/tsfresh)\n",
    "3. Run TSFRESH on dataset\n",
    "4. Calculate Dynamic time Warping (DTW) (https://pypi.org/project/fastdtw/) as an extra feature\n",
    "5. Run classification algorithms:\n",
    "    1. Append primary use type as ground truth labels from meta data **(data/meta_open.csv)**\n",
    "    2. Run Random-Forest on resulting features (TSFRESH + DTW)\n",
    "    3. Run SVM on resulting features (TSFRESH + DTW)\n",
    "6. Evaluation:\n",
    "    1. F-1 micro score using ground truth labels from metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Energy Clustering and Outlier Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file name `functions.py` contails all the functions that will be explaind and use across this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently the following datasets are in the repo:\n",
    "- Building Genome Dataset\n",
    "- Washington D.C. dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T03:51:25.858475Z",
     "start_time": "2018-12-19T03:51:25.853280Z"
    }
   },
   "outputs": [],
   "source": [
    "# functions already ran, csv files can be found in data/\n",
    "\n",
    "import functions as func\n",
    "\n",
    "# load building gnome dataset (BDG)\n",
    "# df_bdg = func.loadDataset('BDG')\n",
    "# print(\"Building Gnome Dataset: hourly meter data from {} buildings\".format(len(df_bdg.columns)))\n",
    "\n",
    "# load dc building dataset (DC)\n",
    "# df_dc = func.loadDataset('DC')\n",
    "# print(\"DC Dataset: 15min interval meter data (resampled to hourly) from {} buildings\".format(len(df_dc.columns)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this step is to make the data homogeneous by grouping the hourley read readings. Currently, the following context are being considered\n",
    "- Weekday `weekday`\n",
    "- Weekend `weekend`\n",
    "- Entire Week `entireweek`\n",
    "\n",
    "The function `extractContext(context, dataframe, datasetName)` from `functions.py` takes a time series dataframe and returns the context-related dataframe of the specified dataset. Meaning it will only keep the instances where its timestamps matches the context. The dataset name is needed because depending on it, only some specific time periods are being evaluated. For more details view the file `RawFeatures_BDG.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T03:51:42.818027Z",
     "start_time": "2018-12-19T03:51:42.812183Z"
    }
   },
   "outputs": [],
   "source": [
    "# functions already ran, csv files can be found in data/\n",
    "\n",
    "# import functions as func\n",
    "\n",
    "# df_weekday_BDG = func.getContext('weekday', df_bdg, 'BDG')\n",
    "# df_weekend_BDG = func.getContext('weekend', df_bdg, 'BDG')\n",
    "# df_weekday_DC = func.getContext('weekday', df_dc, 'DC')\n",
    "# df_weekend_DC = func.getContext('weekend', df_dc, 'DC')\n",
    "\n",
    "# df_weekday_BDG.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T03:51:45.512222Z",
     "start_time": "2018-12-19T03:51:45.500394Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_weekend_BDG.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T03:51:47.680175Z",
     "start_time": "2018-12-19T03:51:47.676828Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_weekday_DC.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T03:51:49.571810Z",
     "start_time": "2018-12-19T03:51:49.565800Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_weekend_DC.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Curves Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance aggregation of the energy consumption based on a specific context and aggregation function. Currently the following functions are implemented:\n",
    "- Average\n",
    "- Median\n",
    "- Linear Regression\n",
    "\n",
    "Up to this point, the dataframes will have the following shape:\n",
    "\n",
    "\\begin{bmatrix}%\n",
    "a_1^1 & a_2^1 & \\dots & a_m^1 \\\\\n",
    "a_1^2 & a_2^2 & \\dots & a_m^2 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a_1^n & a_2^n & \\dots & a_m^n \\\\\n",
    "\\end{bmatrix}\n",
    "\n",
    "Where `n` are the different timestamps (1hour timestamps) and `m` is the different buildings. The values themselves, `a`, are the meter readings. The goal is to aggregate all existing pair of timestamps-values of each building, based on the defined granularity, and perform an aggregation on them.\n",
    "\n",
    "For example, by calling the function `doAggregation(df_weekday_BDG, 'average', 'day', 'BDG')`, we will calculate one load curve for a dataframe from the Building Data Genome Dataset (`BDG`), where the hourly readings for each building have been added by `day`. Finally, each building will have a matrix like the following:\n",
    "\n",
    "\\begin{bmatrix}%\n",
    "b_1^1 & b_2^1 & \\dots & b_j^1 \\\\\n",
    "b_1^2 & b_2^2 & \\dots & b_j^2 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "b_1^i & b_2^i & \\dots & b_j^i \\\\\n",
    "\\end{bmatrix}\n",
    "\n",
    "Where `i` are the different days obtained for the specific building and `j` are the daily hourly timestamp, from 0 to 23 in this case, and the values `b` are the hourly readings. Finally, the aggregation function `average` will calculate the average for each hour for all calculated days. This means that a column-wise average will be calculated, resulting in one vector of shape `(1, 24)`, one building with one day worth of readings.\n",
    "\n",
    "The specific code snippet for each aggregation function is as follows:\n",
    "\n",
    "\n",
    "```python\n",
    "# calculate load curve based on function\n",
    "if function == 'average':\n",
    "    load_curve = np.mean(df_sampledReadings, axis = 0)\n",
    "\n",
    "elif function =='median':\n",
    "    load_curve = np.median(df_sampledReadings, axis = 0)\n",
    "\n",
    "elif function == 'regression':\n",
    "    # 1. Generate one single time series for the entire building\n",
    "    df_one_ts = pd.DataFrame() # empty data frame to hold complete time series\n",
    "    df_trans = df_sampledReadings.T\n",
    "    # iterate through each day worth of readings\n",
    "    for column in range(len(df_trans.columns)):\n",
    "        currentColumn = pd.DataFrame(df_trans.iloc[:, column])\n",
    "        df_one_ts = df_one_ts.append(currentColumn, ignore_index=True)\n",
    "    # rename variables            \n",
    "    x_values = df_one_ts.index.values.reshape(-1, 1)\n",
    "    y_values = df_one_ts.values\n",
    "\n",
    "    # 2. Perform polynomial regressions on the single time series,\n",
    "    # the curve with the lowest Root-mean square error (RMSE) will be kept\n",
    "    degrees = range(1, 21)\n",
    "    base_model = linear_model.LinearRegression().fit(x_values, y_values)\n",
    "    base_curve = base_model.predict(x_values)\n",
    "    rmse = np.sqrt(mean_squared_error(y_values, base_curve))\n",
    "    load_curve = base_curve\n",
    "\n",
    "    for d in degrees: # fit a curve for each degree\n",
    "        polynomial_features= PolynomialFeatures(degree=d)\n",
    "        x_poly = polynomial_features.fit_transform(x_values)    \n",
    "        poly_model = linear_model.LinearRegression()\n",
    "        poly_model.fit(x_poly, y_values)\n",
    "        poly_curve = poly_model.predict(x_poly)\n",
    "        rmse_d = np.sqrt(mean_squared_error(y_values,poly_curve))\n",
    "        \n",
    "        # keep the polynomial with lowest RSME\n",
    "        if rmse_d < rmse :\n",
    "            rmse = rmse_d\n",
    "            load_curve = poly_curve    \n",
    "```\n",
    "\n",
    "\n",
    "After repeating the process for all buildings, csv will be generated (main reason why the `name` parameter is used, to distinguishing saved csv) and the returned dataframe will look like the following:\n",
    "\n",
    "\\begin{bmatrix}%\n",
    "c_1^1 & c_2^1 & \\dots & c_l^1 \\\\\n",
    "c_1^2 & c_2^2 & \\dots & c_l^2 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "c_1^k & c_2^k & \\dots & c_l^k \\\\\n",
    "\\end{bmatrix}\n",
    "\n",
    "Where `k` is the different buildings, `l` are the different hours in a `day`, and the `c` values are the representative curve calculated based on the aggregation function that was chosen, in the example `average`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T03:55:35.258218Z",
     "start_time": "2018-12-19T03:55:35.252243Z"
    }
   },
   "outputs": [],
   "source": [
    "# calculate load curves based on aggregation functions\n",
    "\n",
    "# df_average_weekday_BDG = func.doAggregation(df_weekday_BDG, contexts[0], aggregation_functions[0], 'day', datasets[0])\n",
    "# df_median_weekday_BDG = func.doAggregation(df_weekday_BDG, contexts[0], aggregation_functions[1], 'day', datasets[0])\n",
    "# df_regression_weekday_BDG = func.doAggregation(df_weekday_BDG, contexts[0], aggregation_functions[2], 'day', datasets[0])\n",
    "# df_average_weekend_BDG = func.doAggregation(df_weekend_BDG, contexts[1], aggregation_functions[0], 'day', datasets[0])\n",
    "# df_median_weekend_BDG = func.doAggregation(df_weekend_BDG, contexts[1], aggregation_functions[1], 'day', datasets[0])\n",
    "# df_regression_weekend_BDG = func.doAggregation(df_weekend_BDG, contexts[0], aggregation_functions[2], 'day', datasets[0])\n",
    "\n",
    "# df_average_weekday_DC = func.doAggregation(df_weekday_DC, contexts[0], aggregation_functions[0], 'day', datasets[1])\n",
    "# df_median_weekday_DC = func.doAggregation(df_weekday_DC, contexts[0], aggregation_functions[1], 'day', datasets[1])\n",
    "# df_regression_weekday_DC = func.doAggregation(df_weekday_DC, contexts[0], aggregation_functions[2], 'day', datasets[1])\n",
    "# df_average_weekend_DC = func.doAggregation(df_weekend_DC, contexts[1], aggregation_functions[0], 'day', datasets[1])\n",
    "# df_median_weekend_DC = func.doAggregation(df_weekend_DC, contexts[1], aggregation_functions[1], 'day', datasets[1])\n",
    "# df_regression_weekend_DC = func.doAggregation(df_weekend_DC, contexts[1], aggregation_functions[2], 'day', datasets[1])\n",
    "\n",
    "# functions already ran, csv files can be found in data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features learned using TSFRESH and DTW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Time Series Feature extraction based on scalable hypothesis tests (TSFRESH) library (https://github.com/blue-yonder/tsfresh). Additonally, another feature that will be appended will be the Dynamic Time Wrapping (DTW).\n",
    "\n",
    "It is important to highlight that TSFRESH will require the raw time series values from above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Features from existing work on BGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximately 215 features have already been extracted in previous work (https://github.com/buds-lab/temporal-features-for-nonres-buildings-library)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1: k-Shape on Raw Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Select context csv to work with (see above)\n",
    "2. Download k-Shape library (https://github.com/Mic92/kshape and https://tslearn.readthedocs.io/en/latest/gen_modules/clustering/tslearn.clustering.KShape.html#tslearn.clustering.KShape)\n",
    "3. Run k-Shape algorithm\n",
    "4. Evaluation:\n",
    "    1. Evaluate resulting clusters with sillouhette coefficient plot\n",
    "    2. Evaluate resulting clusters with elbow method\n",
    "    \n",
    "See the notebook `Experiment1_kshape.ipynb` for the actual code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: Feature Extraction and Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Select context csv to work with (see above)\n",
    "2. Download TSFERSH library (https://github.com/blue-yonder/tsfresh)\n",
    "3. Run TSFRESH on dataset\n",
    "4. Calculate Dynamic time Warping (DTW) (https://pypi.org/project/fastdtw/) as an extra feature\n",
    "5. Run clustering algorithms\n",
    "    1. Run K-means on resulting features (TSFRESH + DTW)\n",
    "        1. Run with K = 5\n",
    "        2. Run with K $\\epsilon$ [2,10]\n",
    "    2. Run Hierarchical clustering on resulting features (TSFRESH + DTW)\n",
    "        1. Run with K = 5\n",
    "        2. Run with K $\\epsilon$ [2,10]\n",
    "6. Evaluation:\n",
    "    1. Evaluate resulting clusters with sillouhette coefficient plot\n",
    "    2. Evaluate resulting clusters with elbow method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3: Feature Extraction and  Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Select context csv to work with (see above)\n",
    "2. Download TSFERSH library (https://github.com/blue-yonder/tsfresh)\n",
    "3. Run TSFRESH on dataset\n",
    "4. Calculate Dynamic time Warping (DTW) (https://pypi.org/project/fastdtw/) as an extra feature\n",
    "5. Run classification algorithms:\n",
    "    1. Append primary use type as ground truth labels from meta data **(data/meta_open.csv)**\n",
    "    2. Run Random-Forest on resulting features (TSFRESH + DTW)\n",
    "    3. Run SVM on resulting features (TSFRESH + DTW)\n",
    "6. Evaluation:\n",
    "    1. F-1 micro score using ground truth labels from metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to determine th goodness of the different clusters, the following calculations were executed and from them, the right K was chosen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Cohesion** calculates the sum of squared distances from each data point to its respective centroid. **LOWER THE BETTER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Separation** calculation leverages the fact that the following equation always holds true: TSS = WSS + BSS\n",
    "\n",
    "where TSS is the total sum of squared distances from each data point to the overall centroid. WSS is cohesion and BSS is separation. **LOWER THE BETTER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The **Calinski-Harabasz index (CH)** evaluates the cluster validity based on the average between- and within- cluster sum of squares. It is a ratio of cohesion and separation adjusted by the respective degrees of freedom. **HIGHER THE BETTER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Davies-Bouldin index** iterates every cluster and calculates a statistic using DB_find_max_j. Then, it averages the statistic over all clusters.\n",
    "\n",
    "The statistic is calculated as follows:\n",
    "\n",
    "- For every other cluster, calculate the average distance of every point in that cluster to its centroid\n",
    "- Also calculate the average distance of every point in the current cluster to the centroid\n",
    "- Add the two values together\n",
    "- Divide the sum by the Euclidean distance between the two cluster centroids and obtain a candidate value\n",
    "- Find the maximum value among all the candidate values for every other cluster to be the statistic\n",
    "\n",
    "**The smaller the index is, the better the clustering result is**. By minimizing this index, clusters are the most distinct from each other, and therefore achieves the best partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **R-squared** can be expressed in terms of separation and cohesion as follows: R-squared = separation / (cohesion + separation) **HIGHER BETTER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **RMSSTD** first computes the the sum of squared distances from each data point to its respective centroid, which is SSE or cohesion. Then, it divides the value by the product of the number of attributes and the degree of freedom, which is calculated as the number of data points minus the number of clusters. Lastly, we take the square root of the value to obtain RMSSTD. **LOWER THE BETTER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Xie-Beni** index first calculates SSE or cohesion by taking the sum of the squared distances from each data point to its respective centroid. We denote this by A. Then, it finds the minimum pairwise squared distances between cluster centroids. We denote this by B. We denote the number of data points as n. Xie-Beni index is calculated as A / (n*B).\n",
    "\n",
    "The Xie-Beni index defines the inter-cluster separation as the minimum square distance between cluster centers, and the intra-cluster compactness as the mean square distance between each data object and its cluster center. **The optimal cluster number is reached when the minimum of Xie-Beni index is found.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result pictures can be seen in `img/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing of K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to choose the right K, we list the top 3 K values for each metric. Then, we count how many times each different K value appears in those top 3 for the 7 different metrics. Finally, we choose the K that has the highest repetition among them.\n",
    "\n",
    "If a tie occurs, all K are considered for a next step evaluation, for which kShape is ran for those K candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T11:57:03.237939Z",
     "start_time": "2018-12-21T11:57:03.235541Z"
    }
   },
   "source": [
    "# This notebook will be the only interaction with all any experiment notebook. From here the function calls to all the preprocessing and clustering notebooks will be done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T10:17:45.561980Z",
     "start_time": "2018-12-23T10:17:45.559252Z"
    }
   },
   "outputs": [],
   "source": [
    "# Libraries needed\n",
    "# !pip install nbimporter # uncomment if library is not install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-29T01:43:13.526314Z",
     "start_time": "2019-01-29T01:43:08.557768Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Jupyter notebook from /Users/matias/Documents/Education/Graduate/NUS/Projects/sensor-cluster-er/Preprocessing/preprocessing.ipynb\n",
      "Importing Jupyter notebook from /Users/matias/Documents/Education/Graduate/NUS/Projects/sensor-cluster-er/Preprocessing/context_extraction.ipynb\n",
      "Importing Jupyter notebook from /Users/matias/Documents/Education/Graduate/NUS/Projects/sensor-cluster-er/Preprocessing/load_cuve_generation.ipynb\n",
      "Importing Jupyter notebook from /Users/matias/Documents/Education/Graduate/NUS/Projects/sensor-cluster-er/ClusteringAnalysis/ClusteringValidationMetrics.ipynb\n",
      "Importing Jupyter notebook from /Users/matias/Documents/Education/Graduate/NUS/Projects/sensor-cluster-er/ClusteringAnalysis/ClusteringAlgorithms.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Existing Notebooks\n",
    "import nbimporter\n",
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from Preprocessing.preprocessing import hourly_dataset\n",
    "from Preprocessing.context_extraction import getContext\n",
    "from Preprocessing.load_cuve_generation import doAggregation\n",
    "\n",
    "from ClusteringAnalysis.ClusteringValidationMetrics import get_validation_scores\n",
    "from ClusteringAnalysis.ClusteringAlgorithms import doClustering\n",
    "\n",
    "# Built-in libraries\n",
    "import time\n",
    "from itertools import product\n",
    "from math import log\n",
    "import pickle\n",
    "from datetime import datetime, timedelta # testing\n",
    "from collections import OrderedDict\n",
    "from math import ceil\n",
    "from itertools import cycle\n",
    "\n",
    "# NumPy, SciPy and Pandas\n",
    "from scipy.spatial.distance import cdist, euclidean\n",
    "from scipy.stats import zscore\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Scikit-Learn\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "# Tslearn\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "from tslearn.utils import to_time_series_dataset\n",
    "# from tslearn.clustering import silhouette_score\n",
    "\n",
    "# Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.gridspec as gridspec\n",
    "import plotly.graph_objs as go\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T11:01:20.315851Z",
     "start_time": "2018-12-23T11:01:20.307773Z"
    }
   },
   "outputs": [],
   "source": [
    "# check files\n",
    "def checkFiles(datasetName, context, function):\n",
    "    # if the dataset is the combination, directly load the aggregated dataset for the datasets\n",
    "    if datasetName == 'BDG-DGS':\n",
    "        df1_name = 'BDG'\n",
    "        df2_name = 'DGS'\n",
    "\n",
    "        df1 = pd.read_csv('../data/processed/{}_{}_{}_dataset.csv'.format(df1_name, context, function), index_col=0)\n",
    "        df2 = pd.read_csv('../data/processed/{}_{}_{}_dataset.csv'.format(df2_name, context, function), index_col=0)\n",
    "    \n",
    "        df = df1.append(df2)\n",
    "        df.to_csv('../data/processed/{}_{}_{}_dataset.csv'.format(datasetName, context, function))\n",
    "    else:\n",
    "        # check if dataset has already being processed before\n",
    "        exists_df = os.path.isfile('../data/processed/{}_dataset.csv'.format(datasetName))\n",
    "        if exists_df: # if file exists, read it\n",
    "            df = pd.read_csv('../data/processed/{}_dataset.csv'.format(datasetName), index_col=0)\n",
    "            print(\"Preprocessed dataset already exists, loading it ...\")\n",
    "        else: # if file is missing, produce it\n",
    "            df = hourly_dataset(datasetName)\n",
    "            print(\"Preprocessing dataset ...\")\n",
    "\n",
    "        # check if dataset with context has already being processed before\n",
    "        exists_context = os.path.isfile('../data/processed/{}_{}_dataset.csv'.format(datasetName, context))\n",
    "        if exists_context: # if file exists, read it\n",
    "            df = pd.read_csv('../data/processed/{}_{}_dataset.csv'.format(datasetName, context), index_col=0)\n",
    "            print(\"Dataset with {} context already exists, loading it ...\".format(context))\n",
    "        else: # if file is missing, produce it\n",
    "            df = getContext(datasetName, context)\n",
    "            print(\"Generating context dataset ...\")\n",
    "\n",
    "        # check if dataset with function has already being processed before\n",
    "        exists_function = os.path.isfile('../data/processed/{}_{}_{}_dataset.csv'.format(datasetName, context, function))\n",
    "        if exists_function: # if file exists, read it\n",
    "            df = pd.read_csv('../data/processed/{}_{}_{}_dataset.csv'.format(datasetName, context, function), index_col=0)\n",
    "            print(\"Dataset with {} context and {} load curve aggregation function already exists, loading it ...\".format(context, function))\n",
    "\n",
    "        else: # if file is missing, produce it\n",
    "            df = doAggregation(datasetName, context, function)\n",
    "            print(\"Generating load curves based on {} ...\".format(function))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T04:07:48.500160Z",
     "start_time": "2018-12-27T04:07:48.471459Z"
    }
   },
   "outputs": [],
   "source": [
    "# Running Experiment \n",
    "def runExperiment(datasetName, context, function, algorithm='kshape', \n",
    "                  algo_parameter=range(2,11), validation_metrics='all', appendTotalFile=False):\n",
    "    print(\"Running Experiment with dataset: {}, context:  {}, function: {}, algorithm: {}\".format(datasetName, \n",
    "                                                                                                context,\n",
    "                                                                                                function,\n",
    "                                                                                                algorithm))\n",
    "    # check if file exists and load it\n",
    "    df = checkFiles(datasetName, context, function)\n",
    "    \n",
    "    # Z-normalize the data\n",
    "    df_scaled = TimeSeriesScalerMeanVariance(mu=0.0, std=1.0).fit_transform(df.values)\n",
    "    # df.values will generate a 3d array (the third dimension being 1) so we convert it to a 2d array\n",
    "    df_scaled = np.squeeze(df_scaled) # now is a 2d array\n",
    "    \n",
    "    scores = [] # list of the scores for each parameter for the selected algorithm\n",
    "    \n",
    "    # run selected algorithm with the appropiate parameter\n",
    "    for k in algo_parameter:\n",
    "        model, labels = doClustering(df_scaled, datasetName, algorithm, k, seed=3, max_iter=200)\n",
    "        scores.append(get_validation_scores(df_scaled, labels))\n",
    "        print(\"Running experiment with {} and k = {}\".format(algorithm, k))\n",
    "    \n",
    "    # name for saving the results\n",
    "    obj_name = '../data/results/{}_{}_{}_{}_scores'.format(datasetName, context, function, algorithm)\n",
    "    \n",
    "    # update the final score dataframe\n",
    "    scores = pd.DataFrame.from_dict(scores)\n",
    "    scores.insert(0, 'dataset', '')\n",
    "    scores['dataset'] = datasetName\n",
    "    scores.insert(1, 'context', '')\n",
    "    scores['context'] = context\n",
    "    scores.insert(2, 'function', '')\n",
    "    scores['function'] = function\n",
    "    scores.insert(3, 'algorithm', '')\n",
    "    scores['algorithm'] = algorithm\n",
    "    if \"k\" in algorithm or algorithm == 'hierarchical':\n",
    "        scores.insert(4, 'parameter k', '')\n",
    "        scores['parameter k'] = algo_parameter\n",
    "    \n",
    "    # approximate to two decimals\n",
    "    scores = scores.round(2)\n",
    "    \n",
    "    # save as python pickle\n",
    "    f = open(obj_name + '.pkl', 'wb')\n",
    "    pickle.dump(scores, f)\n",
    "    f.close\n",
    "    \n",
    "    # save as csv\n",
    "    scores.to_csv('{}.csv'.format(obj_name))\n",
    "    print(\"Scores saved in {}.csv\\n\".format(obj_name)) # individual file\n",
    "    \n",
    "    if appendTotalFile:\n",
    "        with open('../data/results/total_scores.csv', 'a') as f: # append to general file\n",
    "            scores.to_csv(f, header=False)\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run batch of experiments\n",
    "def runBatchExperiments(datasetName_list, context_list, function_list, algorithm_list, \n",
    "                  algo_parameter=range(2,11), validation_metrics='all'):\n",
    "    # nested loops for each possible combination of the lists\n",
    "    for datasetName in datasetName_list: # every dataset\n",
    "        for context in context_list: # every context\n",
    "            for function in function_list: # every load aggregation function\n",
    "                for algorithm in algorithm_list: # every algorithm\n",
    "                    runExperiment(datasetName, context, function, algorithm, algo_parameter, validation_metrics, True)\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T10:17:48.118758Z",
     "start_time": "2018-12-23T10:17:48.115255Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generating clusters and centroids\n",
    "def generateClusters(datasetName, context, function, algorithm='kshape', algo_parameter = 5, psu=False):\n",
    "    # check if file exists and load it\n",
    "    df = checkFiles(datasetName, context, function)\n",
    "        \n",
    "    # Z-normalize the data\n",
    "    df_scaled = TimeSeriesScalerMeanVariance(mu=0.0, std=1.0).fit_transform(df.values)\n",
    "    # df.values will generate a 3d array (the third dimension being 1) so we convert it to a 2d array\n",
    "    df_scaled = np.squeeze(df_scaled) # now is a 2d array\n",
    "        \n",
    "    # run selected algorithm with the appropiate parameter\n",
    "    for k in algo_parameter:\n",
    "        model, labels = doClustering(df_scaled, datasetName, algorithm, k, seed=3, max_iter=300, plot=True)    \n",
    "    plt.show()\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateClustersPSU(datasetName, context, function, algorithm='kshape', algo_parameter = 5):\n",
    "    # load metadata\n",
    "    if datasetName == 'BDG':\n",
    "        df_meta = pd.read_csv('../data/raw/meta_open.csv')\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "    # check if file exists and load it\n",
    "    df = checkFiles(datasetName, context, function)\n",
    "        \n",
    "    # Z-normalize the data\n",
    "    df_scaled = TimeSeriesScalerMeanVariance(mu=0.0, std=1.0).fit_transform(df.values)\n",
    "    # df.values will generate a 3d array (the third dimension being 1) so we convert it to a 2d array\n",
    "    df_scaled = np.squeeze(df_scaled) # now is a 2d array\n",
    "         \n",
    "    # get labels for all buildings\n",
    "    ground_truth_labels = df_meta[df_meta.iloc[:, 0].isin(df.index.values)]\n",
    "    ground_truth_labels = ground_truth_labels['primaryspaceusage']\n",
    "    ground_truth_labels = ground_truth_labels.reset_index(drop=True)\n",
    "    \n",
    "    # back to pandas\n",
    "#     df_scaled = pd.DataFrame(df_scaled)\n",
    "#     df_scaled['psu'] = ground_truth_labels\n",
    "            \n",
    "    ground_truth_list = ['Office', 'Dormitory', 'College Classroom', 'Primary/Secondary Classroom', \n",
    "                        'College Laboratory']\n",
    "    colors = ['r', 'g', 'b', 'k', 'y']\n",
    "    \n",
    "    # run selected algorithm with the appropiate parameter\n",
    "    for k in algo_parameter:\n",
    "        model, labels = doClustering(df_scaled, datasetName, algorithm, k, seed=3, max_iter=300)\n",
    "        \n",
    "        if algorithm == 'kshape':\n",
    "            # extract parameters\n",
    "            cluster_centers = []\n",
    "            y_pred = pd.DataFrame()\n",
    "            y_pred.loc[:, 0] = [0] * df_scaled.shape[0] # initilize cluster membership\n",
    "            # for each tuple\n",
    "            for yi in range(k):\n",
    "                cluster_centers.append(model[yi][0]) # get cluster centers as first element of tuple\n",
    "                y_pred.loc[model[yi][1], 0] = yi # update cluster membership\n",
    "            # make them a list\n",
    "            y_pred = y_pred.iloc[:, 0].values \n",
    "        else:\n",
    "            y_pred = model.fit_predict(df_scaled) # fit the data and generate the cluster labels\n",
    "        \n",
    "        # back to pandas\n",
    "        df_scaled = pd.DataFrame(df_scaled)\n",
    "        df_scaled['psu'] = ground_truth_labels\n",
    "\n",
    "        # plot for each cluster\n",
    "        fig = plt.figure(figsize=(20, 40))\n",
    "\n",
    "        for yi in range(k):\n",
    "            plt.subplot(k, 1, 1 + yi)\n",
    "            \n",
    "            # for each time series in current cluster\n",
    "            for index, building in df_scaled[y_pred == yi].iterrows():       \n",
    "                if building[-1] == 'Office':\n",
    "                    idx = 0\n",
    "                elif building[-1] == 'Dormitory':\n",
    "                    idx = 1\n",
    "                elif building[-1] == 'College Classroom':\n",
    "                    idx = 2\n",
    "                elif building[-1] == 'Primary/Secondary Classroom':\n",
    "                    idx = 3\n",
    "                else:\n",
    "                    idx = 4\n",
    "\n",
    "                plt.plot(building[:-1], \"-\", alpha=0.25, label = building[-1], c = colors[idx])\n",
    "\n",
    "                plt.xlim(0, 23)\n",
    "            plt.ylim(-4, 4)\n",
    "            plt.title(\"Cluster %d\" % (yi + 1), fontsize = 30)\n",
    "                # take care of repeating label and group them\n",
    "            handles, labels = plt.gca().get_legend_handles_labels()\n",
    "            by_label = OrderedDict(zip(labels, handles))\n",
    "            plt.legend(by_label.values(), by_label.keys(), loc='center left', bbox_to_anchor=(1, 0.55), prop={'size': 30})\n",
    "\n",
    "        fig.suptitle(\"Dataset: {}\".format(datasetName), fontsize = 35)\n",
    "                \n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotSilhouette(datasetName, context, function, algorithm, k):\n",
    "    plt.ioff() # this way only plt.show() will display figures\n",
    "    \n",
    "    df = pd.read_csv('../data/processed/{}_{}_{}_dataset.csv'.format(datasetName, context, function), index_col=0)\n",
    "     \n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, ax1 = plt.subplots(1, 1)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (k+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(df.values) + (k + 1) * 10])\n",
    "\n",
    "    df_scaled = TimeSeriesScalerMeanVariance(mu=0.0, std=1.0).fit_transform(df.values)\n",
    "    df_scaled = np.squeeze(df_scaled)\n",
    "    \n",
    "    clusterer, cluster_labels = doClustering(df_scaled, datasetName, algorithm, k, seed=3, max_iter=300)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    \n",
    "    silhouette_avg = round(silhouette_score(df_scaled, cluster_labels), 2) # round to two decimals\n",
    "    print(\"For k =\", k, \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(df_scaled, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(k):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / k)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i), fontsize = 30)\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "#     ax1.set_title(\"Silhouette analysis for {} {} using {}\".format(context, function, algorithm), fontsize = 30)\n",
    "    ax1.set_xlabel(\"Silhouette coefficient values\", fontsize = 30)\n",
    "#     ax1.set_xlabel(\"\", fontsize = 20)\n",
    "    ax1.set_ylabel(\"Cluster label\", fontsize = 30)\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.3,-0.2, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    plt.rc('xtick',labelsize=30)\n",
    "    plt.rc('ytick',labelsize=30)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=30)\n",
    "    plt.show()\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T01:13:57.857233Z",
     "start_time": "2019-01-11T01:13:57.850923Z"
    }
   },
   "outputs": [],
   "source": [
    "def batchPlotRawTimeSeries(dataset_list, context_list, function_list, resolution='day', plotType='a'):\n",
    "    for dataset in dataset_list:\n",
    "        for context in context_list:\n",
    "            if plotType == 'a':\n",
    "                plotRawTimeSeries(dataset, context, 'average', resolution, plotType)\n",
    "            else:\n",
    "                for function in function_list:\n",
    "                    plotRawTimeSeries(dataset, context, function, resolution, plotType)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T03:37:10.535433Z",
     "start_time": "2019-01-15T03:37:10.525425Z"
    }
   },
   "outputs": [],
   "source": [
    "def rawTimeSeriesL2(df, z_profile_curve):\n",
    "    score_list = []\n",
    "    for _, curve in df.iterrows():\n",
    "        z_curve = curve.copy()\n",
    "        z_curve = zscore(z_curve)\n",
    "        \n",
    "        # double check there are no NaNs\n",
    "        where_are_NaNs = np.isnan(z_curve)\n",
    "        z_curve[where_are_NaNs] = 0\n",
    "        where_are_NaNs = np.isnan(z_profile_curve)\n",
    "        z_profile_curve[where_are_NaNs] = 0\n",
    "        score_list.append(euclidean(z_profile_curve, z_curve))\n",
    "        \n",
    "    # normalize the metric\n",
    "#     print(score_list)\n",
    "    if (max(score_list) - min(score_list)) == 0:\n",
    "        y = 0\n",
    "    else:\n",
    "        z_score_list = [(i - min(score_list))/(max(score_list) - min(score_list)) for i in score_list]     \n",
    "        y = sum(z_score_list)\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. Raw time series for each dataset    \n",
    "a. raw load curves for all days plotted together\n",
    "b. raw load curves for all days with represetative load curve when using different aggregation functions\n",
    "c. subplots for context and algo, with metric displayed and table of them generated\n",
    "\"\"\"\n",
    "\n",
    "def plotRawTimeSeries(datasetName, context, function, resolution='day', plotType='a'):\n",
    "    dataframe = pd.read_csv('../data/processed/{}_{}_dataset.csv'.format(datasetName, context), parse_dates=True, \n",
    "                            infer_datetime_format=True, index_col=0)\n",
    "\n",
    "    # resample based on parameter\n",
    "    if (resolution == 'day'):\n",
    "        availableSamples = (dataframe.resample('1D').asfreq()).index # get list of timestamps group by day\n",
    "        delta = 23 # timedelta based on resample\n",
    "    else:\n",
    "        print(\"Please choose a valid resolution\")\n",
    "        exit()\n",
    "    \n",
    "    # iterate through all buildings (column)\n",
    "    for column in range(len(dataframe.columns)):\n",
    "        df_sampledReadings = pd.DataFrame() # dataframe to hold new samples for a column\n",
    "        currentColumn = pd.DataFrame(dataframe.iloc[:, column])\n",
    "        \n",
    "        # iterate through each day\n",
    "        for timestamp in availableSamples:\n",
    "            # update time limits to the current date\n",
    "            start = timestamp\n",
    "            end = timestamp + timedelta(hours=delta)\n",
    "            # get meter data from only this resolution\n",
    "            df_reading = currentColumn[(currentColumn.index >= start) & (currentColumn.index <= end)]\n",
    "            # ignore index since they are unique timestamps\n",
    "            df_reading.reset_index(drop=True, inplace=True)         \n",
    "            # append new sample as columns\n",
    "            df_sampledReadings = pd.concat([df_sampledReadings, df_reading], axis=1)\n",
    "            \n",
    "        # make sure sure there are no columns with NaN values\n",
    "        df_sampledReadings.dropna(axis=1, how='all', inplace=True)\n",
    "        df_sampledReadings = df_sampledReadings.T # transpose it so it's easier to see and operate\n",
    "        # up to this point, the matrix above has the shape nxm where is the number of instances and m is the number of readings\n",
    "    \n",
    "        # if any NaN prevailed\n",
    "        df_sampledReadings.fillna(value=0, inplace=True) \n",
    "\n",
    "        # calculate load curve based on function\n",
    "        if function == 'average':\n",
    "            load_curve = np.mean(df_sampledReadings, axis = 0)\n",
    "\n",
    "        elif function =='median':\n",
    "            load_curve = np.median(df_sampledReadings, axis = 0)\n",
    "\n",
    "        else:\n",
    "            print(\"Please choose a valid context\")\n",
    "            exit()\n",
    "\n",
    "        z_load_curve = load_curve.copy()\n",
    "        z_load_curve = zscore(z_load_curve)\n",
    "\n",
    "        ###################################################################\n",
    "        # a. raw load curves for all days plotted together\n",
    "        if plotType != 'c':\n",
    "            fig = plt.figure(figsize=(18,10))\n",
    "            plt.ylim(-5,5)\n",
    "            x_axis = range(0, len(df_sampledReadings.columns))\n",
    "\n",
    "            for _, curve in df_sampledReadings.iterrows():\n",
    "                z_curve = curve.copy()\n",
    "                z_curve = zscore(z_curve)\n",
    "                plt.plot(z_curve, \"k-\", alpha=.2)\n",
    "        \n",
    "        if plotType =='a':\n",
    "            plt.title(\"Load Profiles for all available days for dataset {}, building {}, and context {}\".\n",
    "                      format(datasetName, column, context))\n",
    "            fig.savefig(\"../data/plots/rawtimeseries/a/{}/{}_{}_{}.png\".format(context, datasetName, column, context))\n",
    "            plt.show()\n",
    "        \n",
    "        # b. raw load curves for all days with represetative load curve when using different aggregation functions\n",
    "        elif plotType == 'b':\n",
    "            plt.plot(z_load_curve, \"r-\", linewidth=7.0)\n",
    "            plt.title(\"Load Profiles for all available days for dataset {}, building {}, context {}, and function {}\".\n",
    "                  format(datasetName, column, context, function))\n",
    "            fig.savefig(\"../data/plots/rawtimeseries/b/{}/{}/{}_{}_{}_{}.png\".format(context, function, datasetName, column, context, function))\n",
    "            plt.show()\n",
    "                \n",
    "        elif plotType == 'c':\n",
    "            score_table = pd.DataFrame(columns=['building', 'contex', 'function', 'metric'])\n",
    "            metric = rawTimeSeriesL2(df_sampledReadings, z_load_curve)\n",
    "            score_table = score_table.append({'building': df_sampledReadings.index[0],\n",
    "                                              'contex': context,\n",
    "                                              'function': function,\n",
    "                                              'metric': format(metric, '.2f'),\n",
    "                                             }, ignore_index=True)\n",
    "            print(metric) # debugging\n",
    "            # append to general file\n",
    "            with open('../data/tables/timeseriesScores_{}.csv'.format(datasetName), 'a') as f:\n",
    "                score_table.to_csv(f, header=False)\n",
    "        \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTimeSeriesScore(datasetName, histogram=True):\n",
    "    # load data\n",
    "    df = pd.read_csv(\"../data/tables/timeseriesScores_{}.csv\".format(datasetName), index_col=0)\n",
    "    \n",
    "    functions = ['average', 'median']\n",
    "    contexts = ['weekday', 'weekend', 'fullweek']\n",
    "    \n",
    "    # Four axes, returned as a 2-d array\n",
    "    f, axarr = plt.subplots(2, 3, figsize=(20, 18))\n",
    "    f.subplots_adjust(hspace=0.05, wspace=0.05)\n",
    "    \n",
    "    for i in range(2): # rows (functions)\n",
    "        df_row = df[df['function'] == functions[i]] # filter by function\n",
    "        for j in range(3): # columns (contexts) \n",
    "            df_column = df_row[df_row['context'] == contexts[j]] # filter by context\n",
    "            df_column.reset_index(drop=True, inplace=True)\n",
    "            \n",
    "            y_values = pd.to_numeric(df_column['metric'])\n",
    "            \n",
    "            # raw values\n",
    "            if not histogram:\n",
    "                if datasetName == 'DGS':\n",
    "                    axarr[i, j].set_ylim([0, 500])\n",
    "\n",
    "                else:\n",
    "                    axarr[i, j].set_ylim([0, 200])\n",
    "                    \n",
    "                threshold = y_values.quantile(0.9) # 90th percentile\n",
    "                axarr[i, j].axhline(threshold, linewidth=2, color='r', linestyle=\"--\")\n",
    "                y_values = np.sort(y_values)\n",
    "                axarr[i, j].plot(y_values)\n",
    "            # histogram\n",
    "            else:\n",
    "                axarr[i, j].hist(y_values)\n",
    "\n",
    "    # Fine-tune figure; hide x ticks for top plots and y ticks for right plots\n",
    "    for ax, col in zip(axarr[0], contexts):\n",
    "        ax.set_title(col, fontsize = 30)\n",
    "\n",
    "    for ax, row in zip(axarr[:,0], functions):\n",
    "        ax.set_ylabel(row, rotation=90, fontsize=30)\n",
    "\n",
    "    plt.rc('xtick',labelsize=20)\n",
    "    plt.rc('ytick',labelsize=20)\n",
    "    \n",
    "    plt.setp([a.get_xticklabels() for a in axarr[0, :]], visible=False)\n",
    "    plt.setp([a.get_yticklabels() for a in axarr[:, 1]], visible=False)\n",
    "    plt.setp([a.get_yticklabels() for a in axarr[:, 2]], visible=False)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    if histogram:\n",
    "        f.savefig(\"../data/plots/rawtimeseries/timeseries_metric_hist_{}.png\".format(datasetName))\n",
    "    else:\n",
    "        f.savefig(\"../data/plots/rawtimeseries/timeseries_metric_{}.png\".format(datasetName))\n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-12T08:37:12.722902Z",
     "start_time": "2019-01-12T08:37:12.717921Z"
    }
   },
   "outputs": [],
   "source": [
    "def dayIteration(availableSamples, currentColumn, delta):\n",
    "    df_sampledReadings = pd.DataFrame() # dataframe to hold new samples for a column\n",
    "        \n",
    "    # iterate through each day\n",
    "    for timestamp in availableSamples:\n",
    "        # update time limits to the current date\n",
    "        start = timestamp\n",
    "        end = timestamp + timedelta(hours=delta)\n",
    "        # get meter data from only this resolution\n",
    "        df_reading = currentColumn[(currentColumn.index >= start) & (currentColumn.index <= end)]\n",
    "        # ignore index since they are unique timestamps\n",
    "        df_reading.reset_index(drop=True, inplace=True)         \n",
    "        # append new sample as columns\n",
    "        df_sampledReadings = pd.concat([df_sampledReadings, df_reading], axis=1)\n",
    "            \n",
    "    # make sure sure there are no columns with NaN values\n",
    "    df_sampledReadings.dropna(axis=1, how='all', inplace=True)\n",
    "    df_sampledReadings = df_sampledReadings.T # transpose it so it's easier to see and operate\n",
    "    # up to this point, the matrix above has the shape nxm where is the number of instances and m is the number of readings\n",
    "    \n",
    "    # if any NaN prevailed\n",
    "    df_sampledReadings.fillna(value=0, inplace=True) \n",
    "    \n",
    "    return df_sampledReadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-12T08:41:05.493211Z",
     "start_time": "2019-01-12T08:41:05.476592Z"
    }
   },
   "outputs": [],
   "source": [
    "def subplotRawTimeSeries(datasetName):\n",
    "\n",
    "    contexts = ['weekday', 'weekend', 'fullweek']\n",
    "    function = ['average', 'median']\n",
    "    \n",
    "    # load different contexts\n",
    "    df_c1 = pd.read_csv('../data/processed/{}_{}_dataset.csv'.format(datasetName, contexts[0]), parse_dates=True, \n",
    "                            infer_datetime_format=True, index_col=0)\n",
    "    df_c2 = pd.read_csv('../data/processed/{}_{}_dataset.csv'.format(datasetName, contexts[1]), parse_dates=True, \n",
    "                            infer_datetime_format=True, index_col=0)\n",
    "    df_c3 = pd.read_csv('../data/processed/{}_{}_dataset.csv'.format(datasetName, contexts[2]), parse_dates=True, \n",
    "                            infer_datetime_format=True, index_col=0)\n",
    "\n",
    "    # resample based on parameter\n",
    "    availableSamples_c1 = (df_c1.resample('1D').asfreq()).index # get list of timestamps group by day\n",
    "    availableSamples_c2 = (df_c2.resample('1D').asfreq()).index # get list of timestamps group by day\n",
    "    availableSamples_c3 = (df_c3.resample('1D').asfreq()).index # get list of timestamps group by day\n",
    "    delta = 23 # timedelta based on resample\n",
    "    \n",
    "    # iterate through all buildings (column)\n",
    "    for column in range(len(df_c1.columns)): # any df_c would have worked\n",
    "        currentColumn_c1 = pd.DataFrame(df_c1.iloc[:, column])\n",
    "        currentColumn_c2 = pd.DataFrame(df_c2.iloc[:, column])\n",
    "        currentColumn_c3 = pd.DataFrame(df_c3.iloc[:, column])\n",
    "\n",
    "        df_sampledReadings_c1 = dayIteration(availableSamples_c1, currentColumn_c1, delta)\n",
    "        df_sampledReadings_c2 = dayIteration(availableSamples_c2, currentColumn_c2, delta)\n",
    "        df_sampledReadings_c3 = dayIteration(availableSamples_c3, currentColumn_c3, delta)\n",
    "        \n",
    "        # comment this and remove identation for normal behaviour\n",
    "        if df_sampledReadings_c1.index[0] == 'Office_Joan' or df_sampledReadings_c1.index[0] == 'Office_Paulette':\n",
    "        \n",
    "            # Here starts the big plot for each building (column)\n",
    "            # Four axes, returned as a 2-d array\n",
    "            f, axarr = plt.subplots(2, 3, figsize=(20,18))\n",
    "            f.subplots_adjust(hspace=0.05, wspace=0.05)\n",
    "            \n",
    "            offices = ['Office_Joan', 'Office_Paulette']\n",
    "            \n",
    "            for i in range(2): # rows (functions)\n",
    "                # calculate load curve based on function\n",
    "                if function[i] == 'average':\n",
    "                    z_load_curve_c1 = zscore(np.mean(df_sampledReadings_c1, axis = 0))\n",
    "                    z_load_curve_c2 = zscore(np.mean(df_sampledReadings_c2, axis = 0))\n",
    "                    z_load_curve_c3 = zscore(np.mean(df_sampledReadings_c3, axis = 0))\n",
    "\n",
    "                elif function[i] =='median':\n",
    "                    z_load_curve_c1 = zscore(np.median(df_sampledReadings_c1, axis = 0))\n",
    "                    z_load_curve_c2 = zscore(np.median(df_sampledReadings_c2, axis = 0))\n",
    "                    z_load_curve_c3 = zscore(np.median(df_sampledReadings_c3, axis = 0))\n",
    "\n",
    "                # plot each context\n",
    "                for _, curve in df_sampledReadings_c1.iterrows():               \n",
    "                    z_curve = curve.copy()\n",
    "                    z_curve = zscore(z_curve)\n",
    "                    axarr[i, 0].set_ylim([-5, 5])\n",
    "                    axarr[i, 0].plot(z_curve, \"k-\", alpha=.2)\n",
    "    #                 axarr[i, 0].plot(z_load_curve_c1, \"r-\",  linewidth=7.0)\n",
    "                    axarr[i, 0].xaxis.grid(True)\n",
    "                    axarr[i, 0].yaxis.grid(True)\n",
    "                    metric = format(rawTimeSeriesL2(df_sampledReadings_c1, z_load_curve_c1), '.2f')\n",
    "                    axarr[i, 0].tick_params(axis='both', labelsize=25)\n",
    "    #                 axarr[i, 0].text(1, 4, \"metric: {}\".format(metric), fontsize = 20)\n",
    "\n",
    "                for _, curve in df_sampledReadings_c2.iterrows():               \n",
    "                    z_curve = curve.copy()\n",
    "                    z_curve = zscore(z_curve)\n",
    "                    axarr[i, 1].set_ylim([-5, 5])\n",
    "                    axarr[i, 1].plot(z_curve, \"k-\", alpha=.2)\n",
    "    #                 axarr[i, 1].plot(z_load_curve_c2, \"r-\",  linewidth=7.0)\n",
    "                    axarr[i, 1].xaxis.grid(True)\n",
    "                    axarr[i, 1].yaxis.grid(True)\n",
    "                    metric = format(rawTimeSeriesL2(df_sampledReadings_c2, z_load_curve_c2), '.2f')\n",
    "                    axarr[i, 1].tick_params(axis='both', labelsize=25)\n",
    "    #                 axarr[i, 1].text(1, 4, \"metric: {}\".format(metric), fontsize = 20)\n",
    "\n",
    "                for _, curve in df_sampledReadings_c3.iterrows():               \n",
    "                    z_curve = curve.copy()\n",
    "                    z_curve = zscore(z_curve)\n",
    "                    axarr[i, 2].set_ylim([-5, 5])\n",
    "                    axarr[i, 2].plot(z_curve, \"k-\", alpha=.2)\n",
    "    #                 axarr[i, 2].plot(z_load_curve_c3, \"r-\",  linewidth=7.0)\n",
    "                    axarr[i, 2].xaxis.grid(True)\n",
    "                    axarr[i, 2].yaxis.grid(True)\n",
    "                    metric = format(rawTimeSeriesL2(df_sampledReadings_c3, z_load_curve_c3), '.2f')\n",
    "                    axarr[i, 2].tick_params(axis='both', labelsize=25)\n",
    "    #                 axarr[i, 2].text(1, 4, \"metric: {}\".format(metric), fontsize = 20)\n",
    "\n",
    "\n",
    "            # Fine-tune figure; hide x ticks for top plots and y ticks for right plots\n",
    "            for ax, col in zip(axarr[0], contexts):\n",
    "                ax.set_title(col, fontsize = 40)\n",
    "\n",
    "            for ax, row in zip(axarr[:,0], offices):\n",
    "                ax.set_ylabel(row, rotation=90, fontsize=40)\n",
    "\n",
    "            plt.rc('xtick',labelsize=40)\n",
    "            plt.rc('ytick',labelsize=40)\n",
    "\n",
    "            plt.setp([a.get_xticklabels() for a in axarr[0, :]], visible=False)\n",
    "            plt.setp([a.get_yticklabels() for a in axarr[:, 1]], visible=False)\n",
    "            plt.setp([a.get_yticklabels() for a in axarr[:, 2]], visible=False)\n",
    "            plt.show()\n",
    "\n",
    "            f.savefig(\"../data/plots/rawtimeseries/a_b_subplots/{}_{}.png\".format(datasetName, \n",
    "                                                                                  df_sampledReadings_c1.index[0]))\n",
    "          \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T01:52:29.157760Z",
     "start_time": "2019-01-16T01:52:28.333367Z"
    }
   },
   "outputs": [],
   "source": [
    "def plotSC():\n",
    "    df = pd.read_csv('../data/results/total_scores.csv', index_col=0)\n",
    "    \n",
    "    datasets = ['BDG', 'DGS']\n",
    "    algorithms = ['kshape', 'kmeans', 'hierarchical']\n",
    "    contexts = ['weekday', 'weekend', 'fullweek']\n",
    "    function = ['average', 'median']\n",
    "    \n",
    "    # Four axes, returned as a 2-d array\n",
    "    f, axarr = plt.subplots(3, 2, figsize=(20,18))\n",
    "    f.subplots_adjust(hspace=0.05, wspace=0.05)\n",
    "    \n",
    "    for i in range(3): # rows\n",
    "#         df_row = df[df.iloc[:, 0] == datasets[i]]\n",
    "        df_row = df[df.iloc[:, 3] == algorithms[i]]\n",
    "\n",
    "        for j in range(2): # columns, different algorithm\n",
    "            df_experiment = df_row.copy()\n",
    "#             df_experiment = df_experiment[df_experiment.iloc[:, 3] == algorithms[j]]\n",
    "            df_experiment = df_experiment[df_experiment.iloc[:, 0] == datasets[j]]\n",
    "    \n",
    "            df_experiment_context = df_experiment.iloc[:, 0:4]\n",
    "            df_experiment_context.drop_duplicates(inplace = True) \n",
    "\n",
    "            lines = [\"-\",\"--\",\"-.\",\":\"]\n",
    "            linecycler = cycle(lines)\n",
    "            \n",
    "            # one plot for each context\n",
    "            for _, context in df_experiment_context.iterrows():\n",
    "                current_plot = df_experiment[(df_experiment.iloc[:, 1] == context[1]) &\n",
    "                                            (df_experiment.iloc[:, 2] == context[2])]\n",
    "                axarr[i, j].set_ylim([-0.05, 0.6])\n",
    "                x_values = current_plot.iloc[:, 4]\n",
    "                y_values = current_plot['silhouette_score']\n",
    "                context_name = str(context[1]) + \" \" +  str(context[2])\n",
    "                axarr[i, j].plot(x_values, y_values, next(linecycler), linewidth=4, label=context_name)\n",
    "                axarr[i, j].xaxis.grid(True)\n",
    "                axarr[i, j].yaxis.grid(True)\n",
    "                axarr[i, j].tick_params(axis='both', labelsize=20)\n",
    "                axarr[i, j].set_xticks(np.arange(min(x_values), max(x_values)+1, 1.0))\n",
    "                \n",
    "    # Fine-tune figure; hide x ticks for top plots and y ticks for right plots\n",
    "#     cols = ['K-Shape', 'K-Means', 'Hierarchical']\n",
    "    rows = ['K-Shape', 'K-Means', 'Hierarchical']\n",
    "    \n",
    "    for ax, col in zip(axarr[0], datasets):\n",
    "        ax.set_title(col, fontsize = 20)\n",
    "\n",
    "    for ax, row in zip(axarr[:,0], rows):\n",
    "        ax.set_ylabel(row, rotation=90, fontsize=20)\n",
    "    \n",
    "    f.text(0.5, 0.08, 'K', ha='center', fontsize=20)\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(-0.85, -0.3), prop={'size': 20},  ncol=3)    \n",
    "    plt.setp([a.get_xticklabels() for a in axarr[0, :]], visible=False)\n",
    "    plt.setp([a.get_xticklabels() for a in axarr[1, :]], visible=False)\n",
    "    plt.setp([a.get_yticklabels() for a in axarr[:, 1]], visible=False)\n",
    "#     plt.setp([a.get_yticklabels() for a in axarr[:, 2]], visible=False)\n",
    "    plt.show()\n",
    "    plt.tight_layout()\n",
    "    f.savefig(\"../data/plots/silhouette_all_plots.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Batch implementation of stackedClusterPSU\n",
    "\"\"\"\n",
    "def batchStackedClusterPSU(datasetName_list, context_list, function_list, algorithm_list, rangeK = range(2,11), \n",
    "                           percentage=[True, False], within_overall=[True, False]):\n",
    "\n",
    "    for dataset in datasetName_list:\n",
    "        for context in context_list:\n",
    "            for function in function_list:\n",
    "                for algorithm in algorithm_list:\n",
    "                    for k in rangeK:\n",
    "                        for pcg in percentage:\n",
    "                            for within in within_overall:\n",
    "                                stackedClusterPSU(dataset, context, function, algorithm, k, pcg, within)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-13T01:43:24.208662Z",
     "start_time": "2019-01-13T01:43:24.193812Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3. Comparisons\n",
    "a. Comparison of correctly and incorrectly labeled buildings as absolute number and in percentage \n",
    "    (within and overall), for the choosen k, algo, and context).\n",
    "\"\"\"\n",
    "def stackedClusterPSU(datasetName, context, function, algorithm, k, percentage=False, within=True):\n",
    "    # load data\n",
    "    df_data = pd.read_csv('../data/processed/{}_{}_{}_dataset.csv'.format(datasetName, context, function), \n",
    "                          index_col=0)\n",
    "    # load metadata\n",
    "    if datasetName == 'BDG':\n",
    "        df_meta = pd.read_csv('../data/raw/meta_open.csv')\n",
    "        ground_truth_list = ['Office', 'Dormitory', 'College Classroom', 'Primary/Secondary Classroom', \n",
    "                             'College Laboratory']\n",
    "        df_og_psu = df_meta[df_meta.iloc[:, 0].isin(df_data.index.values)]\n",
    "        df_og_psu = df_og_psu['primaryspaceusage'].value_counts()\n",
    "        \n",
    "    elif datasetName == 'DGS':\n",
    "        df_meta = pd.read_csv('../data/raw/dgs_metadata.csv')\n",
    "        df_aux = pd.read_csv(\"../data/raw/DGS_322_Buildings-15m-By_Building-DST-gap-filled-3-2-18-508pm.csv\")\n",
    "        df_aux = df_aux.T\n",
    "        ground_truth_list = ['Fire Station',\n",
    "                             'Library',\n",
    "                             'K-12 School',\n",
    "                             'Office',\n",
    "                             'Other - Recreation',\n",
    "                             'Other - Public Services',\n",
    "                             'Police Station']\n",
    "        df_og_psu = df_aux[df_aux.iloc[:, 0].isin(df_data.index.values)] # get id based on names\n",
    "        df_og_psu = df_meta[df_meta['id'].isin(df_og_psu.index.values)] # get label based on id\n",
    "        df_og_psu = df_og_psu['espm_type_name'].value_counts()             \n",
    "    \n",
    "    elif datasetName == 'BDG-DGS':\n",
    "        df_meta_bdg = pd.read_csv('../data/raw/meta_open.csv')        \n",
    "        df_meta_dgs = pd.read_csv('../data/raw/dgs_metadata.csv')\n",
    "        df_aux = pd.read_csv(\"../data/raw/DGS_322_Buildings-15m-By_Building-DST-gap-filled-3-2-18-508pm.csv\")\n",
    "        df_aux = df_aux.T\n",
    "        ground_truth_list = ['Office', 'Dormitory', 'College Classroom', 'Primary/Secondary Classroom', \n",
    "                             'College Laboratory', # end of BDG\n",
    "                             'Fire Station',\n",
    "                             'Library',\n",
    "#                              'K-12 School', # this was replace by 'Primary/Secondary Classroom'\n",
    "#                              'Office', # repeated above\n",
    "                             'Other - Recreation',\n",
    "                             'Other - Public Services',\n",
    "                             'Police Station']\n",
    "        \n",
    "        df_og_psu = df_meta_bdg[df_meta_bdg.iloc[:, 0].isin(df_data.index.values)]\n",
    "        # count the times the different ground truth labels exist in each cluster\n",
    "        df_og_psu = df_og_psu['primaryspaceusage']#.value_counts()\n",
    "                \n",
    "        # resample based on building id for the psu labels (DGS)\n",
    "        df_og_psu_aux = df_aux[df_aux.iloc[:, 0].isin(df_data.index.values)] # get id based on names\n",
    "        df_og_psu_aux = df_meta_dgs[df_meta_dgs['id'].isin(df_og_psu_aux.index.values)] # get label based on id\n",
    "                \n",
    "        # merge both set of labels\n",
    "        df_og_psu = df_og_psu.append(df_og_psu_aux['espm_type_name'], ignore_index=True)#.value_counts()\n",
    "        # replace K-12 to Primary/Secondary Classroom\n",
    "        df_og_psu = df_og_psu.replace(to_replace='K-12 School', value='Primary/Secondary Classroom')\n",
    "        # as a final step, produce the counts\n",
    "        df_og_psu = df_og_psu.value_counts()\n",
    "        \n",
    "    # Z-normalize the data\n",
    "    df_scaled = TimeSeriesScalerMeanVariance(mu=0.0, std=1.0).fit_transform(df_data.values)\n",
    "    # df.values will generate a 3d array (the third dimension being 1) so we convert it to a 2d array\n",
    "    df_scaled = np.squeeze(df_scaled) # now is a 2d array\n",
    "    \n",
    "    # run algorithm\n",
    "    model, labels = doClustering(df_scaled, datasetName, algorithm, k, seed=3, max_iter=300)\n",
    "\n",
    "    # get number of elements in each cluster\n",
    "    data_dict = {i: np.where(labels == i)[0] for i in range(k)}\n",
    "\n",
    "    df_dist = pd.DataFrame(index = ground_truth_list)\n",
    "\n",
    "    # for each building in a cluster\n",
    "    for key, value in data_dict.items():\n",
    "        bdg_ids = df_data.iloc[value.tolist()] # retrieve original building name based on index\n",
    "        \n",
    "        if datasetName == 'BDG':\n",
    "            # resample based on building id\n",
    "            ground_truth_labels = df_meta[df_meta.iloc[:, 0].isin(bdg_ids.index.values)]\n",
    "            # count the times the different ground truth labels exist in each cluster\n",
    "            aux = ground_truth_labels['primaryspaceusage'].value_counts()\n",
    "            \n",
    "        elif datasetName == 'DGS':\n",
    "            # resample based on building id for the psu labels\n",
    "            ground_truth_labels = df_aux[df_aux.iloc[:, 0].isin(bdg_ids.index.values)] # get id based on names\n",
    "            # get label based on id\n",
    "            ground_truth_labels = df_meta[df_meta['id'].isin(ground_truth_labels.index.values)]\n",
    "            aux = ground_truth_labels['espm_type_name'].value_counts()   \n",
    "            \n",
    "        elif datasetName == 'BDG-DGS':\n",
    "                # resample based on building id for the psu labels (BDG)\n",
    "                ground_truth_labels = df_meta_bdg[df_meta_bdg.iloc[:, 0].isin(bdg_ids.index.values)]\n",
    "                # count the times the different ground truth labels exist in each cluster\n",
    "                aux = ground_truth_labels['primaryspaceusage']#.value_counts()\n",
    "                \n",
    "                # resample based on building id for the psu labels (DGS)\n",
    "                df_hist = df_aux[df_aux.iloc[:, 0].isin(bdg_ids.index.values)] # get id based on names\n",
    "                df_hist = df_meta_dgs[df_meta_dgs['id'].isin(df_hist.index.values)] # get label based on id\n",
    "                \n",
    "                # merge both set of labels\n",
    "                aux = aux.append(df_hist['espm_type_name'], ignore_index=True)#.value_counts()\n",
    "                # replace K-12 to Primary/Secondary Classroom\n",
    "                aux = aux.replace(to_replace='K-12 School', value='Primary/Secondary Classroom')\n",
    "                # as a final step, produce the counts\n",
    "                aux = aux.value_counts()\n",
    "                \n",
    "        \n",
    "#         print(aux)        \n",
    "        df_dist[key] = aux    \n",
    "        # end of for loop    \n",
    "    \n",
    "#     print(df_dist[0])\n",
    "#     print(df_dist[1])\n",
    "#     print(df_dist[2])\n",
    "#     print(df_dist[3])\n",
    "    \n",
    "    df_dist.index.name = 'PSU'\n",
    "    \n",
    "    # plot stacked bar\n",
    "    ax = df_dist.T.plot.barh(stacked=True, figsize=(20, 18), mark_right = True) # original without text\n",
    "        \n",
    "    # within membership \n",
    "    df_total = df_dist.T.copy()\n",
    "    df_total['total'] = df_total.sum(axis=1)\n",
    "    df_total = df_total['total'] # cluster count\n",
    "    df = df_dist.T.copy() # psu count for each cluster\n",
    "    df_rel = df.div(df_total, 0) * 100 # percentages\n",
    "    df_rel = addTo100(df_rel)\n",
    "    \n",
    "    # overall membership\n",
    "    df_rel_overall = pd.DataFrame(index = df.index.values, columns = df.columns.values)\n",
    "    for column in df.columns.values: # iterate through columns names\n",
    "        currentColumn = df[column]\n",
    "        df_rel_overall[column] = currentColumn.div(df_og_psu[column], 0) * 100\n",
    "        \n",
    "    df_rel_overall = addTo100(df_rel_overall)\n",
    "    \n",
    "    for n in df_rel:\n",
    "        if percentage:\n",
    "            # within cluster percentage\n",
    "            if within:\n",
    "                for i, (cs, ab, pc, tot) in enumerate(zip(df.iloc[:, :].cumsum(1)[n], df[n], df_rel[n], \n",
    "                                                          df_total)):\n",
    "    #                 plt.text(tot, i, str(int(tot)), va='center', fontsize = 30)\n",
    "                    if np.isnan(pc):\n",
    "                        continue\n",
    "                    else:\n",
    "                        plt.text(cs - ab/2, i, str(int(pc)), va='center', ha='center', fontsize = 30)\n",
    "                plotType = \"Within Cluster PSU Membership Percentage\"\n",
    "\n",
    "            # overall cluster percentage            \n",
    "            else:\n",
    "                for i, (cs, ab, pc, tot) in enumerate(zip(df.iloc[:, :].cumsum(1)[n], df[n], df_rel_overall[n], \n",
    "                                                          df_total)):\n",
    "#                     plt.text(tot, i, str(int(tot)), va='center', fontsize = 30)\n",
    "                    if np.isnan(pc):\n",
    "                        continue\n",
    "                    else:\n",
    "                        plt.text(cs - ab/2, i, str(int(pc)), va='center', ha='center', fontsize = 30)\n",
    "                plotType = \"Overall Cluster  PSU Membership Percentage\"\n",
    "                \n",
    "        # absolute numbers        \n",
    "        else:\n",
    "            for i, (cs, ab, pc, tot) in enumerate(zip(df.iloc[:, :].cumsum(1)[n], df[n], df[n], df_total)):\n",
    "                plt.text(tot, i, str(int(tot)), va='center', fontsize = 30)\n",
    "                if np.isnan(pc):\n",
    "                    continue\n",
    "                else:\n",
    "                    plt.text(cs - ab/2, i, str(int(pc)), va='center', ha='center', fontsize = 30)\n",
    "            plotType = \"Within/Overall Cluster PSU Membership Absolute Number\"\n",
    "\n",
    "        # end of foor loop\n",
    "\n",
    "#     plt.title(plotType + \"\\n\" +\n",
    "#               \"Dataset: {}, Context: {}, Function: {}, Algorithm: {}, K: {}\".format(datasetName, context, \n",
    "#                                                                                     function, algorithm, k), \n",
    "#               fontsize = 30)\n",
    "    \n",
    "    # plot tuning\n",
    "    if datasetName == 'BDG-DGS':\n",
    "        plt.legend(loc=[1, 0], prop={'size': 30})\n",
    "    else:\n",
    "        plt.legend(loc='best', prop={'size': 30})\n",
    "    \n",
    "    plt.xticks(fontsize=30)\n",
    "    plt.yticks(fontsize=30)\n",
    "    plt.rc('xtick',labelsize=30)\n",
    "    plt.rc('ytick',labelsize=30)\n",
    "    plt.ylabel('Cluster', fontsize=30)\n",
    "    plt.xlabel('Number of Buildings', fontsize=30)\n",
    "    ax.xaxis.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    fig = ax.get_figure()\n",
    "    if percentage:\n",
    "        folderName = 'percentage'\n",
    "    else:\n",
    "        folderName = 'absolute_values'\n",
    "    fig.savefig('../data/plots/cluster_stackedbars/{}/cluster_stackedbars_{}_{}_{}_{}_{}.png'.format(folderName,\n",
    "                                                                                                     datasetName, \n",
    "                                                                                                     context,\n",
    "                                                                                                     function, \n",
    "                                                                                                     algorithm, \n",
    "                                                                                                     k))\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T05:45:53.360335Z",
     "start_time": "2019-01-16T05:45:53.345028Z"
    }
   },
   "outputs": [],
   "source": [
    "def addTo100(df):\n",
    "#     print(df)\n",
    "    df_rounded_values = df.round(0)\n",
    "#     print(df_rounded_values)\n",
    "    \n",
    "    # iterate all PSU labels\n",
    "    for column in range(len(df_rounded_values.columns)):\n",
    "        currentColumn = df_rounded_values.iloc[:, column]\n",
    "        if currentColumn.sum() > 100:\n",
    "            # decrease the highest\n",
    "            while(currentColumn.sum() > 100):\n",
    "                currentColumn.loc[currentColumn.idxmax()] = currentColumn.loc[currentColumn.idxmax()] - 1\n",
    "                df_rounded_values.iloc[:, column] = currentColumn\n",
    "        \n",
    "        elif currentColumn.sum() < 100:\n",
    "            # increase the highest\n",
    "            while(currentColumn.sum() < 100):\n",
    "                currentColumn.loc[currentColumn.idxmax()] = currentColumn.loc[currentColumn.idxmax()] + 1\n",
    "                df_rounded_values.iloc[:, column] = currentColumn\n",
    "        else:\n",
    "            continue\n",
    "#     print(df_rounded_values)\n",
    "    return df_rounded_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLabelHist(datasetName, context, function):\n",
    "    df = pd.read_csv('../data/processed/{}_{}_{}_dataset.csv'.format(datasetName, context, function), index_col=0)\n",
    "    \n",
    "    if datasetName == 'BDG':\n",
    "        df_meta = pd.read_csv('../data/raw/meta_open.csv') # BDG\n",
    "        df_hist = df_meta[df_meta.iloc[:, 0].isin(df.index.values)]\n",
    "        x_values = df_hist['primaryspaceusage']\n",
    "\n",
    "    elif datasetName == 'DGS':\n",
    "        df_meta = pd.read_csv('../data/raw/dgs_metadata.csv') # DGS\n",
    "        df_aux = pd.read_csv(\"../data/raw/DGS_322_Buildings-15m-By_Building-DST-gap-filled-3-2-18-508pm.csv\")\n",
    "        df_aux = df_aux.T\n",
    "        df_hist = df_aux[df_aux.iloc[:, 0].isin(df.index.values)] # get id based on names\n",
    "        df_hist = df_meta[df_meta['id'].isin(df_hist.index.values)] # get label based on id\n",
    "        x_values = df_hist['espm_type_name'] \n",
    "        print(x_values.value_counts())\n",
    "    hist = go.FigureWidget(\n",
    "        data=[\n",
    "            dict(\n",
    "                type='histogram',\n",
    "                x=x_values, #espm_type_name - primaryspaceusage\n",
    "\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "#     hist.layout.title = '{} PSU Histogram'.format(datasetName)\n",
    "#     hist.layout.titlefont.size = 30\n",
    "    hist.layout.xaxis.tickfont.size = 20\n",
    "    hist.layout.xaxis.tickangle = -90 # proper way to display them\n",
    "    hist.layout.yaxis.tickfont.size = 20\n",
    "    hist.layout.yaxis.title = \"Count\"\n",
    "    hist.layout.yaxis.titlefont.size = 20\n",
    "    \n",
    "    if datasetName == 'BDG':\n",
    "        hist.layout.margin.b = 330 # BDG\n",
    "        hist.layout.height=800\n",
    "        \n",
    "    elif datasetName == 'DGS':\n",
    "        hist.layout.margin.b = 250 # DGS\n",
    "        hist.layout.height=720\n",
    "    \n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-08T05:44:14.804789Z",
     "start_time": "2019-01-08T05:44:14.774070Z"
    }
   },
   "outputs": [],
   "source": [
    "def getMemberships(datasetName, context, function, algorithm, algo_parameter=range(2,11), individual=False):\n",
    "    # load metadata\n",
    "    if datasetName == 'BDG':\n",
    "        df_meta = pd.read_csv('../data/raw/meta_open.csv')#, index_col=0)\n",
    "        # final df for all the information\n",
    "        df_memberships = pd.DataFrame(columns=['dataset','context','function','algorithm','parameter k', \n",
    "                                               'clusterNum', 'clusterLabel', 'Office', 'Dormitory', \n",
    "                                               'College Classroom', 'Primary/Secondary Classroom', \n",
    "                                               'College Laboratory', 'CorrectLabel', 'IncorrectLabel']) \n",
    "        ground_truth_list = ['Office', 'Dormitory', 'College Classroom', 'Primary/Secondary Classroom', \n",
    "                            'College Laboratory']\n",
    "        \n",
    "    elif datasetName == 'DGS':\n",
    "        df_meta = pd.read_csv('../data/raw/dgs_metadata.csv')\n",
    "        df_aux = pd.read_csv(\"../data/raw/DGS_322_Buildings-15m-By_Building-DST-gap-filled-3-2-18-508pm.csv\")\n",
    "        df_aux = df_aux.T\n",
    "        df_memberships = pd.DataFrame(columns=['dataset','context','function','algorithm','parameter k', \n",
    "                                               'clusterNum', 'clusterLabel', \n",
    "                                               'Fire Station',\n",
    "                                               'Library',\n",
    "                                               'K-12 School',\n",
    "                                               'Office',\n",
    "                                               'Other - Recreation',\n",
    "                                               'Other - Public Services',\n",
    "                                               'Police Station',\n",
    "                                               'CorrectLabel',\n",
    "                                               'IncorrectLabel'])\n",
    "        \n",
    "        ground_truth_list = ['Fire Station',\n",
    "                             'Library',\n",
    "                             'K-12 School',\n",
    "                             'Office',\n",
    "                             'Other - Recreation',\n",
    "                             'Other - Public Services',\n",
    "                             'Police Station']\n",
    "    \n",
    "    elif datasetName == 'BDG-DGS':\n",
    "        df_meta_bdg = pd.read_csv('../data/raw/meta_open.csv')        \n",
    "        df_meta_dgs = pd.read_csv('../data/raw/dgs_metadata.csv')\n",
    "        df_aux = pd.read_csv(\"../data/raw/DGS_322_Buildings-15m-By_Building-DST-gap-filled-3-2-18-508pm.csv\")\n",
    "        df_aux = df_aux.T\n",
    "        # final df for all information\n",
    "        df_memberships = pd.DataFrame(columns=['dataset','context','function','algorithm','parameter k', \n",
    "                                               'clusterNum', 'clusterLabel',\n",
    "                                               'Office', 'Dormitory', 'College Classroom', \n",
    "                                               'Primary/Secondary Classroom', 'College Laboratory', # end of BDG\n",
    "                                               'Fire Station',\n",
    "                                               'Library',\n",
    "#                                                'K-12 School', # this was replace by 'Primary/Secondary Classroom'\n",
    "#                                                'Office', # repeated above\n",
    "                                               'Other - Recreation',\n",
    "                                               'Other - Public Services',\n",
    "                                               'Police Station',\n",
    "                                               'CorrectLabel',\n",
    "                                               'IncorrectLabel'])\n",
    "\n",
    "        ground_truth_list = ['Office', 'Dormitory', 'College Classroom', 'Primary/Secondary Classroom', \n",
    "                             'College Laboratory', # end of BDG\n",
    "                             'Fire Station',\n",
    "                             'Library',\n",
    "#                              'K-12 School', # this was replace by 'Primary/Secondary Classroom'\n",
    "#                              'Office', # repeated above\n",
    "                             'Other - Recreation',\n",
    "                             'Other - Public Services',\n",
    "                             'Police Station']\n",
    "\n",
    "    # load data\n",
    "    df_data = pd.read_csv('../data/processed/{}_{}_{}_dataset.csv'.format(datasetName, context, function), \n",
    "                          index_col=0)\n",
    "\n",
    "    # Z-normalize the data\n",
    "    df_scaled = TimeSeriesScalerMeanVariance(mu=0.0, std=1.0).fit_transform(df_data.values)\n",
    "    # df.values will generate a 3d array (the third dimension being 1) so we convert it to a 2d array\n",
    "    df_scaled = np.squeeze(df_scaled) # now is a 2d array\n",
    "    \n",
    "    # for all k's\n",
    "    for k in algo_parameter:\n",
    "        # run algorithm\n",
    "        model, labels = doClustering(df_scaled, datasetName, algorithm, k, seed=3, max_iter=300)\n",
    "        \n",
    "        # get number of elements in each cluster\n",
    "        data_dict = {i: np.where(labels == i)[0] for i in range(k)} # will have the indeces of df_data\n",
    "        df_dist = pd.DataFrame()\n",
    "\n",
    "        # get the ground truth label for buildings in each cluster\n",
    "        for key, value in data_dict.items():\n",
    "            # get correct names of bdgs\n",
    "            bdg_ids = df_data.iloc[value.tolist()] # retrieve original building name based on index\n",
    "\n",
    "            if datasetName == 'BDG':\n",
    "                # resample based on building id for the psu labels\n",
    "                ground_truth_labels = df_meta[df_meta.iloc[:, 0].isin(bdg_ids.index.values)]\n",
    "                # count the times the different ground truth labels exist in each cluster\n",
    "                aux = ground_truth_labels['primaryspaceusage'].value_counts()\n",
    "                \n",
    "            elif datasetName == 'DGS':\n",
    "                # resample based on building id for the psu labels\n",
    "                df_hist = df_aux[df_aux.iloc[:, 0].isin(bdg_ids.index.values)] # get id based on names\n",
    "                df_hist = df_meta[df_meta['id'].isin(df_hist.index.values)] # get label based on id\n",
    "                aux = df_hist['espm_type_name'].value_counts()             \n",
    "                \n",
    "            elif datasetName == 'BDG-DGS':\n",
    "                # resample based on building id for the psu labels (BDG)\n",
    "                ground_truth_labels = df_meta_bdg[df_meta_bdg.iloc[:, 0].isin(bdg_ids.index.values)]\n",
    "                # count the times the different ground truth labels exist in each cluster\n",
    "                aux = ground_truth_labels['primaryspaceusage']#.value_counts()\n",
    "                \n",
    "                # resample based on building id for the psu labels (DGS)\n",
    "                df_hist = df_aux[df_aux.iloc[:, 0].isin(bdg_ids.index.values)] # get id based on names\n",
    "                df_hist = df_meta_dgs[df_meta_dgs['id'].isin(df_hist.index.values)] # get label based on id\n",
    "                \n",
    "                # merge both set of labels\n",
    "                aux = aux.append(df_hist['espm_type_name'], ignore_index=True)#.value_counts()\n",
    "                # replace K-12 to Primary/Secondary Classroom\n",
    "                aux = aux.replace(to_replace='K-12 School', value='Primary/Secondary Classroom')\n",
    "                # as a final step, produce the counts\n",
    "                aux = aux.value_counts()\n",
    "        \n",
    "            # if a particular PSU didnt exist in the cluster\n",
    "            for psu in ground_truth_list:\n",
    "                if psu not in aux.index.values :\n",
    "                    aux = aux.set_value(psu, 0)\n",
    "                    \n",
    "            df_dist[key] = aux\n",
    "            \n",
    "            currentCluster = df_dist.T.tail(1) # so it's easier to treat as columns and only the current cluster\n",
    "            clusterNum = currentCluster.index.values[0] # label assigned by cluster algorithm,\n",
    "\n",
    "            # based on our assumption, label matches the PSU with highest count\n",
    "            clusterLabel = currentCluster.idxmax(axis=1).iloc[0]\n",
    "\n",
    "            # the number of correct labels is the number of buildings whose PSU is the cluster label\n",
    "            correctLabel = currentCluster[clusterLabel]\n",
    "            # the incorrectly clusters are the remaining buildings\n",
    "            incorrectLabel = currentCluster.copy().drop(clusterLabel, axis=1).sum(axis=1)\n",
    "            \n",
    "            # append everything to total dataframe\n",
    "            if datasetName == 'BDG':\n",
    "                df_memberships = df_memberships.append({'dataset': datasetName,\n",
    "                                    'context': context,\n",
    "                                    'function': function,\n",
    "                                    'algorithm': algorithm,\n",
    "                                    'parameter k': k,\n",
    "                                    'clusterNum': clusterNum, \n",
    "                                    'clusterLabel': clusterLabel, \n",
    "                                    'Office': currentCluster['Office'].iloc[0], \n",
    "                                    'Dormitory': currentCluster['Dormitory'].iloc[0],\n",
    "                                    'College Classroom': currentCluster['College Classroom'].iloc[0], \n",
    "                                    'Primary/Secondary Classroom': currentCluster['Primary/Secondary Classroom'].iloc[0], \n",
    "                                    'College Laboratory': currentCluster['College Laboratory'].iloc[0],\n",
    "                                    'CorrectLabel': correctLabel.iloc[0],\n",
    "                                    'IncorrectLabel': incorrectLabel.iloc[0]\n",
    "                                    }, ignore_index=True)\n",
    "            elif datasetName == 'DGS':\n",
    "                 df_memberships = df_memberships.append({'dataset': datasetName,\n",
    "                                    'context': context,\n",
    "                                    'function': function,\n",
    "                                    'algorithm': algorithm,\n",
    "                                    'parameter k': k,\n",
    "                                    'clusterNum': clusterNum, \n",
    "                                    'clusterLabel': clusterLabel, \n",
    "                                    'Fire Station': currentCluster['Fire Station'].iloc[0],\n",
    "                                    'Library': currentCluster['Library'].iloc[0],\n",
    "                                    'K-12 School': currentCluster['K-12 School'].iloc[0],\n",
    "                                    'Office': currentCluster['Office'].iloc[0],\n",
    "                                    'Other - Recreation': currentCluster['Other - Recreation'].iloc[0],\n",
    "                                    'Other - Public Services': currentCluster['Other - Public Services'].iloc[0],\n",
    "                                    'Police Station': currentCluster['Police Station'].iloc[0],\n",
    "                                    'CorrectLabel': correctLabel.iloc[0],\n",
    "                                    'IncorrectLabel': incorrectLabel.iloc[0]\n",
    "                                    }, ignore_index=True)\n",
    "                    \n",
    "            elif datasetName == 'BDG-DGS':\n",
    "                df_memberships = df_memberships.append({'dataset': datasetName,\n",
    "                                    'context': context,\n",
    "                                    'function': function,\n",
    "                                    'algorithm': algorithm,\n",
    "                                    'parameter k': k,\n",
    "                                    'clusterNum': clusterNum, \n",
    "                                    'clusterLabel': clusterLabel, \n",
    "                                    'Office': currentCluster['Office'].iloc[0], \n",
    "                                    'Dormitory': currentCluster['Dormitory'].iloc[0],\n",
    "                                    'College Classroom': currentCluster['College Classroom'].iloc[0], \n",
    "                                    'Primary/Secondary Classroom': currentCluster['Primary/Secondary Classroom'].iloc[0], \n",
    "                                    'College Laboratory': currentCluster['College Laboratory'].iloc[0],\n",
    "                                    'Fire Station': currentCluster['Fire Station'].iloc[0],\n",
    "                                    'Library': currentCluster['Library'].iloc[0],\n",
    "#                                     'K-12 School': currentCluster['K-12 School'].iloc[0],\n",
    "#                                     'Office': currentCluster['Office'].iloc[0],\n",
    "                                    'Other - Recreation': currentCluster['Other - Recreation'].iloc[0],\n",
    "                                    'Other - Public Services': currentCluster['Other - Public Services'].iloc[0],\n",
    "                                    'Police Station': currentCluster['Police Station'].iloc[0],\n",
    "                                    'CorrectLabel': correctLabel.iloc[0],\n",
    "                                    'IncorrectLabel': incorrectLabel.iloc[0]\n",
    "                                    }, ignore_index=True)\n",
    "              \n",
    "    # save as csv\n",
    "    if individual:\n",
    "        df_memberships.to_csv('../data/results/total_memerbships_{}_{}_{}_{}.csv'.format(datasetName,\n",
    "                                                                                        context,\n",
    "                                                                                        function,\n",
    "                                                                                        algorithm))\n",
    "    else:\n",
    "        # append to general file\n",
    "        with open('../data/results/total_memberships_{}.csv'.format(datasetName), 'a') as f:\n",
    "            df_memberships.to_csv(f, header=False)\n",
    "            \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T08:27:44.366709Z",
     "start_time": "2019-01-05T08:27:44.360956Z"
    }
   },
   "outputs": [],
   "source": [
    "def getBatchMemberships(datasetName_list, context_list, function_list, algorithm_list, algo_parameter=range(2,11)):\n",
    "     # nested loops for each possible combination of the lists\n",
    "    for datasetName in datasetName_list: # every dataset\n",
    "        for context in context_list: # every context\n",
    "            for function in function_list: # every load aggregation function\n",
    "                for algorithm in algorithm_list: # every algorithm\n",
    "                    getMemberships(datasetName, context, function, algorithm, algo_parameter=range(2,11))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateFinalTable(dataset, k):\n",
    "    # list values for context\n",
    "    contexts = ['weekday', 'weekend', 'fullweek']\n",
    "    functions = ['average', 'median']\n",
    "    algorithms = ['kshape', 'kmeans', 'hierarchical']\n",
    "    \n",
    "    # load results\n",
    "    df = pd.read_csv(\"../data/results/total_memberships_{}.csv\".format(dataset), index_col=0)\n",
    "\n",
    "    # since each processed dataset has the same buildings but with different hourly readings, we can load any of them\n",
    "    df_pro = pd.read_csv('../data/processed/{}_{}_{}_dataset.csv'.format(dataset, contexts[0], functions[0]), index_col=0)\n",
    "    \n",
    "    # load total counts of buildings for each psu\n",
    "    if dataset == 'BDG':\n",
    "        df_meta = pd.read_csv('../data/raw/meta_open.csv')\n",
    "        df_psu_labels = df_meta[df_meta.iloc[:, 0].isin(df_pro.index.values)]\n",
    "        label_dist = df_psu_labels['primaryspaceusage'].value_counts()\n",
    "        df_final = pd.DataFrame(index = ['Office', 'Dormitory', 'College Classroom', \n",
    "                                        'Primary/Secondary Classroom', 'College Laboratory', 'Total'], \n",
    "                                columns=['Correctly Labeled', 'Incorrectly Labeled'])\n",
    "    elif dataset =='DGS':\n",
    "        df_meta = pd.read_csv('../data/raw/dgs_metadata.csv')\n",
    "        df_aux = pd.read_csv(\"../data/raw/DGS_322_Buildings-15m-By_Building-DST-gap-filled-3-2-18-508pm.csv\")\n",
    "        df_aux = df_aux.T\n",
    "        df_psu_labels = df_aux[df_aux.iloc[:, 0].isin(df_pro.index.values)] # get id based on names\n",
    "        df_psu_labels = df_meta[df_meta['id'].isin(df_psu_labels.index.values)] # get label based on id\n",
    "        label_dist = df_psu_labels['espm_type_name'].value_counts()\n",
    "        df_final = pd.DataFrame(index = ['Fire Station',\n",
    "                                         'Library',\n",
    "                                         'K-12 School',\n",
    "                                         'Office',\n",
    "                                         'Other - Recreation',\n",
    "                                         'Other - Public Services',\n",
    "                                         'Police Station',\n",
    "                                         'Total'],\n",
    "                                columns=['Correctly Labeled', 'Incorrectly Labeled'])\n",
    "    elif dataset == 'BDG-DGS':\n",
    "        # BDG\n",
    "        df_meta_bdg = pd.read_csv('../data/raw/meta_open.csv')\n",
    "        df_psu_labels = df_meta_bdg[df_meta_bdg.iloc[:, 0].isin(df_pro.index.values)]\n",
    "        label_dist = df_psu_labels['primaryspaceusage']#.value_counts()\n",
    "        # DGS\n",
    "        df_meta_dgs = pd.read_csv('../data/raw/dgs_metadata.csv')\n",
    "        df_aux = pd.read_csv(\"../data/raw/DGS_322_Buildings-15m-By_Building-DST-gap-filled-3-2-18-508pm.csv\")\n",
    "        df_aux = df_aux.T\n",
    "        df_psu_labels = df_aux[df_aux.iloc[:, 0].isin(df_pro.index.values)] # get id based on names\n",
    "        df_psu_labels = df_meta_dgs[df_meta_dgs['id'].isin(df_psu_labels.index.values)] # get label based on id\n",
    "        # merge both set of labels\n",
    "        label_dist = label_dist.append(df_psu_labels['espm_type_name'], ignore_index=True)#.value_counts()\n",
    "        # replace K-12 to Primary/Secondary Classroom\n",
    "        label_dist = label_dist.replace(to_replace='K-12 School', value='Primary/Secondary Classroom')\n",
    "        # as a final step, produce the counts\n",
    "        label_dist = label_dist.value_counts()\n",
    "        \n",
    "        df_final = pd.DataFrame(index = ['Office', 'Dormitory', 'College Classroom', \n",
    "                                         'Primary/Secondary Classroom', 'College Laboratory', # end of BDG\n",
    "                                         'Fire Station',\n",
    "                                         'Library',\n",
    "#                                        'K-12 School', # this was replace by 'Primary/Secondary Classroom'\n",
    "#                                        'Office', # repeated above\n",
    "                                         'Other - Recreation',\n",
    "                                         'Other - Public Services',\n",
    "                                         'Police Station',\n",
    "                                         'Total'],\n",
    "                                columns=['Correctly Labeled', 'Incorrectly Labeled'])\n",
    "    \n",
    "    # iterate through all elements of the lists\n",
    "    for context in contexts:\n",
    "        for function in functions:\n",
    "            for algorithm in algorithms:\n",
    "                contextName = \"{}-{}-{}\".format(context, function, algorithm)\n",
    "                print(contextName)\n",
    "                \n",
    "                if dataset == 'BDG':\n",
    "                    current_context = pd.DataFrame(index = ['Office', 'Dormitory', 'College Classroom', \n",
    "                                                            'Primary/Secondary Classroom',\n",
    "                                                            'College Laboratory', 'Total'], \n",
    "                                                   columns=['Correctly Labeled', 'Incorrectly Labeled'])\n",
    "                elif dataset == 'DGS':\n",
    "                    current_context = pd.DataFrame(index = ['Fire Station',\n",
    "                                                            'Library',\n",
    "                                                            'K-12 School',\n",
    "                                                            'Office',\n",
    "                                                            'Other - Recreation',\n",
    "                                                            'Other - Public Services',\n",
    "                                                            'Police Station',\n",
    "                                                            'Total'],\n",
    "                                                   columns=['Correctly Labeled', 'Incorrectly Labeled'])\n",
    "                elif dataset == 'BDG-DGS':\n",
    "                    current_context = pd.DataFrame(index = ['Office', 'Dormitory', 'College Classroom', \n",
    "                                                            'Primary/Secondary Classroom', \n",
    "                                                            'College Laboratory', # end of BDG\n",
    "                                                            'Fire Station',\n",
    "                                                            'Library',\n",
    "#                                                             'K-12 School', # this was replace by 'Primary/Secondary Classroom'\n",
    "#                                                             'Office', # repeated above\n",
    "                                                            'Other - Recreation',\n",
    "                                                            'Other - Public Services',\n",
    "                                                            'Police Station',\n",
    "                                                            'Total'],\n",
    "                                                   columns=['Correctly Labeled', 'Incorrectly Labeled'])\n",
    "                total_sum_correct = 0\n",
    "                total_sum_incorrect = 0 \n",
    "                # get the subset dataset which will be the first two columns\n",
    "                df_resampled = df[(df['context'] == context) & \n",
    "                                  (df['function'] == function) &\n",
    "                                  (df['algorithm'] == algorithm) &\n",
    "                                  (df['parameter k'] == k)]\n",
    "\n",
    "                # iterate through PSU labels\n",
    "                for psu in current_context.index.values:\n",
    "                    if psu == 'Total':\n",
    "                        continue\n",
    "                    # correct labels counts happe when the cluster label is the current psu\n",
    "                    correctLabel = df_resampled[df_resampled['clusterLabel'] == psu] # all rows where that happens\n",
    "                    if correctLabel.empty:\n",
    "                        correctLabel = 0\n",
    "                        correctLabel_count = 0\n",
    "                        total_sum_correct += correctLabel_count\n",
    "                    else:\n",
    "                        correctLabel_count = correctLabel['CorrectLabel'].sum()\n",
    "                        total_sum_correct += correctLabel_count\n",
    "                        correctLabel = (correctLabel_count / label_dist[psu].sum() * 100).round(2)\n",
    "                            \n",
    "                    # incorrect labels counts is total count of this psu bdgs in the dataset - correctLabels\n",
    "                    incorrectLabel_count = label_dist.loc[psu] - correctLabel_count\n",
    "                    total_sum_incorrect += incorrectLabel_count\n",
    "                    incorrectLabel = (incorrectLabel_count / label_dist[psu].sum() * 100).round(2)\n",
    "                        \n",
    "                    current_context.loc[psu] = [correctLabel, incorrectLabel]                        \n",
    "                        \n",
    "                aux_total_sum_correct = total_sum_correct\n",
    "                total_sum_correct = (total_sum_correct / (total_sum_correct + total_sum_incorrect) * 100).round(2)\n",
    "                total_sum_incorrect = (total_sum_incorrect / (aux_total_sum_correct + total_sum_incorrect) * 100).round(2)\n",
    "                            \n",
    "                current_context.loc[psu] = [total_sum_correct, total_sum_incorrect]\n",
    "                # append current context to final table\n",
    "                df_final = pd.concat([df_final, current_context], axis=1)\n",
    "                        \n",
    "    # save to file\n",
    "    df_final.to_csv(\"../data/results/finaltable_{}_{}.csv\".format(dataset, k))\n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-09T08:17:15.287000Z",
     "start_time": "2019-01-09T08:17:15.263221Z"
    }
   },
   "outputs": [],
   "source": [
    "def finalBarPlots(dataset, k, stacked=False, grouped='algorithm'):\n",
    "    df = pd.read_csv(\"../data/results/finaltable_{}_{}.csv\".format(dataset, k), index_col=0)\n",
    "    \n",
    "    contexts = ['weekday-average-kshape', 'weekday-average-kmeans', 'weekday-average-hierarchical',\n",
    "                'weekday-median-kshape', 'weekday-median-kmeans', 'weekday-median-hierarchical', \n",
    "                'weekend-average-kshape', 'weekend-average-kmeans', 'weekend-average-hierarchical',\n",
    "                'weekend-median-kshape', 'weekend-median-kmeans', 'weekend-median-hierarchical',\n",
    "                'fullweek-average-kshape', 'fullweek-average-kmeans', 'fullweek-average-hierarchical',\n",
    "                'fullweek-median-kshape', 'fullweek-median-kmeans', 'fullweek-median-hierarchical']\n",
    "\n",
    "    context_no_algo = ['weekday-average', 'weekday-median', \n",
    "                       'weekend-average','weekend-median',\n",
    "                       'fullweek-average','fullweek-median']\n",
    "    \n",
    "    algorithms = ['kshape', 'kmeans', 'hierarchical']\n",
    "    \n",
    "    # pre-process the dataframe\n",
    "    df_transpose = pd.DataFrame(df.T.reset_index())\n",
    "    df_transpose['index'] = df_transpose['index'].apply(lambda x: x.split('.')[0])\n",
    "    df_transpose.drop(df_transpose.index[[0, 1]], inplace=True)\n",
    "    \n",
    "    # insert context in one columm, each contex has two row values for correctly and incorrectly\n",
    "    df_aux = []\n",
    "    df_aux_single = []\n",
    "    for c in contexts:\n",
    "        df_aux.append(c)\n",
    "        df_aux.append(c)  \n",
    "        df_aux_single.append(c)\n",
    "    df_transpose.insert(0, 'contexts', '')\n",
    "    df_transpose['contexts'] = df_aux\n",
    "    \n",
    "    if stacked:\n",
    "        H = \"/\"\n",
    "        dfall = []\n",
    "    \n",
    "        # change grouping accordingly\n",
    "        if grouped == 'algorithm':\n",
    "            labels = context_no_algo\n",
    "            # generate sub dataframes based on algorithms\n",
    "            for context_algo in context_no_algo:\n",
    "                df_subset = pd.DataFrame(columns=['Correctly', 'Incorrectly'], index=algorithms)\n",
    "                df_context_sub = df_transpose[df_transpose['contexts'].str.contains(context_algo)]\n",
    "                for algo in algorithms:\n",
    "                    value = df_context_sub[df_context_sub['contexts'].str.contains(algo)]\n",
    "                    value = value['Total'].reset_index(drop=True)\n",
    "                    df_subset.loc[algo] = [value.iloc[0], value.iloc[1]]\n",
    "\n",
    "                dfall.append(df_subset)\n",
    "        elif grouped == 'context':\n",
    "            labels = algorithms\n",
    "            # generate sub dataframes based on contexts\n",
    "            for algo in algorithms:\n",
    "                df_subset = pd.DataFrame(columns=['Correctly', 'Incorrectly'], index=context_no_algo)\n",
    "                df_context_sub = df_transpose[df_transpose['contexts'].str.contains(algo)]\n",
    "                for context_algo in context_no_algo:\n",
    "                    value = df_context_sub[df_context_sub['contexts'].str.contains(context_algo)]\n",
    "                    value = value['Total'].reset_index(drop=True)\n",
    "                    df_subset.loc[context_algo] = [value.iloc[0], value.iloc[1]]\n",
    "\n",
    "                dfall.append(df_subset)\n",
    "        \n",
    "        n_df = len(dfall)\n",
    "        n_col = len(dfall[0].columns)\n",
    "        n_ind = len(dfall[0].index)\n",
    "\n",
    "        plt.figure(figsize=(16,14))\n",
    "        axe = plt.subplot(111)\n",
    "        \n",
    "        for df in dfall : # for each data frame\n",
    "            axe = df.plot(kind=\"bar\",\n",
    "                          linewidth=0,\n",
    "                          stacked=True,\n",
    "                          ax=axe,\n",
    "                          legend=False,\n",
    "                          grid=False)  # make bar plots\n",
    "\n",
    "        h,l = axe.get_legend_handles_labels() # get the handles we want to modify\n",
    "        for i in range(0, n_df * n_col, n_col): # len(h) = n_col * n_df\n",
    "            for j, pa in enumerate(h[i:i+n_col]):\n",
    "                for rect in pa.patches: # for each index\n",
    "                    rect.set_x(rect.get_x() + 0.8 / float(n_df + 1) * i / float(n_col))\n",
    "                    rect.set_hatch(H * int(i / n_col)) #edited part     \n",
    "                    rect.set_width(0.8 / float(n_df + 1))\n",
    "\n",
    "        axe.set_xticks((np.arange(0, 2 * n_ind, 2) + 1 / float(n_df + 1)) / 2.)\n",
    "        \n",
    "        if grouped == 'algorithm':\n",
    "            axe.set_xticklabels(df.index, rotation = 0, fontsize=30)\n",
    "        elif grouped == 'context':\n",
    "            axe.set_xticklabels(df.index, rotation = 45, fontsize=30)\n",
    "                \n",
    "        axe.set_title(\"Percentage of Classification using k = {} for {} dataset\".format(k, dataset), fontsize = 30)\n",
    "        axe.tick_params(axis=\"y\", labelsize=30)\n",
    "\n",
    "        axe.axhline(50, linewidth=2, color='r', linestyle=\"--\")\n",
    "\n",
    "        # Add invisible data to add another legend\n",
    "        n=[]        \n",
    "        for i in range(n_df):\n",
    "            n.append(axe.bar(0, 0, color=\"gray\", hatch=H * i))\n",
    "\n",
    "        l1 = axe.legend(h[:n_col], l[:n_col], loc=[1, 0.5], prop={'size': 30})#[1.01, 0.5])\n",
    "                \n",
    "        if labels is not None:\n",
    "            l2 = plt.legend(n, labels, loc=[1, 0.1], prop={'size': 30}) #[1.01, 0.1]\n",
    "        axe.add_artist(l1)\n",
    "        plt.savefig(\"../data/plots/finalBar_{}_{}_groupedBy_{}.png\".format(dataset, k, grouped), bbox_inches='tight')\n",
    "        return axe\n",
    "        \n",
    "    else:\n",
    "        # draw nested bar plot\n",
    "        g = sns.catplot(x=\"contexts\", y=\"Total\", hue=\"index\", data=df_transpose,\n",
    "                    height=10, kind=\"bar\", palette=\"muted\", legend=False)\n",
    "        g.set_xticklabels(rotation=90, fontsize=20)\n",
    "        g.set_yticklabels(fontsize=20)\n",
    "        g.set_ylabels(\"Classification Percentage\",fontsize=30)\n",
    "        g.set_xlabels(\"Contexts\", fontsize=30)\n",
    "        g.despine(left=True)\n",
    "        plt.legend(loc='upper left', prop={'size': 30})\n",
    "        g.savefig(\"../data/plots/finalBar_{}_{}_plots.png\".format(dataset, k), \n",
    "                                                                  bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profileDist(datasetName, context, function):\n",
    "    # load data\n",
    "    df_data = pd.read_csv('../data/processed/{}_{}_{}_dataset.csv'.format(datasetName, context, function), \n",
    "                          index_col=0)\n",
    "    \n",
    "    # load metadata\n",
    "    if datasetName == 'BDG':\n",
    "        df_meta = pd.read_csv('../data/raw/meta_open.csv')\n",
    "        # get labels for all buildings\n",
    "        ground_truth_labels = df_meta[df_meta.iloc[:, 0].isin(df_data.index.values)]\n",
    "        ground_truth_labels = ground_truth_labels['primaryspaceusage']\n",
    "        ground_truth_list = ['Office', 'Dormitory', 'College Classroom', 'Primary/Secondary Classroom', \n",
    "                        'College Laboratory']\n",
    "        colors = ['r', 'g', 'b', 'k', 'y']\n",
    "\n",
    "    else:\n",
    "        df_meta = pd.read_csv('../data/raw/dgs_metadata.csv')\n",
    "        df_aux = pd.read_csv(\"../data/raw/DGS_322_Buildings-15m-By_Building-DST-gap-filled-3-2-18-508pm.csv\")\n",
    "        \n",
    "        # get labels for all buildings\n",
    "        df_aux = df_aux.T\n",
    "        df_hist = df_aux[df_aux.iloc[:, 0].isin(df_data.index.values)] # get id based on names\n",
    "        df_hist = df_meta[df_meta['id'].isin(df_hist.index.values)] # get label based on id\n",
    "        ground_truth_labels = df_hist['espm_type_name']\n",
    "        \n",
    "    \n",
    "    ground_truth_labels = ground_truth_labels.reset_index(drop=True)\n",
    "\n",
    "    # Z-normalize the data\n",
    "    df_scaled = TimeSeriesScalerMeanVariance(mu=0.0, std=1.0).fit_transform(df_data.values)\n",
    "    # df.values will generate a 3d array (the third dimension being 1) so we convert it to a 2d array\n",
    "    df_scaled = np.squeeze(df_scaled) # now is a 2d array\n",
    "        \n",
    "    # back to pandas\n",
    "    df_scaled = pd.DataFrame(df_scaled)\n",
    "    df_scaled['psu'] = ground_truth_labels\n",
    "            \n",
    "    plt.figure(figsize=(18,10))\n",
    "    \n",
    "    for index, building in df_scaled.iterrows():       \n",
    "        if building[-1] == 'Office':\n",
    "            idx = 0\n",
    "        elif building[-1] == 'Dormitory':\n",
    "            idx = 1\n",
    "        elif building[-1] == 'College Classroom':\n",
    "            idx = 2\n",
    "        elif building[-1] == 'Primary/Secondary Classroom':\n",
    "            idx = 3\n",
    "        else:\n",
    "            idx = 4\n",
    "            \n",
    "        plt.plot(building[:-1], \"-\", alpha=0.4, label = building[-1]) # c = colors()dx\n",
    "    \n",
    "    # take care of repeating label and group them\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    by_label = OrderedDict(zip(labels, handles))\n",
    "    plt.legend(by_label.values(), by_label.keys(), loc='center left', bbox_to_anchor=(1, 0.55), prop={'size': 30})\n",
    "\n",
    "    # axis and title\n",
    "    plt.xticks(fontsize=20)\n",
    "    plt.yticks(fontsize=20)\n",
    "    plt.title(\"Load Profiles curves based on {}\".format(function), fontsize = 30)\n",
    "\n",
    "    plt.show()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T11:52:12.723195Z",
     "start_time": "2019-01-18T11:52:12.696144Z"
    }
   },
   "outputs": [],
   "source": [
    "def finalStackedPSU(dataset, context, algorithm, function, k):\n",
    "    df = pd.read_csv(\"../data/results/finaltable_{}_{}.csv\".format(dataset, k), index_col=0)\n",
    "\n",
    "    contexts = ['weekday-average-kshape', 'weekday-average-kmeans', 'weekday-average-hierarchical',\n",
    "                'weekday-median-kshape', 'weekday-median-kmeans', 'weekday-median-hierarchical', \n",
    "                'weekend-average-kshape', 'weekend-average-kmeans', 'weekend-average-hierarchical',\n",
    "                'weekend-median-kshape', 'weekend-median-kmeans', 'weekend-median-hierarchical',\n",
    "                'fullweek-average-kshape', 'fullweek-average-kmeans', 'fullweek-average-hierarchical',\n",
    "                'fullweek-median-kshape', 'fullweek-median-kmeans', 'fullweek-median-hierarchical']\n",
    "\n",
    "    context_no_algo = ['weekday-average', 'weekday-median', \n",
    "                       'weekend-average','weekend-median',\n",
    "                       'fullweek-average','fullweek-median']\n",
    "    \n",
    "    algorithms = ['kshape', 'kmeans', 'hierarchical']\n",
    "    \n",
    "    # pre-process the dataframe\n",
    "    df_transpose = pd.DataFrame(df.T.reset_index())\n",
    "    df_transpose['index'] = df_transpose['index'].apply(lambda x: x.split('.')[0])\n",
    "    df_transpose.drop(df_transpose.index[[0, 1]], inplace=True)\n",
    "    \n",
    "    # insert context in one columm, each contex has two row values for correctly and incorrectly\n",
    "    df_aux = []\n",
    "    df_aux_single = []\n",
    "    for c in contexts:\n",
    "        df_aux.append(c)\n",
    "        df_aux.append(c)  \n",
    "        df_aux_single.append(c)\n",
    "    df_transpose.insert(0, 'contexts', '')\n",
    "    df_transpose['contexts'] = df_aux\n",
    "    \n",
    "    filterContextAlgo = context + \"-\" + algorithm + \"-\" + function\n",
    "    df_transpose = df_transpose[df_transpose['contexts'] == filterContextAlgo]\n",
    "    \n",
    "    df = df_transpose.T\n",
    "    df = df.drop(df.index[0:2])\n",
    "    df = df.drop(df.index[-1])\n",
    "    \n",
    "    print(df)\n",
    "    plt.figure(figsize=(20,18))\n",
    "    axe = plt.subplot(111)\n",
    "    \n",
    "    p1 = plt.bar(df.index.values, 100, label='Outside Main Cluster')# incorrect\n",
    "    p2 = plt.bar(df.index.values, df.iloc[:, 0], label='Grouped Together') # correct\n",
    "    axe.axhline(50, linewidth=2, color='r', linestyle=\"--\")\n",
    "    plt.legend(loc='best', prop={'size': 40})\n",
    "\n",
    "    plt.ylabel('Percentage (%)', fontsize=30)\n",
    "    plt.xticks(fontsize=30, rotation=90)\n",
    "    plt.yticks(fontsize=30)\n",
    "    plt.rc('xtick',labelsize=30)\n",
    "    plt.rc('ytick',labelsize=30)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    fig = axe.get_figure()\n",
    "    \n",
    "    fig.savefig('../data/plots/finalStackedPSU_{}_{}_{}_{}_{}.png'.format(dataset, context, function, algorithm, k))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-29T02:03:07.412539Z",
     "start_time": "2019-01-29T02:03:07.393351Z"
    }
   },
   "outputs": [],
   "source": [
    "def findOffice(datasetName, context, function, algorithm, k):\n",
    "    # load data\n",
    "    df_data = pd.read_csv('../data/processed/{}_{}_{}_dataset.csv'.format(datasetName, context, function), \n",
    "                          index_col=0)\n",
    "    # load metadata\n",
    "    df_meta = pd.read_csv('../data/raw/meta_open.csv')\n",
    "    ground_truth_list = ['Office', 'Dormitory', 'College Classroom', 'Primary/Secondary Classroom', \n",
    "                         'College Laboratory']\n",
    "    df_og_psu = df_meta[df_meta.iloc[:, 0].isin(df_data.index.values)]\n",
    "    df_og_psu = df_og_psu['primaryspaceusage'].value_counts()\n",
    "        \n",
    "    # Z-normalize the data\n",
    "    df_scaled = TimeSeriesScalerMeanVariance(mu=0.0, std=1.0).fit_transform(df_data.values)\n",
    "    # df.values will generate a 3d array (the third dimension being 1) so we convert it to a 2d array\n",
    "    df_scaled = np.squeeze(df_scaled) # now is a 2d array\n",
    "    \n",
    "    # run algorithm\n",
    "    model, labels = doClustering(df_scaled, datasetName, algorithm, k, seed=3, max_iter=300)\n",
    "\n",
    "    # buildings in c0\n",
    "    \n",
    "    # get number of elements in each cluster\n",
    "    data_dict = {i: np.where(labels == i)[0] for i in range(k)}\n",
    "\n",
    "    df_dist = pd.DataFrame(index = ground_truth_list)\n",
    "\n",
    "    # for each building in a cluster\n",
    "    for key, value in data_dict.items():\n",
    "        bdg_ids = df_data.iloc[value.tolist()] # retrieve original building name based on index\n",
    "        \n",
    "        # resample based on building id\n",
    "        ground_truth_labels = df_meta[df_meta.iloc[:, 0].isin(bdg_ids.index.values)]\n",
    "        # count the times the different ground truth labels exist in each cluster\n",
    "        aux = ground_truth_labels['primaryspaceusage'].value_counts()\n",
    "        \n",
    "        if key == 0: # all buildings with labels on cluster 0\n",
    "            df_c0 = ground_truth_labels\n",
    "        elif key == 1: # all buildings with labels on cluster 1\n",
    "            df_c1 = ground_truth_labels\n",
    "\n",
    "        df_dist[key] = aux    \n",
    "        # end of for loop    \n",
    "    \n",
    "    \n",
    "    df_c0_office = df_c0[df_c0['primaryspaceusage'] == 'Office']\n",
    "    df_c1_office = df_c1[df_c1['primaryspaceusage'] == 'Office']\n",
    "\n",
    "    # print names of only offices for both clusters\n",
    "    print(df_c0_office['uid'])\n",
    "    print(df_c1_office['uid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-29T02:03:07.814681Z",
     "start_time": "2019-01-29T02:03:07.696395Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5       Office_Cristina\n",
      "8          Office_Jesus\n",
      "42          Office_Jett\n",
      "45         Office_Jerry\n",
      "49          Office_Lesa\n",
      "59        Office_Jackie\n",
      "113       Office_Jayden\n",
      "150      Office_Georgia\n",
      "152       Office_Skyler\n",
      "164         Office_Joan\n",
      "171        Office_Javon\n",
      "174       Office_Jeanne\n",
      "186      Office_Scottie\n",
      "196         Office_John\n",
      "237        Office_Gemma\n",
      "256         Office_Lena\n",
      "261      Office_Gustavo\n",
      "266      Office_Garrett\n",
      "274    Office_Shawnette\n",
      "292      Office_Lillian\n",
      "316       Office_Glenda\n",
      "326     Office_Gisselle\n",
      "327      Office_Phebian\n",
      "331          Office_Jan\n",
      "335      Office_Jackson\n",
      "339       Office_Gloria\n",
      "344       Office_Sinead\n",
      "350        Office_Shari\n",
      "354         Office_Jude\n",
      "368         Office_Joni\n",
      "378       Office_Gladys\n",
      "379       Office_Stella\n",
      "381       Office_Louise\n",
      "385    Office_Guillermo\n",
      "388     Office_Gabriela\n",
      "406        Office_Glenn\n",
      "413       Office_Leland\n",
      "438       Office_Garman\n",
      "489       Office_Shelly\n",
      "500        Office_Luann\n",
      "502         Office_Lane\n",
      "Name: uid, dtype: object\n",
      "105       Office_Conrad\n",
      "156       Office_Ayesha\n",
      "176       Office_Pamela\n",
      "177      Office_Allyson\n",
      "179       Office_Amelie\n",
      "183          Office_Ava\n",
      "185        Office_Caleb\n",
      "190        Office_Abbey\n",
      "207      Office_Clifton\n",
      "218       Office_Alyson\n",
      "243       Office_Corbin\n",
      "244       Office_Aubrey\n",
      "245       Office_Autumn\n",
      "246      Office_Abigail\n",
      "265        Office_Penny\n",
      "273       Office_Annika\n",
      "275         Office_Cody\n",
      "277      Office_Charles\n",
      "281          Office_Pam\n",
      "287        Office_Asher\n",
      "293           Office_Al\n",
      "295     Office_Clarissa\n",
      "297       Office_Andrea\n",
      "304          Office_Pat\n",
      "313       Office_Angelo\n",
      "314    Office_Anastasia\n",
      "330       Office_Amelia\n",
      "334     Office_Angelina\n",
      "337    Office_Catherine\n",
      "357     Office_Angelica\n",
      "372        Office_Paige\n",
      "386      Office_Paulina\n",
      "394     Office_Carolina\n",
      "400      Office_Alannah\n",
      "401       Office_Aliyah\n",
      "402         Office_Curt\n",
      "419        Office_Ayden\n",
      "424        Office_Colby\n",
      "425      Office_Carissa\n",
      "427        Office_Paula\n",
      "435      Office_Ashanti\n",
      "440         Office_Cora\n",
      "454      Office_Clinton\n",
      "458     Office_Paulette\n",
      "461        Office_Perla\n",
      "462     Office_Precious\n",
      "463     Office_Patricia\n",
      "468     Office_Pasquale\n",
      "497      Office_Pauline\n",
      "503      Office_Cameron\n",
      "Name: uid, dtype: object\n"
     ]
    }
   ],
   "source": [
    "dataset = 'BDG'\n",
    "context = 'weekday'\n",
    "function = 'average'\n",
    "algorithm  = 'kmeans'\n",
    "k = 3\n",
    "findOffice(dataset, context, function, algorithm, k)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

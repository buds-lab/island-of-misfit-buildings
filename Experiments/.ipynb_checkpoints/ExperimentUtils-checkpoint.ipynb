{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T11:57:03.237939Z",
     "start_time": "2018-12-21T11:57:03.235541Z"
    }
   },
   "source": [
    "# This notebook will be the only interaction with all any experiment notebook. From here the function calls to all the preprocessing and clustering notebooks will be done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T06:43:44.742681Z",
     "start_time": "2018-12-23T06:43:44.740212Z"
    }
   },
   "outputs": [],
   "source": [
    "# Libraries needed\n",
    "# !pip install nbimporter # uncomment if library is not install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T06:43:47.289144Z",
     "start_time": "2018-12-23T06:43:44.748857Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Jupyter notebook from /Users/matias/Documents/Education/Graduate/NUS/Projects/sensor-cluster-er/Preprocessing/preprocessing.ipynb\n",
      "Importing Jupyter notebook from /Users/matias/Documents/Education/Graduate/NUS/Projects/sensor-cluster-er/Preprocessing/context_extraction.ipynb\n",
      "Importing Jupyter notebook from /Users/matias/Documents/Education/Graduate/NUS/Projects/sensor-cluster-er/Preprocessing/load_cuve_generation.ipynb\n",
      "Importing Jupyter notebook from /Users/matias/Documents/Education/Graduate/NUS/Projects/sensor-cluster-er/ClusteringAnalysis/ClusteringValidationMetrics.ipynb\n",
      "Importing Jupyter notebook from /Users/matias/Documents/Education/Graduate/NUS/Projects/sensor-cluster-er/ClusteringAnalysis/Kshape.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Existing Notebooks\n",
    "import nbimporter\n",
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from Preprocessing.preprocessing import hourly_dataset\n",
    "from Preprocessing.context_extraction import getContext\n",
    "from Preprocessing.load_cuve_generation import doAggregation\n",
    "\n",
    "from ClusteringAnalysis.ClusteringValidationMetrics import get_validation_scores, combineMetrics\n",
    "from ClusteringAnalysis.Kshape import doKshape\n",
    "# %run ./ClusteringAnalysis/ClusteringValidationMetrics\n",
    "# !jupyter nbconvert --execute ../ClusteringAnalysis/ClusteringValidationMetrics\n",
    "\n",
    "# Built-in libraries\n",
    "import time\n",
    "from itertools import product\n",
    "from math import log\n",
    "import pickle\n",
    "\n",
    "# NumPy, SciPy and Pandas\n",
    "from scipy.spatial.distance import cdist\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Scikit-Learn\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "# Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T06:43:47.293568Z",
     "start_time": "2018-12-23T06:43:47.290938Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T06:43:47.305634Z",
     "start_time": "2018-12-23T06:43:47.295839Z"
    }
   },
   "outputs": [],
   "source": [
    "# Running Experiment \n",
    "def runExperiment(datasetName, context, function, algorithm='kshape', \n",
    "                  algo_parameter=range(2,11), validation_metrics='all'):\n",
    "\n",
    "    # check if dataset has already being processed before\n",
    "    exists_df = os.path.isfile('../data/processed/{}_dataset.csv'.format(datasetName))\n",
    "    if exists_df: # if file exists, read it\n",
    "        df = pd.read_csv('../data/processed/{}_dataset.csv'.format(datasetName), index_col=0)\n",
    "        print(\"Preprocessed dataset already exists, loading it ...\")\n",
    "    else: # if file is missing, produce it\n",
    "        df = hourly_dataset(datasetName)\n",
    "        print(\"Preprocessing dataset ...\")\n",
    "        \n",
    "    # check if dataset with context has already being processed before\n",
    "    exists_context = os.path.isfile('../data/processed/{}_{}_dataset.csv'.format(datasetName, context))\n",
    "    if exists_context: # if file exists, read it\n",
    "        df = pd.read_csv('../data/processed/{}_{}_dataset.csv'.format(datasetName, context), index_col=0)\n",
    "        print(\"Dataset with {} context already already exists, loading it ...\".format(context))\n",
    "    else: # if file is missing, produce it\n",
    "        df = getContext(datasetName, context)\n",
    "        print(\"Generating context dataset ...\")\n",
    "        \n",
    "    # check if dataset with function has already being processed before\n",
    "    exists_function = os.path.isfile('../data/processed/{}_{}_{}_dataset.csv'.format(datasetName, context, function))\n",
    "    if exists_function: # if file exists, read it\n",
    "        df = pd.read_csv('../data/processed/{}_{}_{}_dataset.csv'.format(datasetName, context, function), index_col=0)\n",
    "        print(\"Dataset with {} context and {} load curve aggregation function already already exists, loading it ...\".format(context, function))\n",
    "\n",
    "    else: # if file is missing, produce it\n",
    "        df = doAggregation(datasetName, context, function)\n",
    "        print(\"Generating load curves based on {} ...\".format(function))\n",
    "    \n",
    "    scores = [] # list of the scores for each parameter for the selected algorithm\n",
    "    \n",
    "    # run selected algorithm with the appropiate parameter\n",
    "    if algorithm == 'kshape':\n",
    "        # run kshape for each value of K\n",
    "        for k in algo_parameter:\n",
    "            model, labels = doKshape(df, datasetName, k, seed=3, max_iter=200)\n",
    "            scores.append(get_validation_scores(df.values, labels))\n",
    "            print(\"Running experiment with {} and k = {}\".format(algorithm, k))\n",
    "   \n",
    "    # name for saving the results\n",
    "    obj_name = '../data/results/{}_{}_{}_{}_scores'.format(datasetName, context, function, algorithm)\n",
    "    \n",
    "    # update the final score dataframe\n",
    "    scores = pd.DataFrame.from_dict(scores)\n",
    "    scores.insert(0, 'dataset', '')\n",
    "    scores['dataset'] = datasetName\n",
    "    scores.insert(1, 'context', '')\n",
    "    scores['context'] = context\n",
    "    scores.insert(2, 'function', '')\n",
    "    scores['function'] = function\n",
    "    scores.insert(3, 'algorithm', '')\n",
    "    scores['algorithm'] = algorithm\n",
    "    if \"k\" in algorithm:\n",
    "        scores.insert(4, 'parameter k', '')\n",
    "        scores['parameter k'] = algo_parameter\n",
    "        \n",
    "    # save as python pickle\n",
    "    f = open(obj_name + '.pkl', 'wb')\n",
    "    pickle.dump(scores, f)\n",
    "    f.close\n",
    "    \n",
    "    # save as csv\n",
    "    scores.to_csv('{}.csv'.format(obj_name))\n",
    "    print('Scores saved in {}.csv'.format(obj_name))\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T06:43:47.310259Z",
     "start_time": "2018-12-23T06:43:47.307961Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualizing Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T06:43:47.315761Z",
     "start_time": "2018-12-23T06:43:47.312836Z"
    }
   },
   "outputs": [],
   "source": [
    "# # plotting curves based on metrics\n",
    "# i = 10\n",
    "\n",
    "# filename = [\"data/BDG_avg_weekday_kshape_scores.pkl\", \"data/BDG_median_weekday_kshape_scores.pkl\", \"data/BDG_regression_weekday_kshape_scores.pkl\",\n",
    "#             \"data/BDG_avg_weekend_kshape_scores.pkl\", \"data/BDG_median_weekend_kshape_scores.pkl\", \"data/BDG_regression_weekend_kshape_scores.pkl\",\n",
    "#             \"data/DC_avg_weekday_kshape_scores.pkl\", \"data/DC_median_weekday_kshape_scores.pkl\", \"data/DC_regression_weekday_kshape_scores.pkl\",\n",
    "#             \"data/DC_avg_weekend_kshape_scores.pkl\", \"data/DC_median_weekend_kshape_scores.pkl\", \"data/DC_regression_weekend_kshape_scores.pkl\"]\n",
    "\n",
    "# # read pickle files for each dataset\n",
    "# rangeK = range(2, 11)\n",
    "# pickle_in = open(filename[i], \"rb\")\n",
    "# df_scores = pickle.load(pickle_in)\n",
    "# pickle_in.close()\n",
    "\n",
    "# # print(df_scores[0])\n",
    "# # combine all metrics for each K value (of current dataset)\n",
    "# k_metrics = combineMetrics(df_scores[0], rangeK)\n",
    "\n",
    "# metric_index = 0\n",
    "# f, axarr = plt.subplots(len(k_metrics), sharex=False, figsize =(10,30))\n",
    "# # iterate through every metric and plot the value versus the correspondant k value\n",
    "# for metric in k_metrics.keys():\n",
    "#     axarr[metric_index].plot(rangeK, k_metrics.get(metric), \"k-\")\n",
    "#     axarr[metric_index].set_title(\"{} curve over K values\".format(metric), fontsize = 18)\n",
    "\n",
    "#     metric_index += 1\n",
    "# plt.show()\n",
    "# f.savefig(\"img/{}_kshape_plot.png\".format(dataframes_names[i]), bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

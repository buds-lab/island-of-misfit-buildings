{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T11:57:03.237939Z",
     "start_time": "2018-12-21T11:57:03.235541Z"
    }
   },
   "source": [
    "# This notebook will be the only interaction with all any experiment notebook. From here the function calls to all the preprocessing and clustering notebooks will be done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T10:17:45.561980Z",
     "start_time": "2018-12-23T10:17:45.559252Z"
    }
   },
   "outputs": [],
   "source": [
    "# Libraries needed\n",
    "# !pip install nbimporter # uncomment if library is not install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-29T13:46:58.380760Z",
     "start_time": "2018-12-29T13:46:55.810869Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Jupyter notebook from /Users/matias/Documents/Education/Graduate/NUS/Projects/sensor-cluster-er/Preprocessing/preprocessing.ipynb\n",
      "Importing Jupyter notebook from /Users/matias/Documents/Education/Graduate/NUS/Projects/sensor-cluster-er/Preprocessing/context_extraction.ipynb\n",
      "Importing Jupyter notebook from /Users/matias/Documents/Education/Graduate/NUS/Projects/sensor-cluster-er/Preprocessing/load_cuve_generation.ipynb\n",
      "Importing Jupyter notebook from /Users/matias/Documents/Education/Graduate/NUS/Projects/sensor-cluster-er/ClusteringAnalysis/ClusteringValidationMetrics.ipynb\n",
      "Importing Jupyter notebook from /Users/matias/Documents/Education/Graduate/NUS/Projects/sensor-cluster-er/ClusteringAnalysis/ClusteringAlgorithms.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Existing Notebooks\n",
    "import nbimporter\n",
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from Preprocessing.preprocessing import hourly_dataset\n",
    "from Preprocessing.context_extraction import getContext\n",
    "from Preprocessing.load_cuve_generation import doAggregation\n",
    "\n",
    "from ClusteringAnalysis.ClusteringValidationMetrics import get_validation_scores\n",
    "from ClusteringAnalysis.ClusteringAlgorithms import doClustering\n",
    "\n",
    "# Built-in libraries\n",
    "import time\n",
    "from itertools import product\n",
    "from math import log\n",
    "import pickle\n",
    "from datetime import datetime, timedelta # testing\n",
    "from collections import OrderedDict\n",
    "\n",
    "# NumPy, SciPy and Pandas\n",
    "from scipy.spatial.distance import cdist\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Scikit-Learn\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "# Tslearn\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "from tslearn.utils import to_time_series_dataset\n",
    "# from tslearn.clustering import silhouette_score\n",
    "\n",
    "# Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.gridspec as gridspec\n",
    "import plotly.graph_objs as go\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T11:01:20.315851Z",
     "start_time": "2018-12-23T11:01:20.307773Z"
    }
   },
   "outputs": [],
   "source": [
    "# check files\n",
    "def checkFiles(datasetName, context, function):\n",
    "    # if the dataset is the combination, directly load the aggregated dataset for the datasets\n",
    "    if datasetName == 'BDG-DGS':\n",
    "        df1_name = 'BDG'\n",
    "        df2_name = 'DGS'\n",
    "\n",
    "        df1 = pd.read_csv('../data/processed/{}_{}_{}_dataset.csv'.format(df1_name, context, function), index_col=0)\n",
    "        df2 = pd.read_csv('../data/processed/{}_{}_{}_dataset.csv'.format(df2_name, context, function), index_col=0)\n",
    "    \n",
    "        df = df1.append(df2)\n",
    "        df.to_csv('../data/processed/{}_{}_{}_dataset.csv'.format(datasetName, context, function))\n",
    "    else:\n",
    "        # check if dataset has already being processed before\n",
    "        exists_df = os.path.isfile('../data/processed/{}_dataset.csv'.format(datasetName))\n",
    "        if exists_df: # if file exists, read it\n",
    "            df = pd.read_csv('../data/processed/{}_dataset.csv'.format(datasetName), index_col=0)\n",
    "            print(\"Preprocessed dataset already exists, loading it ...\")\n",
    "        else: # if file is missing, produce it\n",
    "            df = hourly_dataset(datasetName)\n",
    "            print(\"Preprocessing dataset ...\")\n",
    "\n",
    "        # check if dataset with context has already being processed before\n",
    "        exists_context = os.path.isfile('../data/processed/{}_{}_dataset.csv'.format(datasetName, context))\n",
    "        if exists_context: # if file exists, read it\n",
    "            df = pd.read_csv('../data/processed/{}_{}_dataset.csv'.format(datasetName, context), index_col=0)\n",
    "            print(\"Dataset with {} context already exists, loading it ...\".format(context))\n",
    "        else: # if file is missing, produce it\n",
    "            df = getContext(datasetName, context)\n",
    "            print(\"Generating context dataset ...\")\n",
    "\n",
    "        # check if dataset with function has already being processed before\n",
    "        exists_function = os.path.isfile('../data/processed/{}_{}_{}_dataset.csv'.format(datasetName, context, function))\n",
    "        if exists_function: # if file exists, read it\n",
    "            df = pd.read_csv('../data/processed/{}_{}_{}_dataset.csv'.format(datasetName, context, function), index_col=0)\n",
    "            print(\"Dataset with {} context and {} load curve aggregation function already exists, loading it ...\".format(context, function))\n",
    "\n",
    "        else: # if file is missing, produce it\n",
    "            df = doAggregation(datasetName, context, function)\n",
    "            print(\"Generating load curves based on {} ...\".format(function))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T04:07:48.500160Z",
     "start_time": "2018-12-27T04:07:48.471459Z"
    }
   },
   "outputs": [],
   "source": [
    "# Running Experiment \n",
    "def runExperiment(datasetName, context, function, algorithm='kshape', \n",
    "                  algo_parameter=range(2,11), validation_metrics='all', appendTotalFile=False):\n",
    "    print(\"Running Experiment with dataset: {}, context:  {}, function: {}, algorithm: {}\".format(datasetName, \n",
    "                                                                                                context,\n",
    "                                                                                                function,\n",
    "                                                                                                algorithm))\n",
    "    # check if file exists and load it\n",
    "    df = checkFiles(datasetName, context, function)\n",
    "    \n",
    "    # Z-normalize the data\n",
    "    df_scaled = TimeSeriesScalerMeanVariance(mu=0.0, std=1.0).fit_transform(df.values)\n",
    "    # df.values will generate a 3d array (the third dimension being 1) so we convert it to a 2d array\n",
    "    df_scaled = np.squeeze(df_scaled) # now is a 2d array\n",
    "    \n",
    "    scores = [] # list of the scores for each parameter for the selected algorithm\n",
    "    \n",
    "    # run selected algorithm with the appropiate parameter\n",
    "    for k in algo_parameter:\n",
    "        model, labels = doClustering(df_scaled, datasetName, algorithm, k, seed=3, max_iter=200)\n",
    "        scores.append(get_validation_scores(df_scaled, labels))\n",
    "        print(\"Running experiment with {} and k = {}\".format(algorithm, k))\n",
    "    \n",
    "    # name for saving the results\n",
    "    obj_name = '../data/results/{}_{}_{}_{}_scores'.format(datasetName, context, function, algorithm)\n",
    "    \n",
    "    # update the final score dataframe\n",
    "    scores = pd.DataFrame.from_dict(scores)\n",
    "    scores.insert(0, 'dataset', '')\n",
    "    scores['dataset'] = datasetName\n",
    "    scores.insert(1, 'context', '')\n",
    "    scores['context'] = context\n",
    "    scores.insert(2, 'function', '')\n",
    "    scores['function'] = function\n",
    "    scores.insert(3, 'algorithm', '')\n",
    "    scores['algorithm'] = algorithm\n",
    "    if \"k\" in algorithm or algorithm == 'hierarchical':\n",
    "        scores.insert(4, 'parameter k', '')\n",
    "        scores['parameter k'] = algo_parameter\n",
    "    \n",
    "    # approximate to two decimals\n",
    "    scores = scores.round(2)\n",
    "    \n",
    "    # save as python pickle\n",
    "    f = open(obj_name + '.pkl', 'wb')\n",
    "    pickle.dump(scores, f)\n",
    "    f.close\n",
    "    \n",
    "    # save as csv\n",
    "    scores.to_csv('{}.csv'.format(obj_name))\n",
    "    print(\"Scores saved in {}.csv\\n\".format(obj_name)) # individual file\n",
    "    \n",
    "    if appendTotalFile:\n",
    "        with open('../data/results/total_scores.csv', 'a') as f: # append to general file\n",
    "            scores.to_csv(f, header=False)\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run batch of experiments\n",
    "def runBatchExperiments(datasetName_list, context_list, function_list, algorithm_list, \n",
    "                  algo_parameter=range(2,11), validation_metrics='all'):\n",
    "    # nested loops for each possible combination of the lists\n",
    "    for datasetName in datasetName_list: # every dataset\n",
    "        for context in context_list: # every context\n",
    "            for function in function_list: # every load aggregation function\n",
    "                for algorithm in algorithm_list: # every algorithm\n",
    "                    runExperiment(datasetName, context, function, algorithm, algo_parameter, validation_metrics, True)\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T10:17:48.118758Z",
     "start_time": "2018-12-23T10:17:48.115255Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generating clusters and centroids\n",
    "def generateClusters(datasetName, context, function, algorithm='kshape', algo_parameter = 5, psu=False):\n",
    "    # check if file exists and load it\n",
    "    df = checkFiles(datasetName, context, function)\n",
    "        \n",
    "    # Z-normalize the data\n",
    "    df_scaled = TimeSeriesScalerMeanVariance(mu=0.0, std=1.0).fit_transform(df.values)\n",
    "    # df.values will generate a 3d array (the third dimension being 1) so we convert it to a 2d array\n",
    "    df_scaled = np.squeeze(df_scaled) # now is a 2d array\n",
    "        \n",
    "    # run selected algorithm with the appropiate parameter\n",
    "    for k in algo_parameter:\n",
    "        model, labels = doClustering(df_scaled, datasetName, algorithm, k, seed=3, max_iter=300, plot=True)    \n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateClustersPSU(datasetName, context, function, algorithm='kshape', algo_parameter = 5):\n",
    "\n",
    "    # load metadata\n",
    "    if datasetName == 'BDG':\n",
    "        df_meta = pd.read_csv('../data/raw/meta_open.csv')\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "    # check if file exists and load it\n",
    "    df = checkFiles(datasetName, context, function)\n",
    "        \n",
    "    # Z-normalize the data\n",
    "    df_scaled = TimeSeriesScalerMeanVariance(mu=0.0, std=1.0).fit_transform(df.values)\n",
    "    # df.values will generate a 3d array (the third dimension being 1) so we convert it to a 2d array\n",
    "    df_scaled = np.squeeze(df_scaled) # now is a 2d array\n",
    "         \n",
    "    # get labels for all buildings\n",
    "    ground_truth_labels = df_meta[df_meta.iloc[:, 0].isin(df.index.values)]\n",
    "    ground_truth_labels = ground_truth_labels['primaryspaceusage']\n",
    "    ground_truth_labels = ground_truth_labels.reset_index(drop=True)\n",
    "    \n",
    "    # back to pandas\n",
    "#     df_scaled = pd.DataFrame(df_scaled)\n",
    "#     df_scaled['psu'] = ground_truth_labels\n",
    "            \n",
    "    ground_truth_list = ['Office', 'Dormitory', 'College Classroom', 'Primary/Secondary Classroom', \n",
    "                        'College Laboratory']\n",
    "    colors = ['r', 'g', 'b', 'k', 'y']\n",
    "    \n",
    "    # run selected algorithm with the appropiate parameter\n",
    "    for k in algo_parameter:\n",
    "        model, labels = doClustering(df_scaled, datasetName, algorithm, k, seed=3, max_iter=300)\n",
    "        \n",
    "        if algorithm == 'kshape':\n",
    "            # extract parameters\n",
    "            cluster_centers = []\n",
    "            y_pred = pd.DataFrame()\n",
    "            y_pred.loc[:, 0] = [0] * df_scaled.shape[0] # initilize cluster membership\n",
    "            # for each tuple\n",
    "            for yi in range(k):\n",
    "                cluster_centers.append(model[yi][0]) # get cluster centers as first element of tuple\n",
    "                y_pred.loc[model[yi][1], 0] = yi # update cluster membership\n",
    "            # make them a list\n",
    "            y_pred = y_pred.iloc[:, 0].values \n",
    "        else:\n",
    "            y_pred = model.fit_predict(df_scaled) # fit the data and generate the cluster labels\n",
    "        \n",
    "        # back to pandas\n",
    "        df_scaled = pd.DataFrame(df_scaled)\n",
    "        df_scaled['psu'] = ground_truth_labels\n",
    "\n",
    "        # plot for each cluster\n",
    "        fig = plt.figure(figsize=(20, 40))\n",
    "\n",
    "        for yi in range(k):\n",
    "            plt.subplot(k, 1, 1 + yi)\n",
    "            \n",
    "            # for each time series in current cluster\n",
    "            for index, building in df_scaled[y_pred == yi].iterrows():       \n",
    "                if building[-1] == 'Office':\n",
    "                    idx = 0\n",
    "                elif building[-1] == 'Dormitory':\n",
    "                    idx = 1\n",
    "                elif building[-1] == 'College Classroom':\n",
    "                    idx = 2\n",
    "                elif building[-1] == 'Primary/Secondary Classroom':\n",
    "                    idx = 3\n",
    "                else:\n",
    "                    idx = 4\n",
    "\n",
    "                plt.plot(building[:-1], \"-\", alpha=0.25, label = building[-1], c = colors[idx])\n",
    "\n",
    "                plt.xlim(0, 23)\n",
    "            plt.ylim(-4, 4)\n",
    "            plt.title(\"Cluster %d\" % (yi + 1), fontsize = 30)\n",
    "                # take care of repeating label and group them\n",
    "            handles, labels = plt.gca().get_legend_handles_labels()\n",
    "            by_label = OrderedDict(zip(labels, handles))\n",
    "            plt.legend(by_label.values(), by_label.keys(), loc='center left', bbox_to_anchor=(1, 0.55), prop={'size': 20})\n",
    "\n",
    "        fig.suptitle(\"Dataset: {}\".format(datasetName), fontsize = 35)\n",
    "                \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotSilhouette(datasetName, context, function, algorithm, k):\n",
    "    plt.ioff() # this way only plt.show() will display figures\n",
    "    \n",
    "    df = pd.read_csv('../data/processed/{}_{}_{}_dataset.csv'.format(datasetName, context, function), index_col=0)\n",
    "     \n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, ax1 = plt.subplots(1, 1)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (k+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(df.values) + (k + 1) * 10])\n",
    "\n",
    "    df_scaled = TimeSeriesScalerMeanVariance(mu=0.0, std=1.0).fit_transform(df.values)\n",
    "    df_scaled = np.squeeze(df_scaled)\n",
    "    \n",
    "    clusterer, cluster_labels = doClustering(df_scaled, datasetName, algorithm, k, seed=3, max_iter=300)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    \n",
    "    silhouette_avg = round(silhouette_score(df_scaled, cluster_labels), 2) # round to two decimals\n",
    "    print(\"For k =\", k, \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(df_scaled, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(k):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / k)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i), fontsize = 30)\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"Silhouette analysis for {} {} using {}\".format(context, function, algorithm), fontsize = 30)\n",
    "    ax1.set_xlabel(\"Silhouette coefficient values\", fontsize = 30)\n",
    "    ax1.set_ylabel(\"Cluster label\", fontsize = 30)\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.3,-0.2, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    plt.rc('xtick',labelsize=20)\n",
    "    plt.rc('ytick',labelsize=20)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotCurvesOneBuilding(datasetName, context, function, resolution='day'):\n",
    "    dataframe = pd.read_csv('../data/processed/{}_{}_dataset.csv'.format(datasetName, context), parse_dates=True, \n",
    "                            infer_datetime_format=True, index_col=0)\n",
    "    df_load_curves = pd.DataFrame() # dataframe that will hold all load curves\n",
    "\n",
    "    # resample based on parameter\n",
    "    if (resolution == 'day'):\n",
    "        availableSamples = (dataframe.resample('1D').asfreq()).index # get list of timestamps group by day\n",
    "        delta = 23 # timedelta based on resample\n",
    "    else:\n",
    "        print(\"Please choose a valid resolution\")\n",
    "        exit()\n",
    "\n",
    "    # iterate through all buildings (column)\n",
    "    for column in range(len(dataframe.columns)):\n",
    "        df_sampledReadings = pd.DataFrame() # dataframe to hold new samples for a column\n",
    "        currentColumn = pd.DataFrame(dataframe.iloc[:, column])\n",
    "        \n",
    "        # iterate through each day\n",
    "        for timestamp in availableSamples:\n",
    "            # update time limits to the current date\n",
    "            start = timestamp\n",
    "            end = timestamp + timedelta(hours=delta)\n",
    "            # get meter data from only this resolution\n",
    "            df_reading = currentColumn[(currentColumn.index >= start) & (currentColumn.index <= end)]\n",
    "            # ignore index since they are unique timestamps\n",
    "            df_reading.reset_index(drop=True, inplace=True)         \n",
    "            # append new sample as columns\n",
    "            df_sampledReadings = pd.concat([df_sampledReadings, df_reading], axis=1)\n",
    "            \n",
    "        # make sure sure there are no columns with NaN values\n",
    "        df_sampledReadings.dropna(axis=1, how='all', inplace=True)\n",
    "        df_sampledReadings = df_sampledReadings.T # transpose it so it's easier to see and operate\n",
    "        # up to this point, the matrix above has the shape nxm where is the number of instances and m is the number of readings\n",
    "    \n",
    "        # if any NaN prevailed\n",
    "        df_sampledReadings.fillna(value=0, inplace=True) \n",
    "\n",
    "        # calculate load curve based on function\n",
    "        if function == 'average':\n",
    "            load_curve = np.mean(df_sampledReadings, axis = 0)\n",
    "\n",
    "        elif function =='median':\n",
    "            load_curve = np.median(df_sampledReadings, axis = 0)\n",
    "\n",
    "        else:\n",
    "            print(\"Please choose a valid context\")\n",
    "            exit()\n",
    "\n",
    "        ###################################################################\n",
    "#         TODO: coding is for plotting purposes\n",
    "        plt.figure(figsize=(18,10))\n",
    "        plt.ylim(0,7)\n",
    "        x_axis = range(0, len(df_sampledReadings.columns))\n",
    "        for _, curve in df_sampledReadings.iterrows():\n",
    "            plt.plot(curve, \"k-\", alpha=.2)\n",
    "#         plt.plot(load_curve, \"r-\")\n",
    "        plt.title(\"Load Profiles and red representative curve based on {}\".format(function))        \n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(18,10))\n",
    "\n",
    "        plt.plot(load_curve, \"r-\", linewidth=7.0)\n",
    "        \n",
    "        plt.ylim(0,7)\n",
    "        \n",
    "        ###################################################################\n",
    "\n",
    "        # turn into one column dataframe for easier manipulation\n",
    "        load_curve = pd.DataFrame(load_curve)\n",
    "        # keep the instance name as column name\n",
    "        instance_name = []\n",
    "        instance_name.append(df_sampledReadings.index[0])\n",
    "        load_curve.columns = instance_name\n",
    "        # append current load curve to dataframe\n",
    "        df_load_curves = pd.concat([df_load_curves, load_curve], axis=1)\n",
    "        \n",
    "        # end of for loop for one column\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotSC():\n",
    "    df = pd.read_csv('../data/results/total_scores.csv', index_col=0)\n",
    "    \n",
    "    datasets = ['BDG', 'DGS', 'BDG-DGS']\n",
    "    algorithms = ['kshape', 'kmeans', 'hierarchical']\n",
    "    contexts = ['weekday', 'weekend', 'fullweek']\n",
    "    function = ['average', 'median']\n",
    "    \n",
    "    # Four axes, returned as a 2-d array\n",
    "    f, axarr = plt.subplots(3, 3, figsize=(20,18))\n",
    "    f.subplots_adjust(hspace=0.05, wspace=0.05)\n",
    "    \n",
    "    for i in range(3): # rows\n",
    "        df_row = df[df.iloc[:, 0] == datasets[i]]\n",
    "#         print(i)\n",
    "#         print(df_row.iloc[0, 0])\n",
    "        for j in range(3): # columns, different algorithm\n",
    "            \n",
    "            df_experiment = df_row.copy()\n",
    "            df_experiment = df_experiment[df_experiment.iloc[:, 3] == algorithms[j]]\n",
    "            df_experiment_context = df_experiment.iloc[:, 0:4]\n",
    "            df_experiment_context.drop_duplicates(inplace = True) \n",
    "#             print(j)\n",
    "#             print(df_experiment.iloc[0, 3])\n",
    "            # one plot for each context\n",
    "            for _, context in df_experiment_context.iterrows():\n",
    "                current_plot = df_experiment[(df_experiment.iloc[:, 1] == context[1]) &\n",
    "                                            (df_experiment.iloc[:, 2] == context[2])]\n",
    "#                 print(current_plot)\n",
    "                axarr[i, j].set_ylim([-0.05, 0.6])\n",
    "                x_values = current_plot.iloc[:, 4]\n",
    "                y_values = current_plot['silhouette_score']\n",
    "                context_name = str(context[1]) + \" \" +  str(context[2])\n",
    "                axarr[i, j].plot(x_values, y_values, label=context_name)\n",
    "                axarr[i, j].xaxis.grid(True)\n",
    "                axarr[i, j].yaxis.grid(True)\n",
    "\n",
    "    # Fine-tune figure; hide x ticks for top plots and y ticks for right plots\n",
    "    cols = ['K-Shape', 'K-Means', 'Hierarchical']\n",
    "    for ax, col in zip(axarr[0], cols):\n",
    "        ax.set_title(col, fontsize = 30)\n",
    "\n",
    "    for ax, row in zip(axarr[:,0], datasets):\n",
    "        ax.set_ylabel(row, rotation=90, fontsize=30)\n",
    "    \n",
    "    f.text(0.5, 0.08, 'K', ha='center', fontsize=30)\n",
    "    \n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 1.55), prop={'size': 20})\n",
    "    plt.rc('xtick',labelsize=20)\n",
    "    plt.rc('ytick',labelsize=20)\n",
    "\n",
    "    plt.setp([a.get_xticklabels() for a in axarr[0, :]], visible=False)\n",
    "    plt.setp([a.get_xticklabels() for a in axarr[1, :]], visible=False)\n",
    "    plt.setp([a.get_yticklabels() for a in axarr[:, 1]], visible=False)\n",
    "    plt.setp([a.get_yticklabels() for a in axarr[:, 2]], visible=False)\n",
    "    plt.show()\n",
    "    f.savefig(\"../data/plots/silhouette_all_plots.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T15:14:17.544292Z",
     "start_time": "2018-12-30T15:14:17.533171Z"
    }
   },
   "outputs": [],
   "source": [
    "def plotGroundTruthDist(datasetName, context, function, algorithm, k):\n",
    "    # load metadata\n",
    "    if datasetName == 'BDG':\n",
    "        df_meta = pd.read_csv('../data/raw/meta_open.csv')\n",
    "    else:\n",
    "        return\n",
    "    # load data\n",
    "    df_data = pd.read_csv('../data/processed/{}_{}_{}_dataset.csv'.format(datasetName, context, function), index_col=0)\n",
    "\n",
    "    # Z-normalize the data\n",
    "    df_scaled = TimeSeriesScalerMeanVariance(mu=0.0, std=1.0).fit_transform(df_data.values)\n",
    "    # df.values will generate a 3d array (the third dimension being 1) so we convert it to a 2d array\n",
    "    df_scaled = np.squeeze(df_scaled) # now is a 2d array\n",
    "    \n",
    "    # run algorithm\n",
    "    model, labels = doClustering(df_scaled, datasetName, algorithm, k, seed=3, max_iter=300)\n",
    "    # get number of elements in each cluster\n",
    "    data_dict = {i: np.where(labels == i)[0] for i in range(k)}\n",
    "\n",
    "    df_dist = pd.DataFrame()\n",
    "    \n",
    "    ground_truth_list = ['Office', 'Dormitory', 'College Classroom', 'Primary/Secondary Classroom', \n",
    "                        'College Laboratory']\n",
    "\n",
    "    for key, value in data_dict.items():\n",
    "        \n",
    "        # BDG\n",
    "        bdg_ids = df_data.iloc[value.tolist()] # retrieve original building name based on index\n",
    "        # resample based on building id\n",
    "        ground_truth_labels = df_meta[df_meta.iloc[:, 0].isin(bdg_ids.index.values)]\n",
    "\n",
    "        # count the times the different ground truth labels exist in each cluster\n",
    "        aux = ground_truth_labels['primaryspaceusage'].value_counts()\n",
    "        # if a particular PSU didnt exist in the cluster\n",
    "        for psu in ground_truth_list:\n",
    "            if psu not in aux.index.values :\n",
    "                aux = aux.set_value(psu, 0)\n",
    "        df_dist[key] = aux\n",
    "\n",
    "    df_dist.index.name = 'PSU'\n",
    "    \n",
    "    # plot stacked bar\n",
    "    ax = df_dist.T.plot.barh(stacked=True, figsize=(20, 18), mark_right = True) # original without text\n",
    "\n",
    "    plt.legend(loc='best', prop={'size': 30})\n",
    "    \n",
    "    # show membership percentages\n",
    "    df_total = df_dist.T.copy()\n",
    "    df_total['total'] = df_total.sum(axis=1)\n",
    "    df_total = df_total['total']\n",
    "    df = df_dist.T.copy()\n",
    "    df_rel = df.div(df_total, 0)*100\n",
    "    \n",
    "    for n in df_rel:\n",
    "        for i, (cs, ab, pc, tot) in enumerate(zip(df.iloc[:, :].cumsum(1)[n], df[n], df[n], df_total)):\n",
    "            plt.text(tot, i, str(int(tot)), va='center', fontsize = 30)\n",
    "            if np.isnan(pc):\n",
    "                continue\n",
    "            else:\n",
    "                plt.text(cs - ab/2, i, str(int(pc)), va='center', ha='center', fontsize = 20)\n",
    "    \n",
    "    plt.xticks(fontsize=20)\n",
    "    plt.yticks(fontsize=20)\n",
    "    plt.rc('xtick',labelsize=20)\n",
    "    plt.rc('ytick',labelsize=20)\n",
    "    ax.xaxis.grid(True)\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLabelHist(datasetName, context, function):\n",
    "    df_meta = pd.read_csv('../data/raw/dgs_metadata.csv') # DGS\n",
    "#     df_meta = pd.read_csv('../data/raw/meta_open.csv') # BDG\n",
    "\n",
    "    df = pd.read_csv('../data/processed/{}_{}_{}_dataset.csv'.format(datasetName, context, function), index_col=0)\n",
    "    \n",
    "    # DGS\n",
    "    df_aux = pd.read_csv(\"../data/raw/DGS_322_Buildings-15m-By_Building-DST-gap-filled-3-2-18-508pm.csv\")\n",
    "    df_aux = df_aux.T\n",
    "    df_hist = df_aux[df_aux.iloc[:, 0].isin(df.index.values)] # get id based on names\n",
    "    df_hist = df_meta[df_meta['id'].isin(df_hist.index.values)] # get label based on id\n",
    "    \n",
    "    # BDG\n",
    "#     df_hist = df_meta[df_meta.iloc[:, 0].isin(df.index.values)]\n",
    "\n",
    "    \n",
    "    hist = go.FigureWidget(\n",
    "        data=[\n",
    "            dict(\n",
    "                type='histogram',\n",
    "                x=df_hist['espm_type_name'], #espm_type_name - primaryspaceusage\n",
    "\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "\n",
    "    hist.layout.title = '{} PSU Histogram'.format(datasetName)\n",
    "    hist.layout.titlefont.size = 30\n",
    "    hist.layout.xaxis.tickfont.size = 20\n",
    "    hist.layout.xaxis.tickangle = 90\n",
    "    hist.layout.yaxis.tickfont.size = 20\n",
    "\n",
    "#     hist.layout.margin.b = 330 # BDG\n",
    "    hist.layout.margin.b = 550 # DGS\n",
    "    \n",
    "    hist.layout.height=1000 # DGS\n",
    "#     hist.layout.height=1000 # DGS\n",
    "    \n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-08T05:44:14.804789Z",
     "start_time": "2019-01-08T05:44:14.774070Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "As of now, only works for BDG, needs tweaking for DGS and BDG-DGS\n",
    "\"\"\"\n",
    "def getMemberships(datasetName, context, function, algorithm, algo_parameter=range(2,11), individual=False):\n",
    "    # load metadata\n",
    "    if datasetName == 'BDG':\n",
    "        df_meta = pd.read_csv('../data/raw/meta_open.csv')#, index_col=0)\n",
    "        # final df for all the information\n",
    "        df_memberships = pd.DataFrame(columns=['dataset','context','function','algorithm','parameter k', \n",
    "                                               'clusterNum', 'clusterLabel', 'Office', 'Dormitory', \n",
    "                                               'College Classroom', 'Primary/Secondary Classroom', \n",
    "                                               'College Laboratory', 'CorrectLabel', 'IncorrectLabel']) \n",
    "        ground_truth_list = ['Office', 'Dormitory', 'College Classroom', 'Primary/Secondary Classroom', \n",
    "                            'College Laboratory']\n",
    "        \n",
    "    elif datasetName == 'DGS':\n",
    "        df_meta = pd.read_csv('../data/raw/dgs_metadata.csv')\n",
    "        df_aux = pd.read_csv(\"../data/raw/DGS_322_Buildings-15m-By_Building-DST-gap-filled-3-2-18-508pm.csv\")\n",
    "        df_aux = df_aux.T\n",
    "        df_memberships = pd.DataFrame(columns=['dataset','context','function','algorithm','parameter k', \n",
    "                                               'clusterNum', 'clusterLabel', \n",
    "                                               'Other/Specialty Hospital', \n",
    "                                               'Worship Facility', \n",
    "                                               'Senior Care Community',\n",
    "                                               'Other',\n",
    "                                               'Urgent Care Center/Clinic/Other Outpatient Office', \n",
    "                                               'Other - Technology/Science', \n",
    "                                               'Food Sales',\n",
    "                                               'Other - Entertainment/Public Assembly',\n",
    "                                               'Non-Refrigerated Warehouse',\n",
    "                                               'Multifamily Housing',\n",
    "                                               'Other - Lodging/Residential',\n",
    "                                               'Swimming Pool',\n",
    "                                               'Other - Education',\n",
    "                                               'Parking',\n",
    "                                               'Fire Station',\n",
    "                                               'Library',\n",
    "                                               'K-12 School',\n",
    "                                               'Office',\n",
    "                                               'Social/Meeting Hall',\n",
    "                                               'Other - Recreation',\n",
    "                                               'Other - Public Services',\n",
    "                                               'Police Station',\n",
    "                                               'CorrectLabel',\n",
    "                                               'IncorrectLabel'])\n",
    "        \n",
    "        ground_truth_list = ['Other/Specialty Hospital', \n",
    "                             'Worship Facility',\n",
    "                             'Senior Care Community',\n",
    "                             'Other',\n",
    "                             'Urgent Care Center/Clinic/Other Outpatient Office', \n",
    "                             'Other - Technology/Science', \n",
    "                             'Food Sales',\n",
    "                             'Other - Entertainment/Public Assembly',\n",
    "                             'Non-Refrigerated Warehouse',\n",
    "                             'Multifamily Housing',\n",
    "                             'Other - Lodging/Residential',\n",
    "                             'Swimming Pool',\n",
    "                             'Other - Education',\n",
    "                             'Parking',\n",
    "                             'Fire Station',\n",
    "                             'Library',\n",
    "                             'K-12 School',\n",
    "                             'Office',\n",
    "                             'Social/Meeting Hall',\n",
    "                             'Other - Recreation',\n",
    "                             'Other - Public Services',\n",
    "                             'Police Station']\n",
    "    \n",
    "    elif datasetName == 'BDG-DGS':\n",
    "        df_meta_bdg = pd.read_csv('../data/raw/meta_open.csv')        \n",
    "        df_meta_dgs = pd.read_csv('../data/raw/dgs_metadata.csv')\n",
    "        df_aux = pd.read_csv(\"../data/raw/DGS_322_Buildings-15m-By_Building-DST-gap-filled-3-2-18-508pm.csv\")\n",
    "        df_aux = df_aux.T\n",
    "        # final df for all information\n",
    "        df_memberships = pd.DataFrame(columns=['dataset','context','function','algorithm','parameter k', \n",
    "                                               'clusterNum', 'clusterLabel',\n",
    "                                               'Office', 'Dormitory', 'College Classroom', \n",
    "                                               'Primary/Secondary Classroom', 'College Laboratory', # end of BDG\n",
    "                                               'Other/Specialty Hospital', \n",
    "                                               'Worship Facility', \n",
    "                                               'Senior Care Community',\n",
    "                                               'Other',\n",
    "                                               'Urgent Care Center/Clinic/Other Outpatient Office', \n",
    "                                               'Other - Technology/Science', \n",
    "                                               'Food Sales',\n",
    "                                               'Other - Entertainment/Public Assembly',\n",
    "                                               'Non-Refrigerated Warehouse',\n",
    "                                               'Multifamily Housing',\n",
    "                                               'Other - Lodging/Residential',\n",
    "                                               'Swimming Pool',\n",
    "                                               'Other - Education',\n",
    "                                               'Parking',\n",
    "                                               'Fire Station',\n",
    "                                               'Library',\n",
    "#                                                'K-12 School', # this was replace by 'Primary/Secondary Classroom'\n",
    "#                                                'Office', # repeated above\n",
    "                                               'Social/Meeting Hall',\n",
    "                                               'Other - Recreation',\n",
    "                                               'Other - Public Services',\n",
    "                                               'Police Station',\n",
    "                                               'CorrectLabel',\n",
    "                                               'IncorrectLabel'])\n",
    "\n",
    "        ground_truth_list = ['Office', 'Dormitory', 'College Classroom', 'Primary/Secondary Classroom', \n",
    "                             'College Laboratory', # end of BDG\n",
    "                             'Other/Specialty Hospital', \n",
    "                             'Worship Facility',\n",
    "                             'Senior Care Community',\n",
    "                             'Other',\n",
    "                             'Urgent Care Center/Clinic/Other Outpatient Office', \n",
    "                             'Other - Technology/Science', \n",
    "                             'Food Sales',\n",
    "                             'Other - Entertainment/Public Assembly',\n",
    "                             'Non-Refrigerated Warehouse',\n",
    "                             'Multifamily Housing',\n",
    "                             'Other - Lodging/Residential',\n",
    "                             'Swimming Pool',\n",
    "                             'Other - Education',\n",
    "                             'Parking',\n",
    "                             'Fire Station',\n",
    "                             'Library',\n",
    "#                              'K-12 School', # this was replace by 'Primary/Secondary Classroom'\n",
    "#                              'Office', # repeated above\n",
    "                             'Social/Meeting Hall',\n",
    "                             'Other - Recreation',\n",
    "                             'Other - Public Services',\n",
    "                             'Police Station']\n",
    "\n",
    "    # load data\n",
    "    df_data = pd.read_csv('../data/processed/{}_{}_{}_dataset.csv'.format(datasetName, context, function), \n",
    "                          index_col=0)\n",
    "\n",
    "    # Z-normalize the data\n",
    "    df_scaled = TimeSeriesScalerMeanVariance(mu=0.0, std=1.0).fit_transform(df_data.values)\n",
    "    # df.values will generate a 3d array (the third dimension being 1) so we convert it to a 2d array\n",
    "    df_scaled = np.squeeze(df_scaled) # now is a 2d array\n",
    "    \n",
    "    # for all k's\n",
    "    for k in algo_parameter:\n",
    "        # run algorithm\n",
    "        model, labels = doClustering(df_scaled, datasetName, algorithm, k, seed=3, max_iter=300)\n",
    "        \n",
    "        # get number of elements in each cluster\n",
    "        data_dict = {i: np.where(labels == i)[0] for i in range(k)} # will have the indeces of df_data\n",
    "        df_dist = pd.DataFrame()\n",
    "\n",
    "        # get the ground truth label for buildings in each cluster\n",
    "        for key, value in data_dict.items():\n",
    "            # get correct names of bdgs\n",
    "            bdg_ids = df_data.iloc[value.tolist()] # retrieve original building name based on index\n",
    "\n",
    "            if datasetName == 'BDG':\n",
    "                # resample based on building id for the psu labels\n",
    "                ground_truth_labels = df_meta[df_meta.iloc[:, 0].isin(bdg_ids.index.values)]\n",
    "                # count the times the different ground truth labels exist in each cluster\n",
    "                aux = ground_truth_labels['primaryspaceusage'].value_counts()\n",
    "                \n",
    "            elif datasetName == 'DGS':\n",
    "                # resample based on building id for the psu labels\n",
    "                df_hist = df_aux[df_aux.iloc[:, 0].isin(bdg_ids.index.values)] # get id based on names\n",
    "                df_hist = df_meta[df_meta['id'].isin(df_hist.index.values)] # get label based on id\n",
    "                aux = df_hist['espm_type_name'].value_counts()             \n",
    "                \n",
    "            elif datasetName == 'BDG-DGS':\n",
    "                # resample based on building id for the psu labels (BDG)\n",
    "                ground_truth_labels = df_meta_bdg[df_meta_bdg.iloc[:, 0].isin(bdg_ids.index.values)]\n",
    "                # count the times the different ground truth labels exist in each cluster\n",
    "                aux = ground_truth_labels['primaryspaceusage']#.value_counts()\n",
    "                \n",
    "                # resample based on building id for the psu labels (DGS)\n",
    "                df_hist = df_aux[df_aux.iloc[:, 0].isin(bdg_ids.index.values)] # get id based on names\n",
    "                df_hist = df_meta_dgs[df_meta_dgs['id'].isin(df_hist.index.values)] # get label based on id\n",
    "                \n",
    "                # merge both set of labels\n",
    "                aux = aux.append(df_hist['espm_type_name'], ignore_index=True)#.value_counts()\n",
    "                # replace K-12 to Primary/Secondary Classroom\n",
    "                aux = aux.replace(to_replace='K-12 School', value='Primary/Secondary Classroom')\n",
    "                # as a final step, produce the counts\n",
    "                aux = aux.value_counts()\n",
    "        \n",
    "            # if a particular PSU didnt exist in the cluster\n",
    "            for psu in ground_truth_list:\n",
    "                if psu not in aux.index.values :\n",
    "                    aux = aux.set_value(psu, 0)\n",
    "            df_dist[key] = aux\n",
    "            \n",
    "            currentCluster = df_dist.T.tail(1) # so it's easier to treat as columns and only the current cluster\n",
    "            clusterNum = currentCluster.index.values[0] # label assigned by cluster algorithm,\n",
    "\n",
    "            # based on our assumption, label matches the PSU with highest count\n",
    "            clusterLabel = currentCluster.idxmax(axis=1).iloc[0]\n",
    "\n",
    "            # the number of correct labels is the number of buildings whose PSU is the cluster label\n",
    "            correctLabel = currentCluster[clusterLabel]\n",
    "            # the incorrectly clusters are the remaining buildings\n",
    "            incorrectLabel = currentCluster.copy().drop(clusterLabel, axis=1).sum(axis=1)\n",
    "            \n",
    "            # append everything to total dataframe\n",
    "            if datasetName == 'BDG':\n",
    "                df_memberships = df_memberships.append({'dataset': datasetName,\n",
    "                                    'context': context,\n",
    "                                    'function': function,\n",
    "                                    'algorithm': algorithm,\n",
    "                                    'parameter k': k,\n",
    "                                    'clusterNum': clusterNum, \n",
    "                                    'clusterLabel': clusterLabel, \n",
    "                                    'Office': currentCluster['Office'].iloc[0], \n",
    "                                    'Dormitory': currentCluster['Dormitory'].iloc[0],\n",
    "                                    'College Classroom': currentCluster['College Classroom'].iloc[0], \n",
    "                                    'Primary/Secondary Classroom': currentCluster['Primary/Secondary Classroom'].iloc[0], \n",
    "                                    'College Laboratory': currentCluster['College Laboratory'].iloc[0],\n",
    "                                    'CorrectLabel': correctLabel.iloc[0],\n",
    "                                    'IncorrectLabel': incorrectLabel.iloc[0]\n",
    "                                    }, ignore_index=True)\n",
    "            elif datasetName == 'DGS':\n",
    "                 df_memberships = df_memberships.append({'dataset': datasetName,\n",
    "                                    'context': context,\n",
    "                                    'function': function,\n",
    "                                    'algorithm': algorithm,\n",
    "                                    'parameter k': k,\n",
    "                                    'clusterNum': clusterNum, \n",
    "                                    'clusterLabel': clusterLabel, \n",
    "                                    'Other/Specialty Hospital': currentCluster['Other/Specialty Hospital'].iloc[0], \n",
    "                                    'Worship Facility': currentCluster['Worship Facility'].iloc[0], \n",
    "                                    'Senior Care Community': currentCluster['Senior Care Community'].iloc[0],\n",
    "                                    'Other': currentCluster['Other'].iloc[0],\n",
    "                                    'Urgent Care Center/Clinic/Other Outpatient Office': currentCluster['Urgent Care Center/Clinic/Other Outpatient Office'].iloc[0], \n",
    "                                    'Other - Technology/Science': currentCluster['Other - Technology/Science'].iloc[0], \n",
    "                                    'Food Sales': currentCluster['Food Sales'].iloc[0],\n",
    "                                    'Other - Entertainment/Public Assembly': currentCluster['Other - Entertainment/Public Assembly'].iloc[0],\n",
    "                                    'Non-Refrigerated Warehouse': currentCluster['Non-Refrigerated Warehouse'].iloc[0],\n",
    "                                    'Multifamily Housing': currentCluster['Multifamily Housing'].iloc[0],\n",
    "                                    'Other - Lodging/Residential': currentCluster['Other - Lodging/Residential'].iloc[0],\n",
    "                                    'Swimming Pool': currentCluster['Swimming Pool'].iloc[0],\n",
    "                                    'Other - Education': currentCluster['Other - Education'].iloc[0],\n",
    "                                    'Parking': currentCluster['Parking'].iloc[0],\n",
    "                                    'Fire Station': currentCluster['Fire Station'].iloc[0],\n",
    "                                    'Library': currentCluster['Library'].iloc[0],\n",
    "                                    'K-12 School': currentCluster['K-12 School'].iloc[0],\n",
    "                                    'Office': currentCluster['Office'].iloc[0],\n",
    "                                    'Social/Meeting Hall': currentCluster['Social/Meeting Hall'].iloc[0],\n",
    "                                    'Other - Recreation': currentCluster['Other - Recreation'].iloc[0],\n",
    "                                    'Other - Public Services': currentCluster['Other - Public Services'].iloc[0],\n",
    "                                    'Police Station': currentCluster['Police Station'].iloc[0],\n",
    "                                    'CorrectLabel': correctLabel.iloc[0],\n",
    "                                    'IncorrectLabel': incorrectLabel.iloc[0]\n",
    "                                    }, ignore_index=True)\n",
    "                    \n",
    "            elif datasetName == 'BDG-DGS':\n",
    "                df_memberships = df_memberships.append({'dataset': datasetName,\n",
    "                                    'context': context,\n",
    "                                    'function': function,\n",
    "                                    'algorithm': algorithm,\n",
    "                                    'parameter k': k,\n",
    "                                    'clusterNum': clusterNum, \n",
    "                                    'clusterLabel': clusterLabel, \n",
    "                                    'Office': currentCluster['Office'].iloc[0], \n",
    "                                    'Dormitory': currentCluster['Dormitory'].iloc[0],\n",
    "                                    'College Classroom': currentCluster['College Classroom'].iloc[0], \n",
    "                                    'Primary/Secondary Classroom': currentCluster['Primary/Secondary Classroom'].iloc[0], \n",
    "                                    'College Laboratory': currentCluster['College Laboratory'].iloc[0],\n",
    "                                    'Other/Specialty Hospital': currentCluster['Other/Specialty Hospital'].iloc[0], \n",
    "                                    'Worship Facility': currentCluster['Worship Facility'].iloc[0], \n",
    "                                    'Senior Care Community': currentCluster['Senior Care Community'].iloc[0],\n",
    "                                    'Other': currentCluster['Other'].iloc[0],\n",
    "                                    'Urgent Care Center/Clinic/Other Outpatient Office': currentCluster['Urgent Care Center/Clinic/Other Outpatient Office'].iloc[0], \n",
    "                                    'Other - Technology/Science': currentCluster['Other - Technology/Science'].iloc[0], \n",
    "                                    'Food Sales': currentCluster['Food Sales'].iloc[0],\n",
    "                                    'Other - Entertainment/Public Assembly': currentCluster['Other - Entertainment/Public Assembly'].iloc[0],\n",
    "                                    'Non-Refrigerated Warehouse': currentCluster['Non-Refrigerated Warehouse'].iloc[0],\n",
    "                                    'Multifamily Housing': currentCluster['Multifamily Housing'].iloc[0],\n",
    "                                    'Other - Lodging/Residential': currentCluster['Other - Lodging/Residential'].iloc[0],\n",
    "                                    'Swimming Pool': currentCluster['Swimming Pool'].iloc[0],\n",
    "                                    'Other - Education': currentCluster['Other - Education'].iloc[0],\n",
    "                                    'Parking': currentCluster['Parking'].iloc[0],\n",
    "                                    'Fire Station': currentCluster['Fire Station'].iloc[0],\n",
    "                                    'Library': currentCluster['Library'].iloc[0],\n",
    "#                                     'K-12 School': currentCluster['K-12 School'].iloc[0],\n",
    "#                                     'Office': currentCluster['Office'].iloc[0],\n",
    "                                    'Social/Meeting Hall': currentCluster['Social/Meeting Hall'].iloc[0],\n",
    "                                    'Other - Recreation': currentCluster['Other - Recreation'].iloc[0],\n",
    "                                    'Other - Public Services': currentCluster['Other - Public Services'].iloc[0],\n",
    "                                    'Police Station': currentCluster['Police Station'].iloc[0],\n",
    "                                    'CorrectLabel': correctLabel.iloc[0],\n",
    "                                    'IncorrectLabel': incorrectLabel.iloc[0]\n",
    "                                    }, ignore_index=True)\n",
    "              \n",
    "    # save as csv\n",
    "    if individual:\n",
    "        df_memberships.to_csv('../data/results/total_memerbships_{}_{}_{}_{}.csv'.format(datasetName,\n",
    "                                                                                        context,\n",
    "                                                                                        function,\n",
    "                                                                                        algorithm))\n",
    "    else:\n",
    "        # append to general file\n",
    "        with open('../data/results/total_memberships_{}.csv'.format(datasetName), 'a') as f:\n",
    "            df_memberships.to_csv(f, header=False)\n",
    "            \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T08:27:44.366709Z",
     "start_time": "2019-01-05T08:27:44.360956Z"
    }
   },
   "outputs": [],
   "source": [
    "def getBatchMemberships(datasetName_list, context_list, function_list, algorithm_list, algo_parameter=range(2,11)):\n",
    "     # nested loops for each possible combination of the lists\n",
    "    for datasetName in datasetName_list: # every dataset\n",
    "        for context in context_list: # every context\n",
    "            for function in function_list: # every load aggregation function\n",
    "                for algorithm in algorithm_list: # every algorithm\n",
    "                    getMemberships(datasetName, context, function, algorithm, algo_parameter=range(2,11))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateFinalTable(dataset, k):\n",
    "    # list values for context\n",
    "    contexts = ['weekday', 'weekend', 'fullweek']\n",
    "    functions = ['average', 'median']\n",
    "    algorithms = ['kshape', 'kmeans', 'hierarchical']\n",
    "    \n",
    "    # load results\n",
    "    df = pd.read_csv(\"../data/results/total_memberships_{}.csv\".format(dataset), index_col=0)\n",
    "\n",
    "    # since each processed dataset has the same buildings but with different hourly readings, we can load any of them\n",
    "    df_pro = pd.read_csv('../data/processed/{}_{}_{}_dataset.csv'.format(dataset, contexts[0], functions[0]), index_col=0)\n",
    "    \n",
    "    # load total counts of buildings for each psu\n",
    "    if dataset == 'BDG':\n",
    "        df_meta = pd.read_csv('../data/raw/meta_open.csv')\n",
    "        df_psu_labels = df_meta[df_meta.iloc[:, 0].isin(df_pro.index.values)]\n",
    "        label_dist = df_psu_labels['primaryspaceusage'].value_counts()\n",
    "        df_final = pd.DataFrame(index = ['Office', 'Dormitory', 'College Classroom', \n",
    "                                        'Primary/Secondary Classroom', 'College Laboratory', 'Total'], \n",
    "                                columns=['Correctly Labeled', 'Incorrectly Labeled'])\n",
    "    elif dataset =='DGS':\n",
    "        df_meta = pd.read_csv('../data/raw/dgs_metadata.csv')\n",
    "        df_aux = pd.read_csv(\"../data/raw/DGS_322_Buildings-15m-By_Building-DST-gap-filled-3-2-18-508pm.csv\")\n",
    "        df_aux = df_aux.T\n",
    "        df_psu_labels = df_aux[df_aux.iloc[:, 0].isin(df_pro.index.values)] # get id based on names\n",
    "        df_psu_labels = df_meta[df_meta['id'].isin(df_psu_labels.index.values)] # get label based on id\n",
    "        label_dist = df_psu_labels['espm_type_name'].value_counts()\n",
    "        df_final = pd.DataFrame(index = ['Other/Specialty Hospital', \n",
    "                                         'Worship Facility',\n",
    "                                         'Senior Care Community',\n",
    "                                         'Other',\n",
    "                                         'Urgent Care Center/Clinic/Other Outpatient Office', \n",
    "                                         'Other - Technology/Science', \n",
    "                                         'Food Sales',\n",
    "                                         'Other - Entertainment/Public Assembly',\n",
    "                                         'Non-Refrigerated Warehouse',\n",
    "                                         'Multifamily Housing',\n",
    "                                         'Other - Lodging/Residential',\n",
    "                                         'Swimming Pool',\n",
    "                                         'Other - Education',\n",
    "                                         'Parking',\n",
    "                                         'Fire Station',\n",
    "                                         'Library',\n",
    "                                         'K-12 School',\n",
    "                                         'Office',\n",
    "                                         'Social/Meeting Hall',\n",
    "                                         'Other - Recreation',\n",
    "                                         'Other - Public Services',\n",
    "                                         'Police Station',\n",
    "                                         'Total'],\n",
    "                                columns=['Correctly Labeled', 'Incorrectly Labeled'])\n",
    "    elif dataset == 'BDG-DGS':\n",
    "        # BDG\n",
    "        df_meta_bdg = pd.read_csv('../data/raw/meta_open.csv')\n",
    "        df_psu_labels = df_meta_bdg[df_meta_bdg.iloc[:, 0].isin(df_pro.index.values)]\n",
    "        label_dist = df_psu_labels['primaryspaceusage']#.value_counts()\n",
    "        # DGS\n",
    "        df_meta_dgs = pd.read_csv('../data/raw/dgs_metadata.csv')\n",
    "        df_aux = pd.read_csv(\"../data/raw/DGS_322_Buildings-15m-By_Building-DST-gap-filled-3-2-18-508pm.csv\")\n",
    "        df_aux = df_aux.T\n",
    "        df_psu_labels = df_aux[df_aux.iloc[:, 0].isin(df_pro.index.values)] # get id based on names\n",
    "        df_psu_labels = df_meta_dgs[df_meta_dgs['id'].isin(df_psu_labels.index.values)] # get label based on id\n",
    "        # merge both set of labels\n",
    "        label_dist = label_dist.append(df_psu_labels['espm_type_name'], ignore_index=True)#.value_counts()\n",
    "        # replace K-12 to Primary/Secondary Classroom\n",
    "        label_dist = label_dist.replace(to_replace='K-12 School', value='Primary/Secondary Classroom')\n",
    "        # as a final step, produce the counts\n",
    "        label_dist = label_dist.value_counts()\n",
    "        \n",
    "        df_final = pd.DataFrame(index = ['Office', 'Dormitory', 'College Classroom', \n",
    "                                               'Primary/Secondary Classroom', 'College Laboratory', # end of BDG\n",
    "                                               'Other/Specialty Hospital', \n",
    "                                               'Worship Facility', \n",
    "                                               'Senior Care Community',\n",
    "                                               'Other',\n",
    "                                               'Urgent Care Center/Clinic/Other Outpatient Office', \n",
    "                                               'Other - Technology/Science', \n",
    "                                               'Food Sales',\n",
    "                                               'Other - Entertainment/Public Assembly',\n",
    "                                               'Non-Refrigerated Warehouse',\n",
    "                                               'Multifamily Housing',\n",
    "                                               'Other - Lodging/Residential',\n",
    "                                               'Swimming Pool',\n",
    "                                               'Other - Education',\n",
    "                                               'Parking',\n",
    "                                               'Fire Station',\n",
    "                                               'Library',\n",
    "#                                                'K-12 School', # this was replace by 'Primary/Secondary Classroom'\n",
    "#                                                'Office', # repeated above\n",
    "                                               'Social/Meeting Hall',\n",
    "                                               'Other - Recreation',\n",
    "                                               'Other - Public Services',\n",
    "                                               'Police Station',\n",
    "                                               'Total'],\n",
    "                                columns=['Correctly Labeled', 'Incorrectly Labeled'])\n",
    "    \n",
    "    # iterate through all elements of the lists\n",
    "    for context in contexts:\n",
    "        for function in functions:\n",
    "            for algorithm in algorithms:\n",
    "                contextName = \"{}-{}-{}\".format(context, function, algorithm)\n",
    "                print(contextName)\n",
    "                \n",
    "                if dataset == 'BDG':\n",
    "                    current_context = pd.DataFrame(index = ['Office', 'Dormitory', 'College Classroom', \n",
    "                                                            'Primary/Secondary Classroom',\n",
    "                                                            'College Laboratory', 'Total'], \n",
    "                                                   columns=['Correctly Labeled', 'Incorrectly Labeled'])\n",
    "                elif dataset == 'DGS':\n",
    "                    current_context = pd.DataFrame(index = ['Other/Specialty Hospital', \n",
    "                                                            'Worship Facility',\n",
    "                                                            'Senior Care Community',\n",
    "                                                            'Other',\n",
    "                                                            'Urgent Care Center/Clinic/Other Outpatient Office', \n",
    "                                                            'Other - Technology/Science', \n",
    "                                                            'Food Sales',\n",
    "                                                            'Other - Entertainment/Public Assembly',\n",
    "                                                            'Non-Refrigerated Warehouse',\n",
    "                                                            'Multifamily Housing',\n",
    "                                                            'Other - Lodging/Residential',\n",
    "                                                            'Swimming Pool',\n",
    "                                                            'Other - Education',\n",
    "                                                            'Parking',\n",
    "                                                            'Fire Station',\n",
    "                                                            'Library',\n",
    "                                                            'K-12 School',\n",
    "                                                            'Office',\n",
    "                                                            'Social/Meeting Hall',\n",
    "                                                            'Other - Recreation',\n",
    "                                                            'Other - Public Services',\n",
    "                                                            'Police Station',\n",
    "                                                            'Total'],\n",
    "                                                   columns=['Correctly Labeled', 'Incorrectly Labeled'])\n",
    "                elif dataset == 'BDG-DGS':\n",
    "                    current_context = pd.DataFrame(index = ['Office', 'Dormitory', 'College Classroom', \n",
    "                                                            'Primary/Secondary Classroom', \n",
    "                                                            'College Laboratory', # end of BDG\n",
    "                                                            'Other/Specialty Hospital', \n",
    "                                                            'Worship Facility', \n",
    "                                                            'Senior Care Community',\n",
    "                                                            'Other',\n",
    "                                                            'Urgent Care Center/Clinic/Other Outpatient Office', \n",
    "                                                            'Other - Technology/Science', \n",
    "                                                            'Food Sales',\n",
    "                                                            'Other - Entertainment/Public Assembly',\n",
    "                                                            'Non-Refrigerated Warehouse',\n",
    "                                                            'Multifamily Housing',\n",
    "                                                            'Other - Lodging/Residential',\n",
    "                                                            'Swimming Pool',\n",
    "                                                            'Other - Education',\n",
    "                                                            'Parking',\n",
    "                                                            'Fire Station',\n",
    "                                                            'Library',\n",
    "#                                                             'K-12 School', # this was replace by 'Primary/Secondary Classroom'\n",
    "#                                                             'Office', # repeated above\n",
    "                                                            'Social/Meeting Hall',\n",
    "                                                            'Other - Recreation',\n",
    "                                                            'Other - Public Services',\n",
    "                                                            'Police Station',\n",
    "                                                            'Total'],\n",
    "                                                   columns=['Correctly Labeled', 'Incorrectly Labeled'])\n",
    "                total_sum_correct = 0\n",
    "                total_sum_incorrect = 0 \n",
    "                # get the subset dataset which will be the first two columns\n",
    "                df_resampled = df[(df['context'] == context) & \n",
    "                                  (df['function'] == function) &\n",
    "                                  (df['algorithm'] == algorithm) &\n",
    "                                  (df['parameter k'] == k)]\n",
    "\n",
    "                # iterate through PSU labels\n",
    "                for psu in current_context.index.values:\n",
    "                    if psu == 'Total':\n",
    "                        continue\n",
    "                    # correct labels counts happe when the cluster label is the current psu\n",
    "                    correctLabel = df_resampled[df_resampled['clusterLabel'] == psu] # all rows where that happens\n",
    "                    if correctLabel.empty:\n",
    "                        correctLabel = 0\n",
    "                        correctLabel_count = 0\n",
    "                        total_sum_correct += correctLabel_count\n",
    "                    else:\n",
    "                        correctLabel_count = correctLabel['CorrectLabel'].sum()\n",
    "                        total_sum_correct += correctLabel_count\n",
    "                        correctLabel = (correctLabel_count / label_dist[psu].sum() * 100).round(2)\n",
    "                            \n",
    "                    # incorrect labels counts is total count of this psu bdgs in the dataset - correctLabels\n",
    "                    incorrectLabel_count = label_dist.loc[psu] - correctLabel_count\n",
    "                    total_sum_incorrect += incorrectLabel_count\n",
    "                    incorrectLabel = (incorrectLabel_count / label_dist[psu].sum() * 100).round(2)\n",
    "                        \n",
    "                    current_context.loc[psu] = [correctLabel, incorrectLabel]                        \n",
    "                        \n",
    "                aux_total_sum_correct = total_sum_correct\n",
    "                total_sum_correct = (total_sum_correct / (total_sum_correct + total_sum_incorrect) * 100).round(2)\n",
    "                total_sum_incorrect = (total_sum_incorrect / (aux_total_sum_correct + total_sum_incorrect) * 100).round(2)\n",
    "                            \n",
    "                current_context.loc[psu] = [total_sum_correct, total_sum_incorrect]\n",
    "                # append current context to final table\n",
    "                df_final = pd.concat([df_final, current_context], axis=1)\n",
    "                        \n",
    "    # save to file\n",
    "    df_final.to_csv(\"../data/results/finaltable_{}_{}.csv\".format(dataset, k))\n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-09T08:17:15.287000Z",
     "start_time": "2019-01-09T08:17:15.263221Z"
    }
   },
   "outputs": [],
   "source": [
    "def finalBarPlots(dataset, k, stacked=False, grouped='algorithm'):\n",
    "    df = pd.read_csv(\"../data/results/finaltable_{}_{}.csv\".format(dataset, k), index_col=0)\n",
    "    \n",
    "    contexts = ['weekday-average-kshape', 'weekday-average-kmeans', 'weekday-average-hierarchical',\n",
    "                'weekday-median-kshape', 'weekday-median-kmeans', 'weekday-median-hierarchical', \n",
    "                'weekend-average-kshape', 'weekend-average-kmeans', 'weekend-average-hierarchical',\n",
    "                'weekend-median-kshape', 'weekend-median-kmeans', 'weekend-median-hierarchical',\n",
    "                'fullweek-average-kshape', 'fullweek-average-kmeans', 'fullweek-average-hierarchical',\n",
    "                'fullweek-median-kshape', 'fullweek-median-kmeans', 'fullweek-median-hierarchical']\n",
    "\n",
    "    context_no_algo = ['weekday-average', 'weekday-median', \n",
    "                       'weekend-average','weekend-median',\n",
    "                       'fullweek-average','fullweek-median']\n",
    "    \n",
    "    algorithms = ['kshape', 'kmeans', 'hierarchical']\n",
    "    \n",
    "    # pre-process the dataframe\n",
    "    df_transpose = pd.DataFrame(df.T.reset_index())\n",
    "    df_transpose['index'] = df_transpose['index'].apply(lambda x: x.split('.')[0])\n",
    "    df_transpose.drop(df_transpose.index[[0, 1]], inplace=True)\n",
    "    \n",
    "    # insert context in one columm, each contex has two row values for correctly and incorrectly\n",
    "    df_aux = []\n",
    "    df_aux_single = []\n",
    "    for c in contexts:\n",
    "        df_aux.append(c)\n",
    "        df_aux.append(c)  \n",
    "        df_aux_single.append(c)\n",
    "    df_transpose.insert(0, 'contexts', '')\n",
    "    df_transpose['contexts'] = df_aux\n",
    "    \n",
    "    if stacked:\n",
    "        H = \"/\"\n",
    "        dfall = []\n",
    "    \n",
    "        # change grouping accordingly\n",
    "        if grouped == 'algorithm':\n",
    "            labels = context_no_algo\n",
    "            # generate sub dataframes based on algorithms\n",
    "            for context_algo in context_no_algo:\n",
    "                df_subset = pd.DataFrame(columns=['Correctly', 'Incorrectly'], index=algorithms)\n",
    "                df_context_sub = df_transpose[df_transpose['contexts'].str.contains(context_algo)]\n",
    "                for algo in algorithms:\n",
    "                    value = df_context_sub[df_context_sub['contexts'].str.contains(algo)]\n",
    "                    value = value['Total'].reset_index(drop=True)\n",
    "                    df_subset.loc[algo] = [value.iloc[0], value.iloc[1]]\n",
    "\n",
    "                dfall.append(df_subset)\n",
    "        elif grouped == 'context':\n",
    "            labels = algorithms\n",
    "            # generate sub dataframes based on contexts\n",
    "            for algo in algorithms:\n",
    "                df_subset = pd.DataFrame(columns=['Correctly', 'Incorrectly'], index=context_no_algo)\n",
    "                df_context_sub = df_transpose[df_transpose['contexts'].str.contains(algo)]\n",
    "                for context_algo in context_no_algo:\n",
    "                    value = df_context_sub[df_context_sub['contexts'].str.contains(context_algo)]\n",
    "                    value = value['Total'].reset_index(drop=True)\n",
    "                    df_subset.loc[context_algo] = [value.iloc[0], value.iloc[1]]\n",
    "\n",
    "                dfall.append(df_subset)\n",
    "        \n",
    "        n_df = len(dfall)\n",
    "        n_col = len(dfall[0].columns)\n",
    "        n_ind = len(dfall[0].index)\n",
    "\n",
    "        plt.figure(figsize=(16,14))\n",
    "        axe = plt.subplot(111)\n",
    "        \n",
    "        for df in dfall : # for each data frame\n",
    "            axe = df.plot(kind=\"bar\",\n",
    "                          linewidth=0,\n",
    "                          stacked=True,\n",
    "                          ax=axe,\n",
    "                          legend=False,\n",
    "                          grid=False)  # make bar plots\n",
    "\n",
    "        h,l = axe.get_legend_handles_labels() # get the handles we want to modify\n",
    "        for i in range(0, n_df * n_col, n_col): # len(h) = n_col * n_df\n",
    "            for j, pa in enumerate(h[i:i+n_col]):\n",
    "                for rect in pa.patches: # for each index\n",
    "                    rect.set_x(rect.get_x() + 0.8 / float(n_df + 1) * i / float(n_col))\n",
    "                    rect.set_hatch(H * int(i / n_col)) #edited part     \n",
    "                    rect.set_width(0.8 / float(n_df + 1))\n",
    "\n",
    "        axe.set_xticks((np.arange(0, 2 * n_ind, 2) + 1 / float(n_df + 1)) / 2.)\n",
    "        \n",
    "        if grouped == 'algorithm':\n",
    "            axe.set_xticklabels(df.index, rotation = 0, fontsize=30)\n",
    "        elif grouped == 'context':\n",
    "            axe.set_xticklabels(df.index, rotation = 45, fontsize=30)\n",
    "                \n",
    "        axe.set_title(\"Percentage of Classification using k = {} for {} dataset\".format(k, dataset), fontsize = 30)\n",
    "        axe.tick_params(axis=\"y\", labelsize=30)\n",
    "\n",
    "        axe.axhline(50, linewidth=2, color='r', linestyle=\"--\")\n",
    "\n",
    "        # Add invisible data to add another legend\n",
    "        n=[]        \n",
    "        for i in range(n_df):\n",
    "            n.append(axe.bar(0, 0, color=\"gray\", hatch=H * i))\n",
    "\n",
    "        l1 = axe.legend(h[:n_col], l[:n_col], loc=[1, 0.5], prop={'size': 20})#[1.01, 0.5])\n",
    "                \n",
    "        if labels is not None:\n",
    "            l2 = plt.legend(n, labels, loc=[1, 0.1], prop={'size': 20}) #[1.01, 0.1]\n",
    "        axe.add_artist(l1)\n",
    "        plt.savefig(\"../data/plots/finalBar_{}_{}_groupedBy_{}.png\".format(dataset, k, grouped), bbox_inches='tight')\n",
    "        return axe\n",
    "        \n",
    "    else:\n",
    "        # draw nested bar plot\n",
    "        g = sns.catplot(x=\"contexts\", y=\"Total\", hue=\"index\", data=df_transpose,\n",
    "                    height=10, kind=\"bar\", palette=\"muted\", legend=False)\n",
    "        g.set_xticklabels(rotation=90, fontsize=20)\n",
    "        g.set_yticklabels(fontsize=20)\n",
    "        g.set_ylabels(\"Classification Percentage\",fontsize=30)\n",
    "        g.set_xlabels(\"Contexts\", fontsize=30)\n",
    "        g.despine(left=True)\n",
    "        plt.legend(loc='upper left', prop={'size': 20})\n",
    "        g.savefig(\"../data/plots/finalBar_{}_{}_plots.png\".format(dataset, k), \n",
    "                                                                  bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profileDist(datasetName, context, function):\n",
    "    # load data\n",
    "    df_data = pd.read_csv('../data/processed/{}_{}_{}_dataset.csv'.format(datasetName, context, function), index_col=0)\n",
    "    \n",
    "    # load metadata\n",
    "    if datasetName == 'BDG':\n",
    "        df_meta = pd.read_csv('../data/raw/meta_open.csv')\n",
    "        # get labels for all buildings\n",
    "        ground_truth_labels = df_meta[df_meta.iloc[:, 0].isin(df_data.index.values)]\n",
    "        ground_truth_labels = ground_truth_labels['primaryspaceusage']\n",
    "        ground_truth_list = ['Office', 'Dormitory', 'College Classroom', 'Primary/Secondary Classroom', \n",
    "                        'College Laboratory']\n",
    "        colors = ['r', 'g', 'b', 'k', 'y']\n",
    "\n",
    "    else:\n",
    "        df_meta = pd.read_csv('../data/raw/dgs_metadata.csv')\n",
    "        df_aux = pd.read_csv(\"../data/raw/DGS_322_Buildings-15m-By_Building-DST-gap-filled-3-2-18-508pm.csv\")\n",
    "        \n",
    "        # get labels for all buildings\n",
    "        df_aux = df_aux.T\n",
    "        df_hist = df_aux[df_aux.iloc[:, 0].isin(df_data.index.values)] # get id based on names\n",
    "        df_hist = df_meta[df_meta['id'].isin(df_hist.index.values)] # get label based on id\n",
    "        ground_truth_labels = df_hist['espm_type_name']\n",
    "        \n",
    "    \n",
    "    ground_truth_labels = ground_truth_labels.reset_index(drop=True)\n",
    "\n",
    "    # Z-normalize the data\n",
    "    df_scaled = TimeSeriesScalerMeanVariance(mu=0.0, std=1.0).fit_transform(df_data.values)\n",
    "    # df.values will generate a 3d array (the third dimension being 1) so we convert it to a 2d array\n",
    "    df_scaled = np.squeeze(df_scaled) # now is a 2d array\n",
    "        \n",
    "    # back to pandas\n",
    "    df_scaled = pd.DataFrame(df_scaled)\n",
    "    df_scaled['psu'] = ground_truth_labels\n",
    "            \n",
    "    plt.figure(figsize=(18,10))\n",
    "    \n",
    "    for index, building in df_scaled.iterrows():       \n",
    "        if building[-1] == 'Office':\n",
    "            idx = 0\n",
    "        elif building[-1] == 'Dormitory':\n",
    "            idx = 1\n",
    "        elif building[-1] == 'College Classroom':\n",
    "            idx = 2\n",
    "        elif building[-1] == 'Primary/Secondary Classroom':\n",
    "            idx = 3\n",
    "        else:\n",
    "            idx = 4\n",
    "            \n",
    "        plt.plot(building[:-1], \"-\", alpha=0.4, label = building[-1]) # c = colors()dx\n",
    "    \n",
    "    # take care of repeating label and group them\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    by_label = OrderedDict(zip(labels, handles))\n",
    "    plt.legend(by_label.values(), by_label.keys(), loc='center left', bbox_to_anchor=(1, 0.55), prop={'size': 20})\n",
    "\n",
    "    # axis and title\n",
    "    plt.xticks(fontsize=20)\n",
    "    plt.yticks(fontsize=20)\n",
    "    plt.title(\"Load Profiles curves based on {}\".format(function), fontsize = 30)\n",
    "\n",
    "    plt.show()\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

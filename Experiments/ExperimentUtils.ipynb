{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T11:57:03.237939Z",
     "start_time": "2018-12-21T11:57:03.235541Z"
    }
   },
   "source": [
    "# This notebook will be the only interaction with all any experiment notebook. From here the function calls to all the preprocessing and clustering notebooks will be done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T10:17:45.561980Z",
     "start_time": "2018-12-23T10:17:45.559252Z"
    }
   },
   "outputs": [],
   "source": [
    "# Libraries needed\n",
    "# !pip install nbimporter # uncomment if library is not install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-29T13:46:58.380760Z",
     "start_time": "2018-12-29T13:46:55.810869Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Jupyter notebook from /Users/matias/Documents/Education/Graduate/NUS/Projects/sensor-cluster-er/Preprocessing/preprocessing.ipynb\n",
      "Importing Jupyter notebook from /Users/matias/Documents/Education/Graduate/NUS/Projects/sensor-cluster-er/Preprocessing/context_extraction.ipynb\n",
      "Importing Jupyter notebook from /Users/matias/Documents/Education/Graduate/NUS/Projects/sensor-cluster-er/Preprocessing/load_cuve_generation.ipynb\n",
      "Importing Jupyter notebook from /Users/matias/Documents/Education/Graduate/NUS/Projects/sensor-cluster-er/ClusteringAnalysis/ClusteringValidationMetrics.ipynb\n",
      "Importing Jupyter notebook from /Users/matias/Documents/Education/Graduate/NUS/Projects/sensor-cluster-er/ClusteringAnalysis/ClusteringAlgorithms.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Existing Notebooks\n",
    "import nbimporter\n",
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from Preprocessing.preprocessing import hourly_dataset\n",
    "from Preprocessing.context_extraction import getContext\n",
    "from Preprocessing.load_cuve_generation import doAggregation\n",
    "\n",
    "from ClusteringAnalysis.ClusteringValidationMetrics import get_validation_scores\n",
    "from ClusteringAnalysis.ClusteringAlgorithms import doClustering\n",
    "\n",
    "# Built-in libraries\n",
    "import time\n",
    "from itertools import product\n",
    "from math import log\n",
    "import pickle\n",
    "from datetime import datetime, timedelta # testing\n",
    "\n",
    "# NumPy, SciPy and Pandas\n",
    "from scipy.spatial.distance import cdist\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Scikit-Learn\n",
    "from sklearn.metrics import silhouette_samples # , silhouette_score\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from tslearn.clustering import silhouette_score\n",
    "# Tslearn\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "from tslearn.utils import to_time_series_dataset\n",
    "\n",
    "# Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.gridspec as gridspec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T11:01:20.315851Z",
     "start_time": "2018-12-23T11:01:20.307773Z"
    }
   },
   "outputs": [],
   "source": [
    "# check files\n",
    "def checkFiles(datasetName, context, function):\n",
    "    # if the dataset is the combination, directly load the aggregated dataset for the datasets\n",
    "    if datasetName == 'BDG-DGS':\n",
    "        df1_name = 'BDG'\n",
    "        df2_name = 'DGS'\n",
    "\n",
    "        df1 = pd.read_csv('../data/processed/{}_{}_{}_dataset.csv'.format(df1_name, context, function), index_col=0)\n",
    "        df2 = pd.read_csv('../data/processed/{}_{}_{}_dataset.csv'.format(df2_name, context, function), index_col=0)\n",
    "    \n",
    "        df = df1.append(df2)\n",
    "        df.to_csv('../data/processed/{}_{}_{}_dataset.csv'.format(datasetName, context, function))\n",
    "    else:\n",
    "        # check if dataset has already being processed before\n",
    "        exists_df = os.path.isfile('../data/processed/{}_dataset.csv'.format(datasetName))\n",
    "        if exists_df: # if file exists, read it\n",
    "            df = pd.read_csv('../data/processed/{}_dataset.csv'.format(datasetName), index_col=0)\n",
    "            print(\"Preprocessed dataset already exists, loading it ...\")\n",
    "        else: # if file is missing, produce it\n",
    "            df = hourly_dataset(datasetName)\n",
    "            print(\"Preprocessing dataset ...\")\n",
    "\n",
    "        # check if dataset with context has already being processed before\n",
    "        exists_context = os.path.isfile('../data/processed/{}_{}_dataset.csv'.format(datasetName, context))\n",
    "        if exists_context: # if file exists, read it\n",
    "            df = pd.read_csv('../data/processed/{}_{}_dataset.csv'.format(datasetName, context), index_col=0)\n",
    "            print(\"Dataset with {} context already exists, loading it ...\".format(context))\n",
    "        else: # if file is missing, produce it\n",
    "            df = getContext(datasetName, context)\n",
    "            print(\"Generating context dataset ...\")\n",
    "\n",
    "        # check if dataset with function has already being processed before\n",
    "        exists_function = os.path.isfile('../data/processed/{}_{}_{}_dataset.csv'.format(datasetName, context, function))\n",
    "        if exists_function: # if file exists, read it\n",
    "            df = pd.read_csv('../data/processed/{}_{}_{}_dataset.csv'.format(datasetName, context, function), index_col=0)\n",
    "            print(\"Dataset with {} context and {} load curve aggregation function already exists, loading it ...\".format(context, function))\n",
    "\n",
    "        else: # if file is missing, produce it\n",
    "            df = doAggregation(datasetName, context, function)\n",
    "            print(\"Generating load curves based on {} ...\".format(function))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T04:07:48.500160Z",
     "start_time": "2018-12-27T04:07:48.471459Z"
    }
   },
   "outputs": [],
   "source": [
    "# Running Experiment \n",
    "def runExperiment(datasetName, context, function, algorithm='kshape', \n",
    "                  algo_parameter=range(2,11), validation_metrics='all', appendTotalFile=False):\n",
    "    print(\"Running Experiment with dataset: {}, context:  {}, function: {}, algorithm: {}\".format(datasetName, \n",
    "                                                                                                context,\n",
    "                                                                                                function,\n",
    "                                                                                                algorithm))\n",
    "    # check if file exists and load it\n",
    "    df = checkFiles(datasetName, context, function)\n",
    "    \n",
    "    # Z-normalize the data\n",
    "    df_scaled = TimeSeriesScalerMeanVariance(mu=0.0, std=1.0).fit_transform(df.values)\n",
    "    # df.values will generate a 3d array (the third dimension being 1) so we convert it to a 2d array\n",
    "    df_scaled = np.squeeze(df_scaled) # now is a 2d array\n",
    "    \n",
    "    scores = [] # list of the scores for each parameter for the selected algorithm\n",
    "    \n",
    "    # run selected algorithm with the appropiate parameter\n",
    "    for k in algo_parameter:\n",
    "        model, labels = doClustering(df_scaled, datasetName, algorithm, k, seed=3, max_iter=200)\n",
    "        scores.append(get_validation_scores(df_scaled, labels))\n",
    "        print(\"Running experiment with {} and k = {}\".format(algorithm, k))\n",
    "    \n",
    "    # name for saving the results\n",
    "    obj_name = '../data/results/{}_{}_{}_{}_scores'.format(datasetName, context, function, algorithm)\n",
    "    \n",
    "    # update the final score dataframe\n",
    "    scores = pd.DataFrame.from_dict(scores)\n",
    "    scores.insert(0, 'dataset', '')\n",
    "    scores['dataset'] = datasetName\n",
    "    scores.insert(1, 'context', '')\n",
    "    scores['context'] = context\n",
    "    scores.insert(2, 'function', '')\n",
    "    scores['function'] = function\n",
    "    scores.insert(3, 'algorithm', '')\n",
    "    scores['algorithm'] = algorithm\n",
    "    if \"k\" in algorithm or algorithm == 'hierarchical':\n",
    "        scores.insert(4, 'parameter k', '')\n",
    "        scores['parameter k'] = algo_parameter\n",
    "    \n",
    "    # approximate to two decimals\n",
    "    scores = scores.round(2)\n",
    "    \n",
    "    # save as python pickle\n",
    "    f = open(obj_name + '.pkl', 'wb')\n",
    "    pickle.dump(scores, f)\n",
    "    f.close\n",
    "    \n",
    "    # save as csv\n",
    "    scores.to_csv('{}.csv'.format(obj_name))\n",
    "    print(\"Scores saved in {}.csv\\n\".format(obj_name)) # individual file\n",
    "    \n",
    "    if appendTotalFile:\n",
    "        with open('../data/results/total_scores.csv', 'a') as f: # append to general file\n",
    "            scores.to_csv(f, header=False)\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run batch of experiments\n",
    "def runBatchExperiments(datasetName_list, context_list, function_list, algorithm_list, \n",
    "                  algo_parameter=range(2,11), validation_metrics='all'):\n",
    "    # nested loops for each possible combination of the lists\n",
    "    for datasetName in datasetName_list: # every dataset\n",
    "        for context in context_list: # every context\n",
    "            for function in function_list: # every load aggregation function\n",
    "                for algorithm in algorithm_list: # every algorithm\n",
    "                    runExperiment(datasetName, context, function, algorithm, algo_parameter, validation_metrics, True)\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T10:17:48.118758Z",
     "start_time": "2018-12-23T10:17:48.115255Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generating clusters and centroids\n",
    "def generateClusters(datasetName, context, function, algorithm='kshape', algo_parameter = 5):\n",
    "    # check if file exists and load it\n",
    "    df = checkFiles(datasetName, context, function)\n",
    "        \n",
    "    # Z-normalize the data\n",
    "    df_scaled = TimeSeriesScalerMeanVariance(mu=0.0, std=1.0).fit_transform(df.values)\n",
    "    # df.values will generate a 3d array (the third dimension being 1) so we convert it to a 2d array\n",
    "    df_scaled = np.squeeze(df_scaled) # now is a 2d array\n",
    "        \n",
    "    # run selected algorithm with the appropiate parameter\n",
    "    for k in algo_parameter:\n",
    "        model, labels = doClustering(df_scaled, datasetName, algorithm, k, seed=3, max_iter=300, plot=True)    \n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T10:17:48.127713Z",
     "start_time": "2018-12-23T10:17:48.121401Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Visualizing Scores\n",
    "def generateMetricPlots(datasetName, context, function, algorithm='kshape', showPlots=False):\n",
    "    plt.ioff() # this way only plt.show() will display figures\n",
    "    \n",
    "    pickle_in = open(\"../data/results/{}_{}_{}_{}_scores.pkl\".format(datasetName, context, function, algorithm), \n",
    "                         \"rb\")\n",
    "    df_scores = pickle.load(pickle_in)\n",
    "    pickle_in.close()\n",
    "    \n",
    "    # the x-axis is the different parameter values of the algorith,\n",
    "    x_axis = list(df_scores.iloc[:, 4]) # the column of index 4 is where the parameter is stored\n",
    "    \n",
    "    # dataframe of only the validation metrics\n",
    "    df_metrics = df_scores.iloc[:, range(5, len(df_scores.columns))]\n",
    "    # the total number of metrics available is 7\n",
    "    num_metrics = len(df_metrics.columns) # but we double check just in case\n",
    "    # extract column names for plotting\n",
    "    metric_names = df_metrics.columns.values\n",
    "    metric_index = 0\n",
    "    \n",
    "    # iterate through every metric and plot the value versus the correspondant algo parameter\n",
    "    f, axarr = plt.subplots(num_metrics, sharex=False, figsize =(10,30))\n",
    "    for metric in range(len(df_metrics.columns)):\n",
    "        axarr[metric_index].plot(x_axis, df_metrics.iloc[:, metric], \"k-\")\n",
    "        axarr[metric_index].set_title(\"{} curve over K values\".format(metric_names[metric_index]), fontsize = 18)\n",
    "        metric_index += 1\n",
    "    \n",
    "    # if boolean parameter for plotting is True, show the figure\n",
    "    if showPlots:\n",
    "        plt.show()\n",
    "    \n",
    "    f.savefig(\"../data/plots/{}_{}_{}_{}_plots.png\".format(datasetName, context, function, algorithm), \n",
    "                                                              bbox_inches='tight')\n",
    "    print(\"Plots saved in ../data/plots/{}_{}_{}_{}_plots.png\".format(datasetName, context, function, algorithm))\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotSilhouette(datasetName, context, function, algorithm, k):\n",
    "    plt.ioff() # this way only plt.show() will display figures\n",
    "    \n",
    "    df = pd.read_csv('../data/processed/{}_{}_{}_dataset.csv'.format(datasetName, context, function), index_col=0)\n",
    "     \n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, ax1 = plt.subplots(1, 1)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (k+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(df.values) + (k + 1) * 10])\n",
    "\n",
    "    df_scaled = TimeSeriesScalerMeanVariance(mu=0.0, std=1.0).fit_transform(df.values)\n",
    "    df_scaled = np.squeeze(df_scaled)\n",
    "    \n",
    "    clusterer, cluster_labels = doClustering(df_scaled, datasetName, algorithm, k, seed=3, max_iter=300)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    \n",
    "    silhouette_avg = round(silhouette_score(df_scaled, cluster_labels), 2) # round to two decimals\n",
    "    print(\"For k =\", k, \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(df_scaled, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(k):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / k)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i), fontsize = 30)\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"Silhouette analysis for {} {} using {}\".format(context, function, algorithm), fontsize = 30)\n",
    "    ax1.set_xlabel(\"Silhouette coefficient values\", fontsize = 30)\n",
    "    ax1.set_ylabel(\"Cluster label\", fontsize = 30)\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.3,-0.2, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    plt.rc('xtick',labelsize=20)\n",
    "    plt.rc('ytick',labelsize=20)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotCurvesOneBuilding(datasetName, context, function, resolution='day'):\n",
    "    dataframe = pd.read_csv('../data/processed/{}_{}_dataset.csv'.format(datasetName, context), parse_dates=True, \n",
    "                            infer_datetime_format=True, index_col=0)\n",
    "    df_load_curves = pd.DataFrame() # dataframe that will hold all load curves\n",
    "\n",
    "    # resample based on parameter\n",
    "    if (resolution == 'day'):\n",
    "        availableSamples = (dataframe.resample('1D').asfreq()).index # get list of timestamps group by day\n",
    "        delta = 23 # timedelta based on resample\n",
    "    else:\n",
    "        print(\"Please choose a valid resolution\")\n",
    "        exit()\n",
    "\n",
    "    # iterate through all buildings (column)\n",
    "    for column in range(len(dataframe.columns)):\n",
    "        df_sampledReadings = pd.DataFrame() # dataframe to hold new samples for a column\n",
    "        currentColumn = pd.DataFrame(dataframe.iloc[:, column])\n",
    "        \n",
    "        # iterate through each day\n",
    "        for timestamp in availableSamples:\n",
    "            # update time limits to the current date\n",
    "            start = timestamp\n",
    "            end = timestamp + timedelta(hours=delta)\n",
    "            # get meter data from only this resolution\n",
    "            df_reading = currentColumn[(currentColumn.index >= start) & (currentColumn.index <= end)]\n",
    "            # ignore index since they are unique timestamps\n",
    "            df_reading.reset_index(drop=True, inplace=True)         \n",
    "            # append new sample as columns\n",
    "            df_sampledReadings = pd.concat([df_sampledReadings, df_reading], axis=1)\n",
    "            \n",
    "        # make sure sure there are no columns with NaN values\n",
    "        df_sampledReadings.dropna(axis=1, how='all', inplace=True)\n",
    "        df_sampledReadings = df_sampledReadings.T # transpose it so it's easier to see and operate\n",
    "        # up to this point, the matrix above has the shape nxm where is the number of instances and m is the number of readings\n",
    "    \n",
    "        # if any NaN prevailed\n",
    "        df_sampledReadings.fillna(value=0, inplace=True) \n",
    "\n",
    "        # calculate load curve based on function\n",
    "        if function == 'average':\n",
    "            load_curve = np.mean(df_sampledReadings, axis = 0)\n",
    "\n",
    "        elif function =='median':\n",
    "            load_curve = np.median(df_sampledReadings, axis = 0)\n",
    "\n",
    "        else:\n",
    "            print(\"Please choose a valid context\")\n",
    "            exit()\n",
    "\n",
    "        ###################################################################\n",
    "#         TODO: coding is for plotting purposes\n",
    "        plt.figure(figsize=(18,10))\n",
    "        plt.ylim(0,7)\n",
    "        x_axis = range(0, len(df_sampledReadings.columns))\n",
    "        for _, curve in df_sampledReadings.iterrows():\n",
    "            plt.plot(curve, \"k-\", alpha=.2)\n",
    "#         plt.plot(load_curve, \"r-\")\n",
    "        plt.title(\"Load Profiles and red representative curve based on {}\".format(function))        \n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(18,10))\n",
    "\n",
    "        plt.plot(load_curve, \"r-\", linewidth=7.0)\n",
    "        \n",
    "        plt.ylim(0,7)\n",
    "        \n",
    "        ###################################################################\n",
    "\n",
    "        # turn into one column dataframe for easier manipulation\n",
    "        load_curve = pd.DataFrame(load_curve)\n",
    "        # keep the instance name as column name\n",
    "        instance_name = []\n",
    "        instance_name.append(df_sampledReadings.index[0])\n",
    "        load_curve.columns = instance_name\n",
    "        # append current load curve to dataframe\n",
    "        df_load_curves = pd.concat([df_load_curves, load_curve], axis=1)\n",
    "        \n",
    "        # end of for loop for one column\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotSC():\n",
    "    df = pd.read_csv('../data/results/total_scores.csv', index_col=0)\n",
    "    \n",
    "    \n",
    "    datasets = ['BDG', 'DGS', 'BDG-DGS']\n",
    "    algorithms = ['kshape', 'kmeans', 'hierarchical']\n",
    "    contexts = ['weekday', 'weekend', 'fullweek']\n",
    "    function = ['average', 'median']\n",
    "    \n",
    "    # Four axes, returned as a 2-d array\n",
    "    f, axarr = plt.subplots(3, 3, figsize=(20,18))\n",
    "    f.subplots_adjust(hspace=0.05, wspace=0.05)\n",
    "    \n",
    "    for i in range(3): # rows\n",
    "        df_row = df[df.iloc[:, 0] == datasets[i]]\n",
    "#         print(i)\n",
    "#         print(df_row.iloc[0, 0])\n",
    "        for j in range(3): # columns, different algorithm\n",
    "            \n",
    "            df_experiment = df_row.copy()\n",
    "            df_experiment = df_experiment[df_experiment.iloc[:, 3] == algorithms[j]]\n",
    "            df_experiment_context = df_experiment.iloc[:, 0:4]\n",
    "            df_experiment_context.drop_duplicates(inplace = True) \n",
    "#             print(j)\n",
    "#             print(df_experiment.iloc[0, 3])\n",
    "            # one plot for each context\n",
    "            for _, context in df_experiment_context.iterrows():\n",
    "                current_plot = df_experiment[(df_experiment.iloc[:, 1] == context[1]) &\n",
    "                                            (df_experiment.iloc[:, 2] == context[2])]\n",
    "#                 print(current_plot)\n",
    "                axarr[i, j].set_ylim([-0.05, 0.6])\n",
    "                x_values = current_plot.iloc[:, 4]\n",
    "                y_values = current_plot[' ']\n",
    "                context_name = str(context[1]) + \" \" +  str(context[2])\n",
    "                axarr[i, j].plot(x_values, y_values, label=context_name)\n",
    "                axarr[i, j].xaxis.grid(True)\n",
    "\n",
    "    # Fine-tune figure; hide x ticks for top plots and y ticks for right plots\n",
    "    cols = ['K-Shape', 'K-Means', 'Hierarchical']\n",
    "    for ax, col in zip(axarr[0], cols):\n",
    "        ax.set_title(col, fontsize = 30)\n",
    "\n",
    "    for ax, row in zip(axarr[:,0], datasets):\n",
    "        ax.set_ylabel(row, rotation=90, fontsize=30)\n",
    "    \n",
    "    f.text(0.5, 0.08, 'K', ha='center', fontsize=30)\n",
    "    \n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 1.55), prop={'size': 20})\n",
    "    plt.rc('xtick',labelsize=20)\n",
    "    plt.rc('ytick',labelsize=20)\n",
    "\n",
    "    plt.setp([a.get_xticklabels() for a in axarr[0, :]], visible=False)\n",
    "    plt.setp([a.get_xticklabels() for a in axarr[1, :]], visible=False)\n",
    "    plt.setp([a.get_yticklabels() for a in axarr[:, 1]], visible=False)\n",
    "    plt.setp([a.get_yticklabels() for a in axarr[:, 2]], visible=False)\n",
    "    plt.show()\n",
    "    f.savefig(\"../data/plots/silhouette_all_plots.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-30T15:14:17.544292Z",
     "start_time": "2018-12-30T15:14:17.533171Z"
    }
   },
   "outputs": [],
   "source": [
    "def plotGroundTruthDist(datasetName, context, function, algorithm, k):\n",
    "    # load metadata\n",
    "    if datasetName == 'BDG':\n",
    "        df_meta = pd.read_csv('../data/raw/meta_open.csv', index_col=0)\n",
    "    else:\n",
    "        return\n",
    "    # load data\n",
    "    df_data = pd.read_csv('../data/processed/{}_{}_{}_dataset.csv'.format(datasetName, context, function), index_col=0)\n",
    "\n",
    "    # Z-normalize the data\n",
    "    df_scaled = TimeSeriesScalerMeanVariance(mu=0.0, std=1.0).fit_transform(df_data.values)\n",
    "    # df.values will generate a 3d array (the third dimension being 1) so we convert it to a 2d array\n",
    "    df_scaled = np.squeeze(df_scaled) # now is a 2d array\n",
    "    \n",
    "    # run algorithm\n",
    "    model, labels = doClustering(df_scaled, datasetName, algorithm, k, seed=3, max_iter=300)\n",
    "    \n",
    "    # get number of elements in each cluster\n",
    "    data_dict = {i: np.where(labels == i)[0] for i in range(k)}\n",
    "\n",
    "    df_dist = pd.DataFrame()\n",
    "    df_dist_aux = pd.DataFrame()\n",
    "    for key, value in data_dict.items():\n",
    "\n",
    "        print(len(value.tolist()))\n",
    "        print(key)\n",
    "        \n",
    "        ground_truth_labels = df_meta.iloc[value.tolist()]\n",
    "        df_dist_aux = pd.concat([df_dist_aux, ground_truth_labels['primaryspaceusage']], ignore_index=True, axis=1)\n",
    "        \n",
    "        # count the times the different ground truth labels exist in each cluster\n",
    "        df_dist[key] = ground_truth_labels['primaryspaceusage'].value_counts()\n",
    "\n",
    "    df_dist.index.name = 'PSU'\n",
    "\n",
    "    print(df_dist)\n",
    "    \n",
    "    # plot stacked bar\n",
    "    ax = df_dist.T.plot.barh(stacked=True, figsize=(20, 18))\n",
    "    plt.legend(loc='top right', prop={'size': 30})\n",
    "\n",
    "    plt.rc('xtick',labelsize=20)\n",
    "    plt.rc('ytick',labelsize=20)\n",
    "    ax.xaxis.grid(True)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
